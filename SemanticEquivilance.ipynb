{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a395963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 404351 sentences.\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "def load_data(csv_file):\n",
    "    \"\"\"Loads Question Pairs from a CSV file\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to csv_file\n",
    "    Returns:\n",
    "        tuple: A tuple containing supervised data pairs\n",
    "        returns [],[] on error\n",
    "    \"\"\"\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    is_duplicate = []\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        #print(\"Column names:\", df.columns)\n",
    "        sentence1_series = df['question1']\n",
    "        sentence2_series = df['question2']\n",
    "        is_duplicate_series = df['is_duplicate']\n",
    "        \n",
    "        sentences1 = sentence1_series.tolist()\n",
    "        sentences2 = sentence2_series.tolist()\n",
    "        is_duplicate = is_duplicate_series.tolist()\n",
    "        \n",
    "        if len(sentences1) != len(sentences2):\n",
    "            raise ValueError(\"The number of sentences in question1 and question2 do not match.\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(sentences1)} sentences.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Wrong Path\")\n",
    "        return [],[],[]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An {e} Error Occurred\")\n",
    "        return [],[],[]\n",
    "\n",
    "DATA_PATH = r'C:/Users/Jash\\Documents/Research\\Semantic Equivilance\\SemanticEquivilance/question_pairs/questions.csv'\n",
    "sentences1, sentences2, value = load_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54dcd9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 processes.\n"
     ]
    }
   ],
   "source": [
    "from lambeq import BobcatParser, SpacyTokeniser, Rewriter, AtomicType, IQPAnsatz\n",
    "from lambeq.backend.grammar import Diagram as grammatical_diagram\n",
    "from lambeq.backend.quantum import Diagram as quantum_circuit\n",
    "from typing import Optional\n",
    "import os, time, multiprocessing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" #environment variable for multithreading\n",
    "\n",
    "#Global data sequencing variables\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(f\"Using {num_processes} processes.\")\n",
    "\n",
    "_tokenizer = None\n",
    "_parser = None\n",
    "_rewriter = None\n",
    "_ansatz = None\n",
    "\n",
    "def _initializer():\n",
    "    global _tokenizer, _parser, _rewriter, _ansatz\n",
    "    _tokenizer = SpacyTokeniser()  # Initialize tokenizer\n",
    "    _parser = BobcatParser(verbose=\"suppress\")  # Initialize parser \n",
    "    _rewriter = Rewriter(['prepositional_phrase', 'determiner'])  # Initialize rewriter\n",
    "    _ansatz = IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 1}, n_layers=2, n_single_qubit_params=3)  # Initialize ansatz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42ee7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sentence: str, tokeniser, parser, rewriter, ansatz) -> Optional[grammatical_diagram]:\n",
    "    \"\"\"Process a single sentence to a diagram.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Sentence to be converted to a diagram.\n",
    "\n",
    "    Returns:\n",
    "        Optional[quantum_circuit]: Either returns a diagram or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentence = sentence.strip().lower()\n",
    "        tokens = tokeniser.tokenise_sentence(sentence)\n",
    "        diagram = parser.sentence2diagram(tokens, tokenised=True)\n",
    "        if diagram is not None:\n",
    "            diagram = rewriter(diagram)\n",
    "            normalised_diagram = diagram.normal_form()\n",
    "            curry_functor = Rewriter(['curry'])\n",
    "            curried_diagram = curry_functor(normalised_diagram)\n",
    "            circuit = ansatz(curried_diagram)\n",
    "            return circuit\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence {sentence}\")\n",
    "        return None\n",
    "def _process_data_for_pool(sentence: str) -> Optional[grammatical_diagram]:\n",
    "    \"\"\"Process a single sentence for the multiprocessing pool.\"\"\"\n",
    "    return process_data(sentence, _tokenizer, _parser, _rewriter, _ansatz)\n",
    "\n",
    "def process_sentences(sentences: list[str]) -> list[Optional[grammatical_diagram]]:\n",
    "    \"\"\"Process sentences in parallel using multiprocessing.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[str]): List of sentences to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list[Optional[quantum_circuit]]: List of processed diagrams or None for errors.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    batch_size = 50\n",
    "    with multiprocessing.Pool(processes=num_processes, initializer=_initializer) as pool:\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i + batch_size]\n",
    "            print(f\"Processing batch {i // batch_size + 1} with {len(batch)} sentences.\")\n",
    "            current_batch = sentences[i:i + batch_size]\n",
    "            \n",
    "            batch_results = pool.map(_process_data_for_pool, current_batch)\n",
    "        # Collect results from all batches\n",
    "        #results = pool.map(_process_data_for_pool, sentences)\n",
    "        end_time = time.time()\n",
    "        print(f\"Processed {len(sentences)} sentences in {end_time - start_time:.4f} seconds.\")\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ac8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 with 50 sentences.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(sentences1), 50):\n",
    "    batch1 = sentences1[i:i + 50]\n",
    "    #batch2 = sentences2[i:i + 50]\n",
    "    print(f\"Processing batch {i // 50 + 1} with {len(batch1)} sentences.\")\n",
    "    _initializer()\n",
    "    results1 = process_data(sentences1, _tokenizer, _parser, _rewriter, _ansatz)\n",
    "    #results2 = process_sentences(batch2)\n",
    "    \n",
    "    # Here you can handle the results as needed\n",
    "    # For example, you could save them to a file or process them further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bbccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentences1...\n",
      "Processing batch 1 with 50 sentences.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Process sentences1 and sentences2 in parallel\n",
    "    print(\"Processing sentences1...\")\n",
    "    diagrams1 = process_sentences(sentences1)\n",
    "    # print(\"Processing sentences2...\")\n",
    "    # diagrams2 = process_sentences(sentences2)\n",
    "\n",
    "    # # Filter out None values (errors)\n",
    "    # diagrams1 = [d for d in diagrams1 if d is not None]\n",
    "    # diagrams2 = [d for d in diagrams2 if d is not None]\n",
    "\n",
    "    # print(f\"Processed {len(diagrams1)} diagrams from sentences1 and {len(diagrams2)} from sentences2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e80061",
   "metadata": {},
   "outputs": [],
   "source": [
    "_initializer()  # Initialize global variables for the first run\n",
    "for i in range(0, len(sentences1)):\n",
    "    circuit1 = process_data(sentences1[i], _tokenizer, _parser, _rewriter, _ansatz)\n",
    "    #circuit1.draw(figsize=(10, 10))\n",
    "    circuit2 = process_data(sentences2[i], _tokenizer, _parser, _rewriter, _ansatz)\n",
    "    if circuit1 is None or circuit2 is None:\n",
    "        print(f\"Error processing sentences at index {i}. Skipping comparison.\")\n",
    "        continue\n",
    "    if circuit1 == circuit2 or value[i]:\n",
    "        print(\"The circuits are equivalent.\")\n",
    "        circuit1.draw(figsize=(10, 10))\n",
    "        circuit2.draw(figsize=(10, 10))\n",
    "    else:\n",
    "        print(\"The circuits are not equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2718a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m         evaluated_pairs = \u001b[32m2\u001b[39m * (evaluated_pairs - \u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# Scale to [-1, 1]\u001b[39;00m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.xor_net(evaluated_pairs)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m model = XORSentenceModel().from_diagrams(\u001b[43ma\u001b[49m+b, probabilities=\u001b[38;5;28;01mTrue\u001b[39;00m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     34\u001b[39m model.initialise_weights()\n",
      "\u001b[31mNameError\u001b[39m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 12\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "\n",
    "train_labels, train_data = zip(sentences1, sentences2), value #this needs to be portioned out of the data for a train/test/validation split\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "from torch import nn\n",
    "from lambeq import PennyLaneModel\n",
    "class XORSentenceModel(PennyLaneModel): #QNN LTSM Model\n",
    "    def __init__(self, **kwargs):\n",
    "        PennyLaneModel.__init__(self, **kwargs)\n",
    "        self.xor_net = nn.Sequential(\n",
    "            nn.Linear(4, 10),  # Adjust input size based on your\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, circuit1, circuit2): #passes through the network\n",
    "        evaluated_pairs = torch.cat((self.get_diagram_output(circuit1),self.get_diagram_output(circuit2)), dim=1)  # Concatenate the outputs of both diagrams\n",
    "        evaluated_pairs = 2 * (evaluated_pairs - 0.5)  # Scale to [-1, 1]\n",
    "        return self.xor_net(evaluated_pairs)\n",
    "    \n",
    "model = XORSentenceModel().from_diagrams(a+b, probabilities=True, normalize=True)\n",
    "model.initialise_weights()\n",
    "model = model.double()  # Convert model to double precision\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Use Adam optimizer\n",
    "def accuracy(circs, labels):\n",
    "    \"\"\"Calculate the accuracy of the model.\"\"\"\n",
    "    predicted = model(circs)\n",
    "    return (torch.round(torch.flatten(predicted)) == torch.DoubleTensor(labels)).sum().item() / len(circs)\n",
    "\n",
    "best = {'accuracy': 0.0,\n",
    "        'epoch': 0,}\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for circuits, labels in zip(train_data, train_labels):\n",
    "        optimizer.zero_grad()\n",
    "        predicted = model(circuits) #use BCE loss for binary classification\n",
    "        loss = torch.nn.functional.binary_cross_entropy(torch.flatten(predicted), torch.DoubleTensor(labels))\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #eval every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            acc = accuracy(circuits, labels)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}, Accuracy: {acc}\")\n",
    "            if acc > best['accuracy']:\n",
    "                best['accuracy'] = acc\n",
    "                best['epoch'] = epoch\n",
    "                print(f\"New best model found at epoch {epoch} with accuracy {acc}\")\n",
    "                model.save('best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92c72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36013460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2bc6768e060>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment and Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import yaml\n",
    "import os #multithreading\n",
    "\n",
    "# PennyLane and PyTorch\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lambeq\n",
    "from lambeq.backend.quantum import Diagram as LambeqDiagram\n",
    "from discopy.quantum import gates\n",
    "import spacy\n",
    "import discopy\n",
    "from lambeq import BobcatParser, Rewriter, IQPAnsatz, SpacyTokeniser, AtomicType\n",
    "from discopy.rigid import Ty\n",
    "\n",
    "#data handling and plotting\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Patch for discopy\n",
    "monoidal_module = getattr(discopy, \"monoidal\", None)\n",
    "if monoidal_module:\n",
    "    diagram_class = getattr(monoidal_module, \"Diagram\", None)\n",
    "    if diagram_class and not hasattr(diagram_class, \"is_mixed\"):\n",
    "        diagram_class.is_mixed = property(lambda self: False)\n",
    "\n",
    "# Load spacy model\n",
    "spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540c7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading Function\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    sentences1, sentences2, is_duplicate = [], [], []\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentences1 = df['question1'].astype(str).tolist()\n",
    "        sentences2 = df['question2'].astype(str).tolist()\n",
    "        is_duplicate = df['is_duplicate'].tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(sentences1)} sentence pairs.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "def create_balanced_training_set(training_data: list) -> list:\n",
    "    \"\"\"Creates a balanced training set by undersampling the majority class.\"\"\"\n",
    "    positives = [pair for pair in training_data if pair['label'] == 1]\n",
    "    negatives = [pair for pair in training_data if pair['label'] == 0]\n",
    "    \n",
    "    # Undersample the larger class to match the size of the smaller class\n",
    "    if len(positives) > len(negatives):\n",
    "        positives = random.sample(positives, len(negatives))\n",
    "    else:\n",
    "        negatives = random.sample(negatives, len(positives))\n",
    "    \n",
    "    balanced_train_set = positives + negatives\n",
    "    random.shuffle(balanced_train_set)\n",
    "    \n",
    "    print(f\"Created a balanced training set with {len(positives)} positive and {len(negatives)} negative pairs.\")\n",
    "    return balanced_train_set\n",
    "def load_fasttext_embeddings(fasttext_file_path):\n",
    "    \"\"\"\n",
    "    Loads FastText embeddings from a .vec file into a word-to-index dictionary\n",
    "    and an embedding matrix.\n",
    "    \"\"\"\n",
    "    print(f\"Loading FastText embeddings from {fasttext_file_path}...\")\n",
    "    word_to_idx = {}\n",
    "    embeddings = []\n",
    "    \n",
    "    # Add a padding token at index 0 for unknown words\n",
    "    word_to_idx['<pad>'] = 0\n",
    "    # The embedding dimension is 300 for this file\n",
    "    embeddings.append(np.zeros(300)) \n",
    "    \n",
    "    with open(fasttext_file_path, 'r', encoding='utf-8') as f:\n",
    "        # The first line of a .vec file is a header, skip it\n",
    "        next(f) \n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            # Handle potential lines with only a word and no vector\n",
    "            if len(parts) > 2:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                word_to_idx[word] = i + 1\n",
    "                embeddings.append(vector)\n",
    "            \n",
    "    embeddings_matrix = np.array(embeddings)\n",
    "    print(f\"Loaded {len(word_to_idx)} word vectors.\")\n",
    "    return word_to_idx, embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fde61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "def plot_training_history(history):\n",
    "    if not history:\n",
    "        print(\"History is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history, label='Average Loss per Epoch')\n",
    "    plt.title('Training Loss Convergence')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Penalized Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_history(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(len(param_history))\n",
    "    means = [d['mean'] for d in param_history]\n",
    "    stds = [d['std'] for d in param_history]\n",
    "    mins = [d['min'] for d in param_history]\n",
    "    maxs = [d['max'] for d in param_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, means, label='Mean Parameter Value')\n",
    "    plt.fill_between(epochs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.2, label='1 Std. Deviation')\n",
    "    plt.plot(epochs, mins, linestyle='--', color='gray', label='Min/Max Range')\n",
    "    plt.plot(epochs, maxs, linestyle='--', color='gray')\n",
    "    \n",
    "    plt.title('Evolution of Model Parameters During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_evolution_polar(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "    epochs = np.array(range(len(param_history)))\n",
    "    \n",
    "    # --- THE FIX ---\n",
    "    # We use modulo 4*pi to correctly represent the parameter's period.\n",
    "    mean_angles = np.array([d['mean'] for d in param_history]) % (4 * np.pi)\n",
    "    # ---------------\n",
    "\n",
    "    ax.plot(mean_angles, epochs, 'o-', label='Mean Parameter Path')\n",
    "\n",
    "    if len(epochs) > 0:\n",
    "        ax.plot(mean_angles[0], epochs[0], 'gX', markersize=12, label='Start')\n",
    "        ax.plot(mean_angles[-1], epochs[-1], 'rX', markersize=12, label='End')\n",
    "\n",
    "    ax.set_theta_zero_location('N')# pyright: ignore\n",
    "    ax.set_theta_direction(-1)# pyright: ignore\n",
    "    ax.set_rlabel_position(0)# pyright: ignore\n",
    "    ax.set_rlim(0, len(epochs) * 1.05)# pyright: ignore\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_title('Cyclical Evolution of Mean Parameter', pad=20)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "def plot_parameter_deltas(param_history):\n",
    "    if len(param_history) < 2:\n",
    "        print(\"Need at least 2 epochs to plot parameter deltas.\")\n",
    "        return\n",
    "\n",
    "    mean_angles = np.array([d['mean'] for d in param_history])\n",
    "    \n",
    "    # Calculate the shortest angle difference between each epoch\n",
    "    deltas = []\n",
    "    for i in range(1, len(mean_angles)):\n",
    "        prev_angle = mean_angles[i-1]\n",
    "        curr_angle = mean_angles[i]\n",
    "        delta = np.arctan2(np.sin(curr_angle - prev_angle), np.cos(curr_angle - prev_angle))\n",
    "        deltas.append(delta)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # We plot against epochs 1 to N, since the first delta occurs at epoch 1\n",
    "    plt.plot(range(1, len(mean_angles)), deltas, 'o-', label='Change in Mean Parameter (Delta)')\n",
    "    \n",
    "    plt.axhline(0, color='red', linestyle='--', label='No Change')\n",
    "    plt.title('Epoch-to-Epoch Change in Mean Parameter Value')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Shortest Angle Difference (Radians)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, len(mean_angles)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes and plots a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): The ground-truth labels (0s and 1s).\n",
    "        y_pred (np.array): The model's raw probability predictions (overlaps from 0 to 1).\n",
    "        threshold (float): The cutoff for classifying a prediction as 1.\n",
    "    \"\"\"\n",
    "    # Convert probability predictions to binary 0/1 predictions\n",
    "    binary_preds = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, binary_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Non-Duplicate', 'Predicted Duplicate'],\n",
    "                yticklabels=['Actual Non-Duplicate', 'Actual Duplicate'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes and plots the ROC curve and AUC score.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNLP MODEL AND TRAINING PIPELINE\n",
    "# 1. THE QNLP MODEL CLASS\n",
    "# ===============================================================\n",
    "class QNLPModel(nn.Module):\n",
    "    def __init__(self, symbols, word_to_idx, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.from_numpy(embeddings), freeze=False, padding_idx=0)\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.symbols_map = {}\n",
    "        for s in symbols:\n",
    "            word = s.name.split('_')[-1]\n",
    "            if word in self.word_to_idx:\n",
    "                self.symbols_map[s.name.replace('.', '_')] = self.word_to_idx[word]\n",
    "            else:\n",
    "                self.symbols_map[s.name.replace('.', '_')] = 0\n",
    "        self.padding_theta = Parameter(torch.tensor(0.1)) # For RY gates\n",
    "        self.padding_phi = Parameter(torch.tensor(0.1))   # For CPHASE gates\n",
    "\n",
    "    def forward(self, diagram):\n",
    "        # Filter the diagram's symbols to include only non-nouns\n",
    "        parameterized_symbols = [\n",
    "            s for s in diagram.free_symbols if not s.name.lower().startswith('noun')\n",
    "        ]\n",
    "\n",
    "        if not parameterized_symbols:\n",
    "            return torch.tensor([])\n",
    "\n",
    "        # Get word indices only for the symbols that require parameters\n",
    "        param_indices = [self.symbols_map.get(s.name.replace('.', '_'), 0) \n",
    "                         for s in parameterized_symbols]\n",
    "        \n",
    "        indices_tensor = torch.tensor(param_indices, dtype=torch.long)\n",
    "        \n",
    "        # Look up the FastText vector for each of these non-noun words\n",
    "        embedded_vectors = self.embedding(indices_tensor)\n",
    "        \n",
    "        # Apply the deterministic mapping to get the \"warm start\" parameters\n",
    "        quantum_params = torch.sum(embedded_vectors[:, :10], dim=1) * 0.1\n",
    "        \n",
    "        return quantum_params\n",
    "\n",
    "# ===============================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ===============================================================\n",
    "def calculate_quantum_angle(squared_fidelity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the quantum angle (Fubini-Study distance) between two state vectors.\n",
    "\n",
    "    This angle represents the geometric distance between two pure states on the\n",
    "    surface of the generalized Bloch sphere (Hilbert space). It is a key metric for\n",
    "    quantifying the similarity of quantum states.\n",
    "\n",
    "    Args:\n",
    "        squared_fidelity (float): The squared magnitude of the inner product\n",
    "                                  between the two states, i.e., |<ψ|φ>|². This is\n",
    "                                  the value typically estimated by a swap test.\n",
    "\n",
    "    Returns:\n",
    "        float: The angle between the two state vectors in radians, a value from\n",
    "               0 (identical states) to π/2 (orthogonal states).\n",
    "    \"\"\"\n",
    "    # Take the square root to get the fidelity |<ψ|φ>|\n",
    "    fidelity = math.sqrt(abs(squared_fidelity))\n",
    "\n",
    "    # Clamp the value between -1.0 and 1.0 for numerical stability with acos\n",
    "    clamped_fidelity = max(-1.0, min(1.0, fidelity))\n",
    "\n",
    "    # The angle is the arccosine of the fidelity\n",
    "    return math.acos(clamped_fidelity)\n",
    "\n",
    "def get_diagram_width(diagram):\n",
    "    \"\"\"Calculates the true maximum width of a diagram at any point.\"\"\"\n",
    "    if not diagram.boxes:\n",
    "        return len(diagram.cod)\n",
    "    # The width is the maximum wire index a box acts on.\n",
    "    return max(\n",
    "        [offset + len(box.dom) for box, offset in zip(diagram.boxes, diagram.offsets)]\n",
    "        + [len(diagram.cod)]\n",
    "    )\n",
    "    \n",
    "def execute_discopy_diagram(current_width, diagram, params, wires, embedding_method='simple_pad', rotation_param=None, entangling_param=None):\n",
    "    \"\"\"\n",
    "    Executes a DisCoPy/lambeq diagram's instructions on a specific set of wires,\n",
    "    and optionally applies an entangling layer afterwards.\n",
    "    \"\"\"\n",
    "    # Step 1: Execute the original sentence diagram as before\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "    # Step 2: Apply Padding Method\n",
    "    ancilla_wires = wires[current_width:]\n",
    "    if embedding_method == 'parameterized':\n",
    "        # 1. Apply parameterized rotation layer\n",
    "        if rotation_param is not None:\n",
    "            for w in ancilla_wires:\n",
    "                qml.RY(rotation_param, wires=w)\n",
    "        # 2. Apply parameterized entangling layer (circular CPHASE)\n",
    "        if entangling_param is not None and len(ancilla_wires) > 1:\n",
    "            for i in range(len(ancilla_wires)):\n",
    "                qml.CPHASE(entangling_param, wires=[ancilla_wires[i], ancilla_wires[(i + 1) % len(ancilla_wires)]])\n",
    "\n",
    "    elif embedding_method == 'superposition':\n",
    "        # 1. Apply Hadamard gates to all ancilla qubits to create superposition.\n",
    "        for w in ancilla_wires:\n",
    "            qml.Hadamard(wires=w)\n",
    "        # 2. Apply a chain of CNOTs to entangle the ancilla qubits with each other.\n",
    "        if len(ancilla_wires) > 1:\n",
    "            for i in range(len(ancilla_wires) - 1):\n",
    "                qml.CNOT(wires=[ancilla_wires[i], ancilla_wires[i+1]])\n",
    "            \n",
    "    elif embedding_method == 'entangle':\n",
    "        # This remains the CNOT ladder between sentence and ancilla qubits.\n",
    "        for i in range(min(current_width, len(ancilla_wires))):\n",
    "            control_wire = wires[i]\n",
    "            target_wire = ancilla_wires[i]\n",
    "            qml.CNOT(wires=[control_wire, target_wire])\n",
    "            \n",
    "    # If embedding_method is 'simple_pad', we do nothing extra.\n",
    "# ===============================================================\n",
    "# 3. PREPROCESSING FUNCTION\n",
    "# ===============================================================\n",
    "def preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    print(f\"Starting preprocessing with a qubit limit of {qubit_limit}...\")\n",
    "    filtered_pairs, all_symbols, n_max = [], set(), 0\n",
    "    for s1, s2, is_duplicate in data_pairs:\n",
    "        try:\n",
    "            d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "            width1 = get_diagram_width(d1)\n",
    "            width2 = get_diagram_width(d2)\n",
    "            if width1 <= qubit_limit and width2 <= qubit_limit:\n",
    "                pair_data = {\n",
    "                    's1': s1, 's2': s2, 'label': is_duplicate, 'd1': d1, 'd2': d2,\n",
    "                    'structural_disparity': abs(len(d1.cod) - len(d2.cod)),\n",
    "                    'width1': width1,\n",
    "                    'width2': width2\n",
    "                }\n",
    "                filtered_pairs.append(pair_data)\n",
    "                all_symbols.update(d1.free_symbols)\n",
    "                all_symbols.update(d2.free_symbols)\n",
    "                n_max = max(n_max, width1, width2)\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    print(f\"Total unique parameters (symbols) found: {len(all_symbols)}\")\n",
    "    print(f\"N_Max for the filtered dataset is: {n_max}\")\n",
    "    return filtered_pairs, sorted(list(all_symbols), key=lambda s: s.name), n_max\n",
    "\n",
    "# ===============================================================\n",
    "# 4. THE TRAINING FUNCTION (with Adam & Param Tracking)\n",
    "# ===============================================================\n",
    "def train_model(model, data, n_max, device_name, base_learning_rate, lambda_penalty, epochs, embedding_method='entangle'):\n",
    "    optimizer = Adam(model.parameters(), lr=base_learning_rate)\n",
    "    swap_dev = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "\n",
    "    loss_history, param_history = [], []\n",
    "    print(\"--- Starting training with END-TO-END PyTorch ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch, num_trained_pairs = 0, 0\n",
    "        for i, pair in enumerate(data):\n",
    "            \n",
    "            # --- THE FIX: Define the QNode INSIDE the loop ---\n",
    "            # This allows it to \"close over\" the non-tensor pair data.\n",
    "            @qml.qnode(swap_dev, interface=\"torch\")\n",
    "            def swap_test_qnode(p1, p2, theta, phi):\n",
    "                qml.Hadamard(wires=0)\n",
    "                execute_discopy_diagram(\n",
    "                    pair['width1'], pair['d1'], p1, wires=range(1, 1 + n_max), \n",
    "                    embedding_method=embedding_method, \n",
    "                    rotation_param=theta, entangling_param=phi)\n",
    "                execute_discopy_diagram(\n",
    "                    pair['width2'], pair['d2'], p2, wires=range(1 + n_max, 1 + 2 * n_max), \n",
    "                    embedding_method=embedding_method, \n",
    "                    rotation_param=theta, entangling_param=phi)\n",
    "                for j in range(n_max):\n",
    "                    qml.CSWAP(wires=[0, 1 + j, 1 + n_max + j])\n",
    "                qml.Hadamard(wires=0)\n",
    "                return qml.expval(qml.PauliZ(0))\n",
    "            # ----------------------------------------------------\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            params1 = model(pair['d1'])\n",
    "            params2 = model(pair['d2'])\n",
    "            \n",
    "            if params1.nelement() == 0 or params2.nelement() == 0:\n",
    "                continue\n",
    "            num_trained_pairs += 1\n",
    "\n",
    "            # The QNode is now called with only the trainable tensors\n",
    "            measured_overlap = swap_test_qnode(\n",
    "                params1, params2, \n",
    "                model.padding_theta, model.padding_phi\n",
    "            )\n",
    "            \n",
    "            fidelity_loss = (measured_overlap - pair['label'])**2\n",
    "            structural_penalty = lambda_penalty * pair['structural_disparity']\n",
    "            loss = fidelity_loss + structural_penalty\n",
    "\n",
    "            # This will now work correctly\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss_epoch / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        all_params = torch.cat([p.data.flatten() for p in model.parameters()]).detach().numpy()\n",
    "        if all_params.size > 0:\n",
    "            param_history.append({'mean': np.mean(all_params), 'std': np.std(all_params),\n",
    "                                  'min': np.min(all_params), 'max': np.max(all_params)})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Penalized Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, loss_history, param_history\n",
    "# ===============================================================\n",
    "# 5. Inference Function\n",
    "# ===============================================================\n",
    "def evaluate_model(model, test_data, n_max, device_name, embedding_method='entangle'):\n",
    "    \"\"\"\n",
    "    Evaluates a trained model. This version is updated to pass the\n",
    "    pre-calculated diagram widths to the execution function.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Evaluation on Test Set ---\")\n",
    "    model.eval()\n",
    "    swap_dev = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, pair in enumerate(test_data):\n",
    "            params1, params2 = model(pair['d1']), model(pair['d2'])\n",
    "            padding_theta = model.padding_theta\n",
    "            padding_phi = model.padding_phi\n",
    "            \n",
    "            if params1.nelement() == 0 or params2.nelement() == 0:\n",
    "                continue\n",
    "\n",
    "            @qml.qnode(swap_dev, interface=\"torch\")\n",
    "            def swap_test_qnode(p1, p2, theta, phi):\n",
    "                qml.Hadamard(wires=0)\n",
    "                execute_discopy_diagram(\n",
    "                    pair['width1'], pair['d1'], p1, wires=range(1, 1 + n_max),\n",
    "                    embedding_method=embedding_method,\n",
    "                    rotation_param=theta, entangling_param=phi)\n",
    "                execute_discopy_diagram(\n",
    "                    pair['width2'], pair['d2'], p2, wires=range(1 + n_max, 1 + 2 * n_max),\n",
    "                    embedding_method=embedding_method,\n",
    "                    rotation_param=theta, entangling_param=phi)\n",
    "                for j in range(n_max):\n",
    "                    qml.CSWAP(wires=[0, 1 + j, 1 + n_max + j])\n",
    "                qml.Hadamard(wires=0)\n",
    "                return qml.expval(qml.PauliZ(0))\n",
    "            \n",
    "            measured_overlap = swap_test_qnode(params1, params2, padding_theta, padding_phi)\n",
    "            \n",
    "            predictions.append(measured_overlap.item())\n",
    "            true_labels.append(pair['label'])\n",
    "\n",
    "    if not predictions:\n",
    "        print(\"No valid pairs in the test set to evaluate.\")\n",
    "        return\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    mse = np.mean((predictions - true_labels)**2)\n",
    "    print(f\"Test Set Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "    binary_preds = (predictions > 0.5).astype(int)\n",
    "    accuracy = np.mean(binary_preds == true_labels) * 100\n",
    "    print(f\"Test Set Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\n--- Evaluation Plots ---\")\n",
    "    plot_confusion_matrix(true_labels, predictions)\n",
    "    plot_roc_curve(true_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e94f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "data:\n",
      "  fasttext_path: C:/Users/Jash/Documents/Research/Semantic Equivilance/SemanticEquivilance/crawl-300d-2M.vec\n",
      "  path: C:/Users/Jash/Documents/Research/Semantic Equivilance/SemanticEquivilance/question_pairs/questions.csv\n",
      "  qubit_limit: 12\n",
      "  sample_fraction: 0.05\n",
      "  test_size: 0.2\n",
      "qnlp:\n",
      "  embedding_method: entangle\n",
      "  n_layers: 1\n",
      "  rewrite_rules:\n",
      "  - curry\n",
      "  - prepositional_phrase\n",
      "  - determiner\n",
      "simulation:\n",
      "  cpu_cores: 6\n",
      "  device: lightning.qubit\n",
      "training:\n",
      "  base_learning_rate: 0.001\n",
      "  epochs: 5\n",
      "  lambda_penalty: 0.1\n",
      "\n",
      "\n",
      "---> Set simulator CPU cores to 6 <---\n",
      "\n",
      "Loading FastText embeddings from C:/Users/Jash/Documents/Research/Semantic Equivilance/SemanticEquivilance/crawl-300d-2M.vec...\n",
      "Loaded 1999996 word vectors.\n",
      "Loaded 20218 sentence pairs.\n",
      "Starting preprocessing with a qubit limit of 12...\n",
      "Preprocessing complete. Found 498 valid pairs.\n",
      "Total unique parameters (symbols) found: 3973\n",
      "N_Max for the filtered dataset is: 12\n",
      "Created a balanced training set with 142 positive and 142 negative pairs.\n",
      "\n",
      "Original data split into 284 training pairs and 100 test pairs.\n",
      "--- Starting training with END-TO-END PyTorch ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     83\u001b[39m     config_file_path = \u001b[33m'\u001b[39m\u001b[33mconfig.yaml\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(config_path)\u001b[39m\n\u001b[32m     57\u001b[39m model = QNLPModel(symbols, word_to_idx, embeddings)\n\u001b[32m     58\u001b[39m device_name = config[\u001b[33m'\u001b[39m\u001b[33msimulation\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m trained_model, loss_history, param_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the initialized model\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_learning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbase_learning_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlambda_penalty\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqnlp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43membedding_method\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# --- 6. Evaluate and Plot ---\u001b[39;00m\n\u001b[32m     71\u001b[39m evaluate_model(trained_model, test_data, n_max, device_name, embedding_method=config[\u001b[33m'\u001b[39m\u001b[33mqnlp\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33membedding_method\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 214\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, data, n_max, device_name, base_learning_rate, lambda_penalty, epochs, embedding_method)\u001b[39m\n\u001b[32m    211\u001b[39m loss = fidelity_loss + structural_penalty\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# --- AUTOMATIC GRADIENT CALCULATION ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m optimizer.step()\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#Click this to run model\n",
    "# ===============================================================\n",
    "# MAIN EXECUTION BLOCK (with Configuration File)\n",
    "# ===============================================================\n",
    "def main(config_path: str):\n",
    "    \"\"\"Main function to run the entire workflow from a config file.\"\"\"\n",
    "    \n",
    "    # --- 1. Load Configuration ---\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration loaded:\")\n",
    "    print(yaml.dump(config, indent=2))\n",
    "\n",
    "    # --- Set CPU Cores for Simulator ---\n",
    "    if 'simulation' in config and 'cpu_cores' in config['simulation']:\n",
    "        num_cores = config['simulation']['cpu_cores']\n",
    "        if num_cores and num_cores > 0:\n",
    "            os.environ['OMP_NUM_THREADS'] = str(num_cores)\n",
    "            print(f\"\\n---> Set simulator CPU cores to {num_cores} <---\\n\")\n",
    "\n",
    "    # --- 2. Initialize Objects from Config ---\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(config['qnlp']['rewrite_rules'])\n",
    "    \n",
    "    N = AtomicType.NOUN\n",
    "    S = AtomicType.SENTENCE\n",
    "    OB_MAP: dict[Ty, int] = { N: 1, S: 1 }# pyright: ignore\n",
    "\n",
    "    ansatz = IQPAnsatz(OB_MAP, n_layers=config['qnlp']['n_layers'])# pyright: ignore\n",
    "\n",
    "    # --- 3. Load FastText & Preprocess Data ---\n",
    "    word_to_idx, embeddings = load_fasttext_embeddings(config['data']['fasttext_path'])\n",
    "    \n",
    "    sentences1, sentences2, value = load_data(\n",
    "        config['data']['path'], \n",
    "        sample_fraction=config['data']['sample_fraction']\n",
    "    )\n",
    "    data_pairs = list(zip(sentences1, sentences2, value))\n",
    "    \n",
    "    filtered_data, symbols, n_max = preprocess_data_for_model(\n",
    "        data_pairs, tokeniser, ansatz, parser, rewriter, \n",
    "        qubit_limit=config['data']['qubit_limit']\n",
    "    )\n",
    "    \n",
    "    if filtered_data and n_max > 0:\n",
    "        # --- 4. Create Datasets with Train/Test Split ---\n",
    "        labels = [pair['label'] for pair in filtered_data]\n",
    "        train_data_raw, test_data = train_test_split(\n",
    "            filtered_data, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        training_data = create_balanced_training_set(train_data_raw)\n",
    "        \n",
    "        print(f\"\\nOriginal data split into {len(training_data)} training pairs and {len(test_data)} test pairs.\")\n",
    "        \n",
    "        # --- 5. Initialize Model and Train ---\n",
    "        model = QNLPModel(symbols, word_to_idx, embeddings)\n",
    "        device_name = config['simulation']['device']\n",
    "        trained_model, loss_history, param_history = train_model(\n",
    "            model,  # Pass the initialized model\n",
    "            training_data, \n",
    "            n_max,\n",
    "            device_name,\n",
    "            base_learning_rate=config['training']['base_learning_rate'],\n",
    "            lambda_penalty=config['training']['lambda_penalty'],\n",
    "            epochs=config['training']['epochs'],\n",
    "            embedding_method=config['qnlp']['embedding_method']\n",
    "        )\n",
    "\n",
    "        # --- 6. Evaluate and Plot ---\n",
    "        evaluate_model(trained_model, test_data, n_max, device_name, embedding_method=config['qnlp']['embedding_method'])\n",
    "        \n",
    "        print(\"\\n--- Training Analysis ---\")\n",
    "        plot_training_history(loss_history)\n",
    "        plot_parameter_evolution_polar(param_history)\n",
    "        plot_parameter_deltas(param_history)\n",
    "    else:\n",
    "        print(\"\\nNo data to train on. Please check your config file.\")\n",
    "\n",
    "    print(\"\\nProgram finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_file_path = 'config.yaml'\n",
    "    main(config_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

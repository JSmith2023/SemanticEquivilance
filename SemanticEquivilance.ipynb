{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43923fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment ready (patched is_mixed if needed)\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import discopy\n",
    "\n",
    "# Patch missing .is_mixed (for lambeq + newer discopy)\n",
    "if not hasattr(getattr(discopy, \"monoidal\", None).Diagram, \"is_mixed\"):\n",
    "    discopy.monoidal.Diagram.is_mixed = property(lambda self: False)\n",
    "\n",
    "print(\"✅ Environment ready (patched is_mixed if needed)\")\n",
    "\n",
    "from lambeq import AtomicType\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "P = AtomicType.PREPOSITIONAL_PHRASE\n",
    "CONJ = AtomicType.CONJUNCTION\n",
    "PUNCT = AtomicType.PUNCTUATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42ac0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import AtomicType\n",
    "from lambeq import IQPAnsatz\n",
    "\n",
    "# Example: noun → 1 qubit, sentence → 1 qubit\n",
    "ob_map = dict({\n",
    "    N: 2,\n",
    "    S: 1,\n",
    "    P: 0,\n",
    "    CONJ: 1,\n",
    "    PUNCT: 0\n",
    "})\n",
    "\n",
    "ansatz = IQPAnsatz(ob_map, n_layers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a395963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 404 sentences.\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    \"\"\"Loads Question Pairs from a CSV file\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to csv_file\n",
    "        sample_fraction (float): Fraction of data to sample, default is 1.0\n",
    "    Returns:\n",
    "        tuple: A tuple containing supervised data pairs\n",
    "        returns [],[] on error\n",
    "    \"\"\"\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    is_duplicate = []\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        #print(\"Column names:\", df.columns)\n",
    "        \n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentence1_series = df['question1']\n",
    "        sentence2_series = df['question2']\n",
    "        is_duplicate_series = df['is_duplicate']\n",
    "        \n",
    "        sentences1 = sentence1_series.tolist()\n",
    "        sentences2 = sentence2_series.tolist()\n",
    "        is_duplicate = is_duplicate_series.tolist()\n",
    "        \n",
    "        if len(sentences1) != len(sentences2):\n",
    "            raise ValueError(\"The number of sentences in question1 and question2 do not match.\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(sentences1)} sentences.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Wrong Path\")\n",
    "        return [],[],[]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An {e} Error Occurred\")\n",
    "        return [],[],[]\n",
    "\n",
    "DATA_PATH = r'C:/Users/Jash\\Documents/Research\\Semantic Equivilance\\SemanticEquivilance/question_pairs/questions.csv'\n",
    "sentences1, sentences2, value = load_data(DATA_PATH, sample_fraction=0.001)\n",
    "data_pairs = list(zip(sentences1, sentences2, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7815b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "from lambeq import AtomicType, BobcatParser, Rewriter, IQPAnsatz, SpacyTokeniser\n",
    "\n",
    "\n",
    "def swap_test(state1_vec, state2_vec, num_qubits, initial_state=0):\n",
    "    \"\"\"\n",
    "    Performs a Quantum Swap Test between two quantum state vectors.\n",
    "\n",
    "    Args:\n",
    "        state1_vec (np.ndarray): The first state vector.\n",
    "        state2_vec (np.ndarray): The second state vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The estimated squared overlap (fidelity) between the two states.\n",
    "    \"\"\"\n",
    "    SHOT_COUNT = 1000000 #expected less than 1% error only done on swap test, expectation values calculated with different amount of shots\n",
    "    if 2**num_qubits != len(state1_vec):\n",
    "        raise ValueError(\"State vectors must have a length that is a power of 2.\")\n",
    "\n",
    "    total_qubits = 1 + 2 * num_qubits #1 Ancilla qubit + 2 state qubits\n",
    "\n",
    "    dev = qml.device(\"lightning.qubit\", wires=total_qubits, shots=SHOT_COUNT)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(): #|0 , psi, phi>\n",
    "        # Step 1: Prepare the ancilla qubit in a superposition\n",
    "        qml.Hadamard(wires=0)\n",
    "        print(f\"Comparing: {state1_vec} and {state2_vec}\")\n",
    "        # Step 2: Prepare the two input states\n",
    "        #basis for protocol 1\n",
    "        qml.StatePrep(state1_vec, wires=range(1, 1 + num_qubits), normalize=True)\n",
    "        #basis for protocol 1\n",
    "        qml.StatePrep(state2_vec, wires=range(1 + num_qubits, 1 + 2 * num_qubits))\n",
    "\n",
    "        # Step 3: Apply controlled-SWAP gates\n",
    "        for i in range(num_qubits):\n",
    "            qml.CSWAP(wires=[0, 1 + i, 1 + num_qubits + i]) #selects every register of phi and psi for swap\n",
    "\n",
    "        # Step 4: Apply Hadamard to ancilla\n",
    "        qml.Hadamard(wires=0)\n",
    "        # Step 5: Measure the ancilla qubit\n",
    "        return qml.sample(wires=0)\n",
    "\n",
    "    measurement_results = circuit()\n",
    "    squared_overlap = 1 - 2/len(measurement_results) * np.sum(measurement_results)\n",
    "    \n",
    "    return abs(squared_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fbb2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.pennylane import to_pennylane as to_qml\n",
    "def lambeq_sentence_to_circuit(sentence, Tokeniser, ansatz, parser, rewriter, return_type='state', include_debug_prints=False, expval = qml.PauliZ(0), target_qubits=None):\n",
    "    \"\"\"\n",
    "    Converts a natural language sentence into a quantum state vector\n",
    "    using Lambeq's BobcatParser and IQPAnsatz, handling parameterization\n",
    "    via PennyLaneModel.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        ansatz (lambeq.ansatz.Ansatz): The quantum ansatz to apply.\n",
    "        parser (lambeq.parser.Parser): The parser to convert sentence to diagram. This is a GPT\n",
    "        rewriter (lambeq.rewrite.Rewriter): The rewriter to simplify the diagram. This is a GPT\n",
    "        include_debug_prints (bool): Whether to include detailed debug prints.\n",
    "        start_basis_state (int): The basis state to initialize the qubits. Default is |0>.\n",
    "        seed (int, optional): Seed for random number generation for reproducibility.\n",
    "        return_type (str): 'state' or 'expval' to specify the return type.\n",
    "        expval (qml.Observable): The observable for expval value calculation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the state vector (np.ndarray) and\n",
    "               the number of qubits (int).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if include_debug_prints:\n",
    "            print(f\"\\n--- Debugging: Sentence '{sentence}' ---\")\n",
    "        # Step 0: Tokenize the sentence\n",
    "        tokens = Tokeniser.tokenise_sentence(sentence) # Not optional for non-clean inputs\n",
    "        \n",
    "        # Step 1: Convert sentence to a DisCoPy diagram\n",
    "        diagram = parser.sentence2diagram(tokens, tokenised=True)\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 1: Sentence parsed to diagram.\")\n",
    "\n",
    "        # Step 2: Rewrite the diagram\n",
    "        rewritten_diagram = rewriter(diagram)\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 2: Diagram rewritten.\")\n",
    "\n",
    "        # Step 3: Normalize the diagram\n",
    "        normalized_diagram = rewritten_diagram.normal_form()\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 3: Diagram normalized.\")\n",
    "            \n",
    "        # Step 4: Apply the ansatz to the normalized diagram to get a DisCoPy circuit\n",
    "        circuit = ansatz(normalized_diagram)\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 4: Ansatz applied to create DisCoPy circuit.\")\n",
    "\n",
    "        # Step 5: Convert the DisCoPy circuit to a PennyLane circuit object\n",
    "        temp_qml_circuit = to_qml(circuit)\n",
    "        \n",
    "        num_qubits = temp_qml_circuit._n_qubits\n",
    "        param_structure = temp_qml_circuit._params\n",
    "        device_qubits = target_qubits if target_qubits is not None and target_qubits >= num_qubits else num_qubits\n",
    "        \n",
    "        if include_debug_prints:\n",
    "            print(\"Step 5: DisCoPy circuit converted to PennyLane object.\")\n",
    "            print(f\"Parameter structure: {param_structure}\")\n",
    "            print(f\"Number of qubits: {num_qubits}, Device Wires: {device_qubits}\")\n",
    "        \n",
    "        # Build parameters in the exact same structure as _params\n",
    "        structured_params = []\n",
    "            \n",
    "        if include_debug_prints:\n",
    "            print(\"Step 6: Structured parameters generated.\")\n",
    "            \n",
    "        dev = qml.device(\"lightning.qubit\", wires=device_qubits)\n",
    "        \n",
    "        @qml.qnode(dev)\n",
    "        def qnode_circuit(structured_params):\n",
    "            circuit_func = temp_qml_circuit.make_circuit()\n",
    "            circuit_func(structured_params)\n",
    "            if return_type == 'expval':\n",
    "                return qml.expval(expval)\n",
    "            else:\n",
    "                return qml.state()  # Return the state vector after applying the circuit\n",
    "        \n",
    "        return qnode_circuit, param_structure, device_qubits\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Failed to process circuit: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f02b4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def initialize_structured_params(param_structure, sentence_seed=None):\n",
    "    \"\"\"\n",
    "    Initializes structured parameters based on the given parameter structure.\n",
    "\n",
    "    Args:\n",
    "        param_structure (list): The structure of parameters as a list of lists.\n",
    "        seed (int): Seed for random number generation for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of structured parameters with random values.\n",
    "    \"\"\"\n",
    "    if sentence_seed is not None:\n",
    "        random.seed(sentence_seed)\n",
    "    structured_params = []\n",
    "    for param_group in param_structure:\n",
    "        if isinstance(param_group, list) and len(param_group) > 0:\n",
    "            # This parameter group has parameters - create random values for each\n",
    "            group_values = [random.uniform(0.1, 2 * np.pi - 0.1) for _ in param_group]\n",
    "            structured_params.append(group_values)\n",
    "        else:\n",
    "            # Empty parameter group\n",
    "            structured_params.append([])\n",
    "    \n",
    "    return structured_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3256c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def FischerInformation(Fidelity): #Statisitcal Information and DISTANCE metric Fubini-Study Metric/ Wooters Distance\n",
    "    rootFidelity = math.sqrt(Fidelity) #Square root fidelity term\n",
    "    return math.acos(rootFidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be0c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(\n",
    "    overlap, \n",
    "    target_fidelity,  # <--- THIS IS YOUR GROUND TRUTH (is_duplicate: 0 or 1)\n",
    "    structural_disparity, \n",
    "    lambda_penalty\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines Fidelity MSE loss (driven by ground truth) with Structural Disparity Penalty.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Fidelity MSE Loss (Ground Truth Term)\n",
    "    # The (target_fidelity - overlap)**2 term is where the ground truth is applied.\n",
    "    # It drives the measured overlap toward the ground-truth label.\n",
    "    fidelity_loss = (target_fidelity - overlap)**2\n",
    "    \n",
    "    # 2. Structural Penalty Term\n",
    "    structural_penalty = lambda_penalty * structural_disparity\n",
    "    \n",
    "    # 3. Total Loss\n",
    "    return fidelity_loss + structural_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab15ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(qnode_func, params1, params2):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between two quantum states prepared by the same QNode\n",
    "    but with different parameters. This is a shortcut that only works with quantum simulators, for real quantum hardware, use the swap test function.\n",
    "\n",
    "    Args:\n",
    "        qnode_func (qml.QNode): The quantum circuit QNode.\n",
    "        params1 (list): Structured parameters for the first state.\n",
    "        params2 (list): Structured parameters for the second state.\n",
    "\n",
    "    Returns:\n",
    "        float: The estimated squared overlap (fidelity) between the two states.\n",
    "    \"\"\"\n",
    "    # Get the state vectors for both sets of parameters\n",
    "    state1 = qnode_func(params1)\n",
    "    state2 = qnode_func(params2)\n",
    "    \n",
    "    # Calculate the overlap\n",
    "    overlap = np.abs(np.vdot(state1, state2))**2\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a3d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_gradients(qnode_func, params, s1, s2, target_fidelity, structural_disparity):\n",
    "    \"\"\"\n",
    "    Calculates gradients for a pair of sentences with respect to the shared parameters.\n",
    "    Returns a dictionary of gradients indexed by (group, element) tuple.\n",
    "    \"\"\"\n",
    "    gradients = {idx: 0.0 for idx in [(g, e) for g in range(len(params)) for e in range(len(params[g]))]}\n",
    "\n",
    "    # We need to compute the gradient for each parameter\n",
    "    for group_idx, group in enumerate(params):\n",
    "        for elem_idx in range(len(group)):\n",
    "            param_index = (group_idx, elem_idx)\n",
    "\n",
    "            # Shift the parameters for both sentences in the pair\n",
    "            params_plus_s1 = params.copy()\n",
    "            params_plus_s1[group_idx][elem_idx] += np.pi / 2\n",
    "            \n",
    "            params_minus_s1 = params.copy()\n",
    "            params_minus_s1[group_idx][elem_idx] -= np.pi / 2\n",
    "\n",
    "            # Overlap for shifted s1\n",
    "            overlap_plus = calculate_overlap(qnode_func, params_plus_s1, params)\n",
    "            overlap_minus = calculate_overlap(qnode_func, params_minus_s1, params)\n",
    "\n",
    "            # Gradient contribution from S1\n",
    "            grad_s1 = (loss_function(overlap_plus, target_fidelity, structural_disparity, 0) - loss_function(overlap_minus, target_fidelity, structural_disparity, 0)) / 2\n",
    "\n",
    "            # Now, shift the parameters for the second sentence (S2)\n",
    "            params_plus_s2 = params.copy()\n",
    "            params_plus_s2[group_idx][elem_idx] += np.pi / 2\n",
    "            \n",
    "            params_minus_s2 = params.copy()\n",
    "            params_minus_s2[group_idx][elem_idx] -= np.pi / 2\n",
    "\n",
    "            # Overlap for shifted S2\n",
    "            overlap_plus_2 = calculate_overlap(qnode_func, params, params_plus_s2)\n",
    "            overlap_minus_2 = calculate_overlap(qnode_func, params, params_minus_s2)\n",
    "\n",
    "            # Gradient contribution from S2\n",
    "            grad_s2 = (loss_function(overlap_plus_2, target_fidelity, structural_disparity, 0) - loss_function(overlap_minus_2, target_fidelity, structural_disparity, 0)) / 2\n",
    "            \n",
    "            # The total gradient is the sum of contributions from both states\n",
    "            gradients[param_index] = grad_s1 + grad_s2\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_qubits_for_dataset(data_pairs, Tokeniser, ansatz, parser, rewriter):\n",
    "    \"\"\"Calculates the maximum number of qubits required by any sentence in the dataset.\"\"\"\n",
    "    print(\"Determining maximum qubit count for the entire dataset...\")\n",
    "    max_qubits = 0\n",
    "    # Use the original (unmodified) lambeq_sentence_to_circuit to get the *required* qubits\n",
    "    for s1, s2, _ in data_pairs:\n",
    "        # Note: We call a temporary version of the circuit builder that is not padded.\n",
    "        # It's an internal detail, but for simplicity, we assume a helper can get the required wires.\n",
    "        # Here, we will just use the unpadded version's output for the required number of qubits.\n",
    "        _, _, required_qubits_s1 = lambeq_sentence_to_circuit(\n",
    "            s1, Tokeniser, ansatz, parser, rewriter, return_type='state'\n",
    "        )\n",
    "        _, _, required_qubits_s2 = lambeq_sentence_to_circuit(\n",
    "            s2, Tokeniser, ansatz, parser, rewriter, return_type='state'\n",
    "        )\n",
    "        \n",
    "        if required_qubits_s1 is not None and required_qubits_s1 > max_qubits:\n",
    "            max_qubits = required_qubits_s1\n",
    "        if required_qubits_s2 is not None and required_qubits_s2 > max_qubits:\n",
    "            max_qubits = required_qubits_s2\n",
    "            \n",
    "    print(f\"Maximum required qubits (N_max) across the dataset is: {max_qubits}\")\n",
    "    return max_qubits\n",
    "history = [] #for plots\n",
    "#Hyperparameters\n",
    "BASE_LEARNING_RATE = 0.01\n",
    "LAMDA_PENALTY = 0.1  # Weight for the structural disparity penalty\n",
    "\n",
    "def main_workflow(data_pairs, parser, rewriter, ansatz, Tokeniser, base_learning_rate=BASE_LEARNING_RATE, lambda_penalty=LAMDA_PENALTY):\n",
    "    \n",
    "    # 1. Preprocessing: Determine the global maximum qubit count\n",
    "    N_MAX = get_max_qubits_for_dataset(data_pairs, Tokeniser, ansatz, parser, rewriter)\n",
    "    if N_MAX == 0:\n",
    "        print(\"No valid sentences found to train on. Exiting.\")\n",
    "        return {}\n",
    "        \n",
    "        \n",
    "    #Store relevent metadata (target-fidelity, structural disparity) for each pair\n",
    "    processed_pairs = []\n",
    "    for s1, s2, is_duplicate in data_pairs:\n",
    "        _, _, N1 = lambeq_sentence_to_circuit(s1,Tokeniser,ansatz,parser,rewriter,return_type='state')\n",
    "        _, _, N2 = lambeq_sentence_to_circuit(s2,Tokeniser,ansatz,parser,rewriter,return_type='state')\n",
    "        \n",
    "        #calculate structural disparity (absolute qubit difference)\n",
    "        structural_disparity = abs(N1 - N2) if N1 is not None and N2 is not None else N_MAX\n",
    "        processed_pairs.append((s1, s2, is_duplicate, structural_disparity))\n",
    "        \n",
    "    print(f\"\\n--- Starting Training on Dimension N_MAX = {N_MAX} ---\")\n",
    "    \n",
    "    # 2. Model Initialization\n",
    "    rep_sentence = data_pairs[0][0] # Use first sentence to get parameter structure\n",
    "    \n",
    "    # The QNode will now be set to size N_MAX for the entire training\n",
    "    qnode_func, param_structure, device_qubits = lambeq_sentence_to_circuit(\n",
    "        rep_sentence, Tokeniser, ansatz, parser, rewriter, \n",
    "        return_type='state', target_qubits=N_MAX # <--- Use N_MAX for device\n",
    "    )\n",
    "    \n",
    "    # Check if circuit generation succeeded\n",
    "    if qnode_func is None or device_qubits != N_MAX:\n",
    "        print(\"Failed to generate representative circuit. Exiting.\")\n",
    "        return {}\n",
    "        \n",
    "    # 3. Initialize a single set of parameters for the N_MAX dimension\n",
    "    params = initialize_structured_params(param_structure)\n",
    "    \n",
    "    # 4. Training Loop with Dynamic Feedback\n",
    "    for s1, s2, target_fidelity, structural_disparity in processed_pairs:\n",
    "        #overlap\n",
    "        overlap = calculate_overlap(qnode_func, params, params)\n",
    "        qangle = FischerInformation(overlap)\n",
    "        normalized_qangle = qangle / (math.pi / 2)\n",
    "        if target_fidelity == 1:\n",
    "            eta_dynamic = base_learning_rate * normalized_qangle\n",
    "        else:\n",
    "            eta_dynamic = base_learning_rate * (1 - normalized_qangle)\n",
    "        # Calculate gradients using the shared QNode (qnode_func is already set to N_MAX)\n",
    "        gradients = calculate_pair_gradients(qnode_func, params, s1, s2, target_fidelity, structural_disparity)\n",
    "        \n",
    "        # Apply the update\n",
    "        for group_idx, group in enumerate(params):\n",
    "            for elem_idx in range(len(group)):\n",
    "                param_index = (group_idx, elem_idx)\n",
    "                params[group_idx][elem_idx] -= eta_dynamic * gradients[param_index]\n",
    "        \n",
    "        # Logging for monitoring\n",
    "        total_loss = loss_function(overlap, target_fidelity, structural_disparity, lambda_penalty)\n",
    "        history.append({\n",
    "            'step': len(history) + 1, # Use +1 since len(history) is the index of the next item\n",
    "            's1': s1,\n",
    "            's2': s2,\n",
    "            'target_fidelity': target_fidelity,\n",
    "            'structural_disparity': structural_disparity,\n",
    "            'fidelity': overlap,\n",
    "            'q_angle': qangle,\n",
    "            'eta_dyn': eta_dynamic,\n",
    "            'total_loss': total_loss\n",
    "        })\n",
    "    trained_params = {N_MAX: params} # Store the single trained model\n",
    "    \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    \n",
    "    # 5. Final Evaluation with Swap Test\n",
    "    print(\"\\n--- Performing Final Swap Tests with Trained Parameters ---\")\n",
    "    # Sample a few pairs for evaluation\n",
    "    sample_pairs = data_pairs[:5] if len(processed_pairs) > 5 else processed_pairs\n",
    "    \n",
    "    for s1, s2, target_fidelity, structural_disparity in sample_pairs:\n",
    "        print(f\"\\nEvaluating Pair: '{s1}' vs. '{s2}' (Label: {target_fidelity}, Disparity: {structural_disparity}\")\n",
    "        \n",
    "        optimized_params = trained_params[N_MAX]\n",
    "        \n",
    "        # Generate State 1 and State 2 vectors, both will be padded to N_MAX\n",
    "        qnode1, _, _ = lambeq_sentence_to_circuit(s1, Tokeniser, ansatz, parser, rewriter, return_type='state', target_qubits=N_MAX)\n",
    "        qnode2, _, _ = lambeq_sentence_to_circuit(s2, Tokeniser, ansatz, parser, rewriter, return_type='state', target_qubits=N_MAX)\n",
    "        \n",
    "        vec1 = qnode1(optimized_params)\n",
    "        vec2 = qnode2(optimized_params)\n",
    "\n",
    "        # Call swap_test, passing N_MAX as the number of qubits for *one* register\n",
    "        overlap = swap_test(vec1, vec2, N_MAX) \n",
    "        Qangle = FischerInformation(overlap)\n",
    "        normalized_qangle = Qangle / (math.pi / 2)\n",
    "        \n",
    "        #Calculate final loss\n",
    "        final_loss = loss_function(overlap, target_fidelity, structural_disparity, lambda_penalty)\n",
    "        #Determine dynamic learning rate based on Qangle\n",
    "        print(f\"Final Overlap (Fidelity): {overlap:.4f}\")\n",
    "        print(f\"Estimated Quantum Angle (Normalized): {normalized_qangle:.4f}\")\n",
    "        print(f\"Total Penalized Loss: {final_loss:.4f}\")\n",
    "    return trained_params, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5456e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Generates two key plots showing VQC training convergence and feedback.\"\"\"\n",
    "    \n",
    "    if not history:\n",
    "        print(\"History list is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    # Extracting data from history list\n",
    "    steps = np.array([d['step'] for d in history])\n",
    "    losses = np.array([d['total_loss'] for d in history])\n",
    "    q_angles = np.array([d['q_angle'] for d in history])\n",
    "    eta_dyn = np.array([d['eta_dyn'] for d in history])\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # PLOT 1: Loss Convergence and Dynamic Learning Rate\n",
    "    # -------------------------------------------------------------\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot Loss (Left Y-Axis)\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Training Step (Pair)')\n",
    "    ax1.set_ylabel('Total Penalized Loss', color=color)\n",
    "    ax1.plot(steps, losses, color=color, label='Total Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_yscale('log') # Use log scale for loss for better visualization\n",
    "\n",
    "    # Plot Dynamic Learning Rate (Right Y-Axis)\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel(f'Dynamic Learning Rate (Max={BASE_LEARNING_RATE:.2f})', color=color)  \n",
    "    ax2.plot(steps, eta_dyn, color=color, linestyle='--', alpha=0.6, label='$\\eta_{dyn}$')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    plt.title('VQC Training Convergence and Dynamic Feedback')\n",
    "    fig.tight_layout() \n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.savefig('vqc_convergence_and_eta.png')\n",
    "    plt.close(fig)\n",
    "    print(\"Saved plot: vqc_convergence_and_eta.png\")\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # PLOT 2: Quantum Angle Evolution (Geometric Feedback)\n",
    "    # -------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.set_xlabel('Training Step (Pair)')\n",
    "    ax.set_ylabel('Quantum Angle $\\Theta$ (Fubini-Study Distance)')\n",
    "    \n",
    "    # The Quantum Angle is in [0, pi/2] radians\n",
    "    ax.plot(steps, q_angles, color='tab:green', label='Quantum Angle')\n",
    "    \n",
    "    # Add target lines (This is a simplified view)\n",
    "    ax.axhline(0, color='red', linestyle=':', linewidth=1.5, label='Target Angle (Duplicate)')\n",
    "    ax.axhline(np.pi/2, color='orange', linestyle=':', linewidth=1.5, label='Target Angle (Non-Duplicate)')\n",
    "    \n",
    "    plt.title('Evolution of Quantum Angle (Geometric Distance)')\n",
    "    plt.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.savefig('quantum_angle_evolution.png')\n",
    "    plt.close(fig)\n",
    "    print(\"Saved plot: quantum_angle_evolution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating states from sentences ---\n",
      "Error processing sentence 'Alice loves the dog that Bob purchased.': too many values to unpack (expected 2)\n",
      "Error processing sentence 'Bob loves the dog that Alice sold.': too many values to unpack (expected 2)\n",
      "Error processing sentence 'The big cat sleeps peacefully.': too many values to unpack (expected 2)\n",
      "Error processing sentence 'The small bird sings loudly.': too many values to unpack (expected 2)\n",
      "Error processing sentence 'The lizard basks in the sun.': too many values to unpack (expected 2)\n",
      "Error processing sentence 'The sun shines on the lizard': too many values to unpack (expected 2)\n",
      "\n",
      "--- Performing Swap Tests ---\n",
      "No valid multi-qubit states generated. Cannot perform Swap Tests meaningfully.\n"
     ]
    }
   ],
   "source": [
    "#OLD FUNC DONT USE (FOR REFERENCE ONLY)\n",
    "import spacy\n",
    "if __name__ == \"__main__\":\n",
    "    spacy.load('en_core_web_sm')\n",
    "    Tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(['curry', 'prepositional_phrase', 'determiner'])\n",
    "    # Increase ansatz parameters to get more complex quantum states\n",
    "    # ansatz = StronglyEntanglingAnsatz(\n",
    "    # {AtomicType.NOUN: 2, AtomicType.SENTENCE: 1}, \n",
    "    # n_layers=1\n",
    "    ansatz = IQPAnsatz({AtomicType.NOUN: 2, AtomicType.SENTENCE: 1, AtomicType.CONJUNCTION: 1}, n_layers=1\n",
    ")\n",
    "\n",
    "    print(\"--- Generating states from sentences ---\")\n",
    "\n",
    "    sentence1 = \"Alice loves the dog that Bob purchased.\"\n",
    "    sentence2 = \"Bob loves the dog that Alice sold.\"\n",
    "    sentence3 = \"The big cat sleeps peacefully.\"\n",
    "    sentence4 = \"The small bird sings loudly.\"\n",
    "    sentence5 = \"The lizard basks in the sun.\"\n",
    "    sentence6 = \"The sun shines on the lizard\"\n",
    "    \n",
    "    sentences = [sentence1, sentence2, sentence3, sentence4, sentence5, sentence6]\n",
    "    state_data = {}\n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        try:\n",
    "            state_vec, num_qubits = lambeq_sentence_to_circuit(sentence, Tokeniser, ansatz, parser, rewriter)\n",
    "            #state_vec, num_qubits = lambeq_sentence_to_state_vector(sentence, ansatz, parser, rewriter, include_debug_prints=False)\n",
    "            state_data[sentence] = (state_vec, num_qubits)\n",
    "            print(f\"Sentence {s_idx+1}: '{sentence}'\")\n",
    "            print(f\"Generated state with {num_qubits} qubits\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence '{sentence}': {e}\")\n",
    "            state_data[sentence] = (None, None)\n",
    "\n",
    "    print(\"\\n--- Performing Swap Tests ---\")\n",
    "    \n",
    "    # Filter out sentences that did not produce valid states\n",
    "    valid_sentences = [s for s in sentences if state_data[s][0] is not None and state_data[s][1] is not None and state_data[s][1] > 0]\n",
    "\n",
    "    if not valid_sentences:\n",
    "        print(\"No valid multi-qubit states generated. Cannot perform Swap Tests meaningfully.\")\n",
    "    else:\n",
    "        first_num_qubits = state_data[valid_sentences[0]][1]\n",
    "        all_same_qubits = all(state_data[s][1] == first_num_qubits for s in valid_sentences)\n",
    "\n",
    "        if not all_same_qubits:\n",
    "            print(\"\\nWarning: Not all valid sentences resulted in circuits with the same number of qubits.\")\n",
    "            print(\"Swap Test requires states to have the same number of qubits.\")\n",
    "            print(\"Pairs with different qubit counts will be skipped.\")\n",
    "            for s in valid_sentences:\n",
    "                print(f\"  '{s}': {state_data[s][1]} qubits\")\n",
    "\n",
    "        for i in range(len(valid_sentences)):\n",
    "            for j in range(i, len(valid_sentences)):\n",
    "                s1 = valid_sentences[i]\n",
    "                s2 = valid_sentences[j]\n",
    "\n",
    "                vec1, nq1 = state_data[s1]\n",
    "                vec2, nq2 = state_data[s2]\n",
    "\n",
    "                if nq1 == nq2:\n",
    "                    print(f\"\\nSwap Test between '{s1}' and '{s2}':\")\n",
    "                    # Fix: Use nq1 (or nq2, they're equal)\n",
    "                    overlap = swap_test(vec1, vec2, nq1)\n",
    "                    print(overlap)\n",
    "                    Qangle = FischerInformation(overlap) # 0 - pi/2\n",
    "                    \n",
    "                    print(f\"Estimated Quantum Angle: {Qangle:.4f}\")\n",
    "                    if s1 == s2:\n",
    "                        print(\" (Expected to be close to 1.0 for identical states)\")\n",
    "                else:\n",
    "                    print(f\"\\nSkipping Swap Test between '{s1}' ({nq1} qubits) and '{s2}' ({nq2} qubits) due to different qubit counts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e385c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining maximum qubit count for the entire dataset...\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'A Rs . 5000 item can be insured for its total value by paying a premium of Rs . N. If the probability of theft in a given year is estimated to be .01 , what premium should the insurance company charge if it wants the expected gain to be equal to Rs . 1000 ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'Is there a St. Marie Adolphine Dierks ? How did she become a saint and what was her contribution to the faith ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'Is there a St. Marie Trezelle ? How did she become a saint and what was her contribution to the faith ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'What is it like to be homeless in Algeria ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'Which is the best smartphone I can buy under Rs.6000 ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'Which is the best phone to buy under Rs.6000 ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse 'I am trying to become a math scholar . What books do you recommend ?'.\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse \"Why do n't people like Nickelback ?\".\n",
      "CRITICAL ERROR: Failed to process circuit: Bobcat failed to parse \"Why do n't people dislike Islam ?\".\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import spacy\n",
    "    spacy.load('en_core_web_sm')\n",
    "    Tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(['curry', 'prepositional_phrase', 'determiner'])\n",
    "    # ansatz = StronglyEntanglingAnsatz(\n",
    "    # {AtomicType.NOUN: 2, AtomicType.SENTENCE: 1}, \n",
    "    # n_layers=1 \n",
    "    trained_params, history = main_workflow(data_pairs, parser, rewriter, ansatz, Tokeniser)\n",
    "    plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

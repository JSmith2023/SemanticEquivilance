{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36013460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import yaml\n",
    "import os #multithreading\n",
    "\n",
    "import fasttext\n",
    "\n",
    "# PennyLane and PyTorch\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lambeq\n",
    "from lambeq.backend.quantum import Diagram as LambeqDiagram\n",
    "from discopy.quantum import gates\n",
    "import spacy\n",
    "import discopy\n",
    "from lambeq import BobcatParser, Rewriter, SpacyTokeniser, AtomicType, StronglyEntanglingAnsatz\n",
    "from discopy.rigid import Ty\n",
    "\n",
    "#data handling and plotting\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Patch for discopy\n",
    "monoidal_module = getattr(discopy, \"monoidal\", None)\n",
    "if monoidal_module:\n",
    "    diagram_class = getattr(monoidal_module, \"Diagram\", None)\n",
    "    if diagram_class and not hasattr(diagram_class, \"is_mixed\"):\n",
    "        diagram_class.is_mixed = property(lambda self: False)\n",
    "\n",
    "# Load spacy model\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading Function\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    sentences1, sentences2, is_duplicate = [], [], []\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentences1 = df['question1'].astype(str).tolist()\n",
    "        sentences2 = df['question2'].astype(str).tolist()\n",
    "        is_duplicate = df['is_duplicate'].tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(sentences1)} sentence pairs.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "def create_balanced_training_set(training_data: list) -> list:\n",
    "    \"\"\"Creates a balanced training set by undersampling the majority class.\"\"\"\n",
    "    positives = [pair for pair in training_data if pair['label'] == 1]\n",
    "    negatives = [pair for pair in training_data if pair['label'] == 0]\n",
    "    \n",
    "    # Undersample the larger class to match the size of the smaller class\n",
    "    if len(positives) > len(negatives):\n",
    "        positives = random.sample(positives, len(negatives))\n",
    "    else:\n",
    "        negatives = random.sample(negatives, len(positives))\n",
    "    \n",
    "    balanced_train_set = positives + negatives\n",
    "    random.shuffle(balanced_train_set)\n",
    "    \n",
    "    print(f\"Created a balanced training set with {len(positives)} positive and {len(negatives)} negative pairs.\")\n",
    "    return balanced_train_set\n",
    "def load_fasttext_model(model_path: str):\n",
    "    \"\"\"Loads the full FastText .bin model.\"\"\"\n",
    "    print(f\"Loading FastText model from {model_path}...\")\n",
    "    model = fasttext.load_model(model_path)\n",
    "    print(\"FastText model loaded successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "def plot_training_history(history: dict):\n",
    "    \"\"\"Plots the training loss and the average fidelity over epochs.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='Training Loss (Local Cost)')\n",
    "    if 'avg_fidelity' in history and history['avg_fidelity']:\n",
    "        plt.plot(history['avg_fidelity'], label='Avg. Fidelity (from SWAP Test)', linestyle='--')\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_parameter_history(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(len(param_history))\n",
    "    means = [d['mean'] for d in param_history]\n",
    "    stds = [d['std'] for d in param_history]\n",
    "    mins = [d['min'] for d in param_history]\n",
    "    maxs = [d['max'] for d in param_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, means, label='Mean Parameter Value')\n",
    "    plt.fill_between(epochs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.2, label='1 Std. Deviation')\n",
    "    plt.plot(epochs, mins, linestyle='--', color='gray', label='Min/Max Range')\n",
    "    plt.plot(epochs, maxs, linestyle='--', color='gray')\n",
    "    \n",
    "    plt.title('Evolution of Model Parameters During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_evolution_polar(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "    epochs = np.array(range(len(param_history)))\n",
    "    \n",
    "    mean_angles = np.array([d['mean'] for d in param_history]) % (4 * np.pi)\n",
    "\n",
    "    ax.plot(mean_angles, epochs, 'o-', label='Mean Parameter Path')\n",
    "\n",
    "    if len(epochs) > 0:\n",
    "        ax.plot(mean_angles[0], epochs[0], 'gX', markersize=12, label='Start')\n",
    "        ax.plot(mean_angles[-1], epochs[-1], 'rX', markersize=12, label='End')\n",
    "\n",
    "    ax.set_theta_zero_location('N')# pyright: ignore\n",
    "    ax.set_theta_direction(-1)# pyright: ignore\n",
    "    ax.set_rlabel_position(0)# pyright: ignore\n",
    "    ax.set_rlim(0, len(epochs) * 1.05)# pyright: ignore\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_title('Cyclical Evolution of Mean Parameter', pad=20)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "def plot_parameter_deltas(param_history):\n",
    "    if len(param_history) < 2:\n",
    "        print(\"Need at least 2 epochs to plot parameter deltas.\")\n",
    "        return\n",
    "\n",
    "    mean_angles = np.array([d['mean'] for d in param_history])\n",
    "    \n",
    "    # Calculate the shortest angle difference between each epoch\n",
    "    deltas = []\n",
    "    for i in range(1, len(mean_angles)):\n",
    "        prev_angle = mean_angles[i-1]\n",
    "        curr_angle = mean_angles[i]\n",
    "        delta = np.arctan2(np.sin(curr_angle - prev_angle), np.cos(curr_angle - prev_angle))\n",
    "        deltas.append(delta)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # We plot against epochs 1 to N, since the first delta occurs at epoch 1\n",
    "    plt.plot(range(1, len(mean_angles)), deltas, 'o-', label='Change in Mean Parameter (Delta)')\n",
    "    \n",
    "    plt.axhline(0, color='red', linestyle='--', label='No Change')\n",
    "    plt.title('Epoch-to-Epoch Change in Mean Parameter Value')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Shortest Angle Difference (Radians)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, len(mean_angles)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes and plots a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): The ground-truth labels (0s and 1s).\n",
    "        y_pred (np.array): The model's raw probability predictions (overlaps from 0 to 1).\n",
    "        threshold (float): The cutoff for classifying a prediction as 1.\n",
    "    \"\"\"\n",
    "    # Convert probability predictions to binary 0/1 predictions\n",
    "    binary_preds = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, binary_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Non-Duplicate', 'Predicted Duplicate'],\n",
    "                yticklabels=['Actual Non-Duplicate', 'Actual Duplicate'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes and plots the ROC curve and AUC score.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNLP MODEL AND TRAINING PIPELINE\n",
    "def execute_discopy_diagram(current_width, diagram, params, wires, embedding_method='simple_pad', rotation_param=None, entangling_param=None):\n",
    "    \"\"\"Executes a DisCoPy/lambeq diagram's instructions, including padding.\"\"\"\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "    ancilla_wires = wires[current_width:]\n",
    "    if embedding_method == 'parameterized':\n",
    "        if rotation_param is not None:\n",
    "            for w in ancilla_wires: qml.RY(rotation_param, wires=w)\n",
    "        if entangling_param is not None and len(ancilla_wires) > 1:\n",
    "            for i in range(len(ancilla_wires)):\n",
    "                qml.CPHASE(entangling_param, wires=[ancilla_wires[i], ancilla_wires[(i + 1) % len(ancilla_wires)]])\n",
    "\n",
    "def execute_discopy_diagram_local(diagram, params, wires):\n",
    "    \"\"\"Executes a DisCoPy/lambeq diagram without padding for local measurements.\"\"\"\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "def get_diagram_width(diagram):\n",
    "    \"\"\"Calculates the true maximum width of a diagram at any point.\"\"\"\n",
    "    if not diagram.boxes:\n",
    "        return len(diagram.cod)\n",
    "    # The width is the maximum wire index a box acts on.\n",
    "    return max(\n",
    "        [offset + len(box.dom) for box, offset in zip(diagram.boxes, diagram.offsets)]\n",
    "        + [len(diagram.cod)]\n",
    "    )\n",
    "    \n",
    "def preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    \"\"\"\n",
    "    Preprocesses data, retrying with an implicit \"You\" if a Ty(p) error occurs.\n",
    "    \"\"\"\n",
    "    print(f\"Starting preprocessing with qubit limit {qubit_limit}...\")\n",
    "    filtered_pairs, all_symbols, n_max = [], set(), 0\n",
    "    N = AtomicType.NOUN\n",
    "\n",
    "    for i, (s1, s2, is_duplicate) in enumerate(data_pairs):\n",
    "        processed_successfully = False\n",
    "        try:\n",
    "            # First attempt\n",
    "            d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "            processed_successfully = True # Mark success if no exception\n",
    "\n",
    "        except Exception as e:\n",
    "            # Check if the error is the specific Ty(p) issue\n",
    "            # Note: The exact error message or type might vary, adjust if needed\n",
    "            error_str = str(e)\n",
    "            is_type_p_error = \"Ty(p)\" in error_str or \"'Ty' object has no attribute 'label'\" in error_str # Check for both potential messages\n",
    "\n",
    "            if is_type_p_error:\n",
    "                print(f\"Warning: Pair #{i+1} failed with Ty(p) related error. Retrying with implicit 'You'.\")\n",
    "                try:\n",
    "                    # Modify sentence(s) and retry processing\n",
    "                    # Decide if only one or both sentences need modification based on error details if possible\n",
    "                    # For simplicity, let's retry both if either fails with Ty(p)\n",
    "                    s1_modified = \"You \" + s1\n",
    "                    s2_modified = \"You \" + s2\n",
    "\n",
    "                    d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1_modified), tokenised=True)))\n",
    "                    d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2_modified), tokenised=True)))\n",
    "                    processed_successfully = True # Mark success on retry\n",
    "                    print(f\"  --> Retry successful for pair #{i+1}.\")\n",
    "\n",
    "                except Exception as e_retry:\n",
    "                    # If retry also fails, log the second error\n",
    "                    print(f\"Warning: Retry failed for pair #{i+1}. Reason: {e_retry}\")\n",
    "                    continue # Skip this pair\n",
    "            else:\n",
    "                # If it's a different error (e.g., Bobcat parse failure), log it and skip\n",
    "                print(f\"Warning: Failed to process pair #{i+1}. Reason: {e}\")\n",
    "                continue # Skip this pair\n",
    "\n",
    "        # If either the first attempt or the retry was successful:\n",
    "        if processed_successfully:\n",
    "            try:\n",
    "                # Proceed with width calculation and filtering using the successful diagrams\n",
    "                width1 = len(d1.cod) # Use simple len(cod) with ansatz\n",
    "                width2 = len(d2.cod)\n",
    "\n",
    "                if width1 <= qubit_limit and width2 <= qubit_limit:\n",
    "                    pair_data = {'s1': s1, 's2': s2, 'label': is_duplicate, 'd1': d1, 'd2': d2,\n",
    "                                 'structural_disparity': abs(width1 - width2),\n",
    "                                 'width1': width1, 'width2': width2}\n",
    "                    filtered_pairs.append(pair_data)\n",
    "                    all_symbols.update(d1.free_symbols)\n",
    "                    all_symbols.update(d2.free_symbols)\n",
    "                    n_max = max(n_max, width1, width2)\n",
    "                # else: # Optional: Log if filtered out due to qubit limit after successful parse\n",
    "                #     print(f\"Note: Pair #{i+1} successfully parsed but exceeded qubit limit ({width1}, {width2})\")\n",
    "\n",
    "            except Exception as e_post_process:\n",
    "                # Catch potential errors in width calculation or symbol extraction after successful parse/retry\n",
    "                print(f\"Warning: Error after processing pair #{i+1}. Reason: {e_post_process}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    print(f\"Total unique parameters (symbols) found: {len(all_symbols)}\")\n",
    "    print(f\"N_Max for the filtered dataset is: {n_max}\")\n",
    "    return filtered_pairs, sorted(list(all_symbols), key=lambda s: s.name), n_max\n",
    "\n",
    "class QNLPModel(nn.Module):\n",
    "    \"\"\"Generates the exact number of parameters required by the diagram.\"\"\"\n",
    "    def __init__(self, symbols, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        # We need a way to map the averaged sentence vector to a variable number of params.\n",
    "        # Let's use a simple MLP instead of just Linear. Kharti's method\n",
    "        # Outputting a large fixed size, we will slice later if needed, but based on symbols count.\n",
    "        # Max possible symbols could be large, let's estimate or find max_symbols needed across dataset.\n",
    "        # For now, let's assume a reasonable max (e.g., 50 parameters typical?)\n",
    "        # A better approach might be needed if this max is too small or too large.\n",
    "        max_params_heuristic = 50 # Adjust as needed\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim // 2, max_params_heuristic)\n",
    "        )\n",
    "        self.symbol_to_word = {s.name.replace('.', '_'): s.name.split('_')[-1] for s in symbols}\n",
    "\n",
    "    def forward(self, diagram, fasttext_model):\n",
    "        symbols = diagram.free_symbols\n",
    "        num_required_params = len(symbols)\n",
    "        \n",
    "        if not symbols or num_required_params == 0:\n",
    "            return torch.tensor([]) # Return empty tensor if no params needed\n",
    "\n",
    "        words = [self.symbol_to_word.get(s.name.replace('.', '_')) for s in symbols]\n",
    "        vectors = [fasttext_model.get_word_vector(w) for w in words if w]\n",
    "        \n",
    "        if not vectors:\n",
    "            # If no words found, return zeros matching required params\n",
    "            return torch.zeros(num_required_params)\n",
    "\n",
    "        sentence_vector = torch.tensor(vectors, dtype=torch.float32).mean(dim=0)\n",
    "        \n",
    "        # Generate a large fixed set of potential parameters\n",
    "        all_params = self.encoder(sentence_vector)\n",
    "        \n",
    "        # Slice exactly the number needed for this diagram\n",
    "        if num_required_params > all_params.shape[0]:\n",
    "             # Handle cases needing more params than our heuristic max\n",
    "             # This might indicate the heuristic needs increasing or a different strategy\n",
    "             print(f\"Warning: Diagram requires {num_required_params} params, but model only outputs {all_params.shape[0]}. Truncating.\")\n",
    "             return all_params # Or handle error appropriately\n",
    "        \n",
    "        quantum_params = all_params[:num_required_params]\n",
    "        return quantum_params\n",
    "\n",
    "def train_model(model, fasttext_model, training_data, validation_data, n_max, device_name,\n",
    "                base_learning_rate, lambda_penalty, epochs, n_layers, embedding_method):\n",
    "    \"\"\"Trains with a local cost function and logs SWAP test fidelity.\"\"\"\n",
    "    optimizer = Adam(model.parameters(), lr=base_learning_rate)\n",
    "    dev = qml.device(device_name, wires=n_max)\n",
    "    swap_dev = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'avg_fidelity': [], 'param_history': []}\n",
    "    print(\"--- Starting training with LOCAL cost function ---\")\n",
    "\n",
    "    @qml.qnode(dev, interface=\"torch\")\n",
    "    def local_expval_qnode(params, diagram, num_qubits):\n",
    "        execute_discopy_diagram_local(diagram, params, wires=range(num_qubits))\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_fidelity, num_trained_pairs = 0, 0, 0\n",
    "        for pair in training_data:\n",
    "            optimizer.zero_grad()\n",
    "            params1 = model(pair['d1'], fasttext_model)\n",
    "            params2 = model(pair['d2'], fasttext_model)\n",
    "            \n",
    "            if params1.nelement() == 0 or params2.nelement() == 0:\n",
    "                print(f\"Skipping pair with no parameters needed.\\n\")\n",
    "                continue  # Skip pairs where no parameters are needed (highly probability of being an error case)\n",
    "            \n",
    "            exp_vals1 = torch.stack(local_expval_qnode(params1, pair['d1'], pair['width1']))\n",
    "            exp_vals2 = torch.stack(local_expval_qnode(params2, pair['d2'], pair['width2']))\n",
    "            \n",
    "            if pair['width1'] < n_max: exp_vals1 = torch.cat([exp_vals1, torch.zeros(n_max - pair['width1'])])\n",
    "            if pair['width2'] < n_max: exp_vals2 = torch.cat([exp_vals2, torch.zeros(n_max - pair['width2'])])\n",
    "\n",
    "            mse = torch.mean((exp_vals1 - exp_vals2)**2)\n",
    "            target_distance = 1 - pair['label']\n",
    "            local_loss = (mse - target_distance)**2\n",
    "            structural_penalty = torch.tensor(lambda_penalty * pair['structural_disparity'])\n",
    "            loss = local_loss + structural_penalty\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                @qml.qnode(swap_dev, interface=\"torch\")\n",
    "                def swap_test_qnode(p1, p2):\n",
    "                    qml.Hadamard(wires=0)\n",
    "                    # Note: These dummy thetas/phis are for the parameterized padding method\n",
    "                    dummy_theta = torch.tensor(0.1); dummy_phi = torch.tensor(0.1)\n",
    "                    execute_discopy_diagram(pair['width1'], pair['d1'], p1, range(1, 1+n_max), embedding_method, dummy_theta, dummy_phi)\n",
    "                    execute_discopy_diagram(pair['width2'], pair['d2'], p2, range(1+n_max, 1+2*n_max), embedding_method, dummy_theta, dummy_phi)\n",
    "                    for j in range(n_max): qml.CSWAP(wires=[0, 1+j, 1+n_max+j])\n",
    "                    qml.Hadamard(wires=0)\n",
    "                    return qml.expval(qml.PauliZ(0))\n",
    "                fidelity = swap_test_qnode(params1, params2).item()\n",
    "                total_fidelity += fidelity\n",
    "            num_trained_pairs += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        avg_fidelity = total_fidelity / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        history['train_loss'].append(avg_train_loss); history['avg_fidelity'].append(avg_fidelity)\n",
    "        all_params = torch.cat([p.data.flatten() for p in model.parameters()]).detach().numpy()\n",
    "        if all_params.size > 0:\n",
    "            history['param_history'].append({'mean': np.mean(all_params), 'std': np.std(all_params), 'min': np.min(all_params), 'max': np.max(all_params)})\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Fidelity: {avg_fidelity:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, fasttext_model, test_data, n_max, device_name, embedding_method):\n",
    "    \"\"\"Evaluates the trained model on the test set using the SWAP test.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation on Test Set ---\")\n",
    "    model.eval()\n",
    "    swap_dev = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for pair in test_data:\n",
    "            @qml.qnode(swap_dev, interface=\"torch\")\n",
    "            def swap_test_qnode(p1, p2):\n",
    "                qml.Hadamard(wires=0)\n",
    "                dummy_theta = torch.tensor(0.1); dummy_phi = torch.tensor(0.1)\n",
    "                execute_discopy_diagram(pair['width1'], pair['d1'], p1, range(1, 1 + n_max), embedding_method, dummy_theta, dummy_phi)\n",
    "                execute_discopy_diagram(pair['width2'], pair['d2'], p2, range(1 + n_max, 1 + 2 * n_max), embedding_method, dummy_theta, dummy_phi)\n",
    "                for j in range(n_max): qml.CSWAP(wires=[0, 1 + j, 1 + n_max + j])\n",
    "                qml.Hadamard(wires=0)\n",
    "                return qml.expval(qml.PauliZ(0))\n",
    "            \n",
    "            params1 = model(pair['d1'], fasttext_model)\n",
    "            params2 = model(pair['d2'], fasttext_model)\n",
    "            measured_overlap = swap_test_qnode(params1, params2)\n",
    "            predictions.append(measured_overlap.item())\n",
    "            true_labels.append(pair['label'])\n",
    "    \n",
    "    if predictions:\n",
    "        # Convert predictions to binary (0 or 1) based on a threshold (e.g., 0.5)\n",
    "        threshold = 0.5\n",
    "        binary_preds = (np.array(predictions) >= threshold).astype(int)\n",
    "        true_labels_arr = np.array(true_labels)\n",
    "\n",
    "        # ✅ Calculate F1 Score\n",
    "        f1 = f1_score(true_labels_arr, binary_preds)\n",
    "        print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # You can also calculate accuracy here if desired\n",
    "        accuracy = np.mean(binary_preds == true_labels_arr)\n",
    "        print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Evaluation Plots ---\")\n",
    "        plot_confusion_matrix(true_labels_arr, predictions, threshold=threshold) # Pass true_labels_arr\n",
    "        plot_roc_curve(true_labels_arr, predictions) # Pass true_labels_arr\n",
    "    else:\n",
    "        print(\"No valid pairs in the test set to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click this to run model\n",
    "def main(config_path: str):\n",
    "    \"\"\"Main function to run the entire workflow.\"\"\"\n",
    "    with open(config_path, 'r') as f: config = yaml.safe_load(f)\n",
    "    print(\"Configuration loaded:\\n\", yaml.dump(config, indent=2))\n",
    "\n",
    "    # --- Initialize Lambeq Objects ---\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(config['qnlp']['rewrite_rules'])\n",
    "    N = AtomicType.NOUN; S = AtomicType.SENTENCE\n",
    "    ansatz = StronglyEntanglingAnsatz({N: 1, S: 1}, n_layers=config['qnlp']['n_layers'])\n",
    "\n",
    "    # --- Load Data and Models ---\n",
    "    fasttext_model = load_fasttext_model(config['data']['fasttext_path'])\n",
    "    if not fasttext_model: return\n",
    "    \n",
    "    sentences1, sentences2, value = load_data(config['data']['path'], config['data']['sample_fraction'])\n",
    "    data_pairs = list(zip(sentences1, sentences2, value))\n",
    "    \n",
    "    filtered_data, symbols, n_max = preprocess_data_for_model(\n",
    "        data_pairs, tokeniser, ansatz, parser, rewriter, config['data']['qubit_limit']\n",
    "    )\n",
    "    \n",
    "    if filtered_data and n_max > 0:\n",
    "        # --- Create Datasets ---\n",
    "        train_val_data_raw, test_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
    "        train_data_raw, val_data = train_test_split(train_val_data_raw, test_size=0.25, random_state=42)\n",
    "        training_data = create_balanced_training_set(train_data_raw)\n",
    "        print(f\"\\nData split into {len(training_data)} training pairs, {len(val_data)} validation pairs, and {len(test_data)} test pairs.\")\n",
    "        \n",
    "        # --- Initialize Model and Train ---\n",
    "        embedding_dim = fasttext_model.get_dimension()\n",
    "        model = QNLPModel(symbols, embedding_dim=embedding_dim)\n",
    "        \n",
    "        trained_model, history = train_model(\n",
    "            model=model, fasttext_model=fasttext_model, training_data=training_data,\n",
    "            validation_data=val_data, n_max=n_max, device_name=config['simulation']['device'],\n",
    "            base_learning_rate=config['training']['base_learning_rate'],\n",
    "            lambda_penalty=config['training']['lambda_penalty'], epochs=config['training']['epochs'],\n",
    "            n_layers=config['qnlp']['n_layers'], embedding_method=config['qnlp']['embedding_method']\n",
    "        )\n",
    "\n",
    "        # --- Evaluate and Plot ---\n",
    "        evaluate_model(\n",
    "            model=trained_model, fasttext_model=fasttext_model, test_data=test_data,\n",
    "            n_max=n_max, device_name=config['simulation']['device'],\n",
    "            embedding_method=config['qnlp']['embedding_method']\n",
    "        )\n",
    "        plot_training_history(history)\n",
    "        if 'param_history' in history and history['param_history']:\n",
    "            plot_parameter_evolution_polar(history['param_history'])\n",
    "            plot_parameter_deltas(history['param_history'])\n",
    "    else:\n",
    "        print(\"\\nNo data to train on.\")\n",
    "\n",
    "# Call main\n",
    "config_file_path = 'config.yaml'\n",
    "main(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_lambeq_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    \"\"\"Prepares lambeq Circuit objects suitable for PennylaneModel.\"\"\"\n",
    "    print(f\"Starting preprocessing for PennylaneModel with qubit limit {qubit_limit}...\")\n",
    "    filtered_pairs, all_symbols, n_max = [], set(), 0\n",
    "    N = AtomicType.NOUN\n",
    "    for i, (s1, s2, is_duplicate) in enumerate(data_pairs):\n",
    "        try:\n",
    "            # Full pipeline: parse -> rewrite -> ansatz = lambeq Circuit\n",
    "            c1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            c2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "\n",
    "            width1 = len(c1.cod) # Width determined by ansatz output\n",
    "            width2 = len(c2.cod)\n",
    "\n",
    "            if width1 <= qubit_limit and width2 <= qubit_limit:\n",
    "                pair_data = {'s1': s1, 's2': s2, 'label': is_duplicate,\n",
    "                             'circuit1': c1, 'circuit2': c2, # Store circuits\n",
    "                             'width1': width1, 'width2': width2}\n",
    "                filtered_pairs.append(pair_data)\n",
    "                all_symbols.update(c1.free_symbols)\n",
    "                all_symbols.update(c2.free_symbols)\n",
    "                n_max = max(n_max, width1, width2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to process pair #{i+1}. Reason: {e}\")\n",
    "            continue\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    # Return circuits and labels directly, plus symbols for weight init\n",
    "    circuits1 = [p['circuit1'] for p in filtered_pairs]\n",
    "    circuits2 = [p['circuit2'] for p in filtered_pairs]\n",
    "    labels = [p['label'] for p in filtered_pairs]\n",
    "    return circuits1, circuits2, labels, sorted(list(all_symbols), key=lambda s: s.name), n_max\n",
    "from lambeq.training import SPSAOptimizer\n",
    "from lambeq.training.quantum_model import QuantumModel\n",
    "from sympy import Symbol\n",
    "\n",
    "def initialize_weights_from_fasttext(symbols: list[Symbol], fasttext_model) -> torch.Tensor:\n",
    "    \"\"\"Creates initial weights based on FastText embeddings.\"\"\"\n",
    "    symbol_to_word = {s.name.replace('.', '_'): s.name.split('_')[-1] for s in symbols}\n",
    "    initial_weights = []\n",
    "    embedding_dim = fasttext_model.get_dimension()\n",
    "    # Simple linear layer to map embedding to a scalar parameter\n",
    "    # NOTE: This assumes 1 parameter per symbol, adjust if ansatz needs more\n",
    "    encoder = nn.Linear(embedding_dim, 1) # Use a fresh encoder instance\n",
    "\n",
    "    with torch.no_grad(): # We only need the initial values\n",
    "        for sym in symbols:\n",
    "            word = symbol_to_word.get(sym.name.replace('.', '_'), None)\n",
    "            if word:\n",
    "                vector = fasttext_model.get_word_vector(word)\n",
    "                vector_tensor = torch.tensor(vector, dtype=torch.float32)\n",
    "                # Generate initial parameter value\n",
    "                param_val = encoder(vector_tensor).squeeze().item()\n",
    "                initial_weights.append(param_val)\n",
    "            else:\n",
    "                initial_weights.append(0.1) # Default for symbols without clear words\n",
    "\n",
    "    return torch.tensor(initial_weights, dtype=torch.float32)\n",
    "from lambeq.training.trainer import Trainer\n",
    "from lambeq.training.dataset import Dataset\n",
    "from torch.optim import Adam\n",
    "import pennylane as qml\n",
    "\n",
    "def train_with_lambeq_pennylane_model(circuits1, circuits2, labels, symbols, fasttext_model, n_max, device_name, config):\n",
    "    \"\"\"Sets up and trains using lambeq.training.PennylaneModel.\"\"\"\n",
    "\n",
    "    # 1. Initialize weights\n",
    "    initial_weights = initialize_weights_from_fasttext(symbols, fasttext_model)\n",
    "\n",
    "    # 2. Define the QuantumModel - uses SWAP test for comparing two circuits\n",
    "    # We need to use QuantumModel.from_diagrams for comparison tasks\n",
    "    # This requires diagrams, not circuits - let's adjust preprocessing slightly if needed\n",
    "    # Assuming circuits1/circuits2 are compatible lists of lambeq circuits\n",
    "    # Note: PennylaneModel expects a single list of diagrams usually.\n",
    "    # For pair comparison, we might need a custom model or structure adaptation.\n",
    "    # Let's try defining a custom PyTorch model that uses PennylaneModel internally,\n",
    "    # or adapt the input format if possible.\n",
    "\n",
    "    # --- Alternative Approach: Custom PyTorch Module using QNodes ---\n",
    "    # This might be closer to what PennylaneModel expects, wrapping QNodes.\n",
    "\n",
    "    class PairComparisonModel(torch.nn.Module):\n",
    "        def __init__(self, n_max, device_name, initial_weights):\n",
    "            super().__init__()\n",
    "            self.n_max = n_max\n",
    "            self.device = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "            # Store weights as parameters\n",
    "            self.weights = torch.nn.Parameter(initial_weights)\n",
    "\n",
    "        def get_qnode(self, circuit1, circuit2, width1, width2):\n",
    "            # Define QNode dynamically inside, similar to previous approaches\n",
    "            @qml.qnode(self.device, interface='torch')\n",
    "            def swap_test_qnode(weights):\n",
    "                # Map global weights to the specific symbols needed by circuits\n",
    "                symbols1 = circuit1.free_symbols\n",
    "                params1 = torch.tensor([weights[symbols.index(s)] for s in symbols1 if s in symbols]) # Map weights\n",
    "                \n",
    "                symbols2 = circuit2.free_symbols\n",
    "                params2 = torch.tensor([weights[symbols.index(s)] for s in symbols2 if s in symbols]) # Map weights\n",
    "\n",
    "                qml.Hadamard(wires=0)\n",
    "                # Need execute_discopy_diagram here to run the lambeq circuit objects\n",
    "                # (Assuming execute_discopy_diagram is available)\n",
    "                execute_discopy_diagram(width1, circuit1, params1, range(1, 1+self.n_max), 'parameterized', torch.tensor(0.1), torch.tensor(0.1)) # Use dummy padding params for now\n",
    "                execute_discopy_diagram(width2, circuit2, params2, range(1+self.n_max, 1+2*self.n_max), 'parameterized', torch.tensor(0.1), torch.tensor(0.1))\n",
    "                for j in range(self.n_max): qml.CSWAP(wires=[0, 1+j, 1+self.n_max+j])\n",
    "                qml.Hadamard(wires=0)\n",
    "                return qml.expval(qml.PauliZ(0))\n",
    "            return swap_test_qnode\n",
    "\n",
    "        def forward(self, data):\n",
    "            # Data should be a batch of (circuit1, circuit2, width1, width2) tuples/lists\n",
    "            results = []\n",
    "            for c1, c2, w1, w2 in data:\n",
    "                 qnode = self.get_qnode(c1, c2, w1, w2)\n",
    "                 # We need the global list of symbols available here\n",
    "                 # This architecture gets complex quickly.\n",
    "                 # PennylaneModel might abstract this better if used correctly.\n",
    "                 # Let's reconsider PennylaneModel's intended use.\n",
    "\n",
    "    # --- Reverting to a simpler PennylaneModel setup (might not fit pair comparison directly) ---\n",
    "    # PennylaneModel is typically for classifying single diagrams.\n",
    "    # For pairs, we might need two models or a custom Pytorch layer.\n",
    "\n",
    "    # --- Let's focus on the structure you'd use IF PennylaneModel supported pairs easily ---\n",
    "    # (This is conceptual - the library might need extending or a different approach)\n",
    "\n",
    "    print(\"Note: Direct pair comparison with lambeq.PennylaneModel might require custom adaptation.\")\n",
    "    print(\"Setting up a basic structure assuming such adaptation is possible or using a workaround.\")\n",
    "\n",
    "    # Placeholder: Define a loss (needs adaptation for pair comparison)\n",
    "    bce_loss = torch.nn.BCELoss() # Binary Cross Entropy for 0/1 labels\n",
    "\n",
    "    # Placeholder: Define optimizer\n",
    "    optimizer = Adam # Using Adam class\n",
    "\n",
    "    # Create PennylaneModel instance (conceptual)\n",
    "    # model = PennylaneModel(model_spec={'backend': device_name, 'n_wires': 1 + 2*n_max}) # Simplified\n",
    "\n",
    "    # Prepare dataset (conceptual)\n",
    "    # Need to structure data appropriately for the model's forward pass\n",
    "    # train_data = list(zip(circuits1, circuits2)) # Example structure\n",
    "    # train_dataset = Dataset(train_data, labels)\n",
    "\n",
    "    # Setup Trainer (conceptual)\n",
    "    # trainer = Trainer(model=model,\n",
    "    #                   loss_function=bce_loss,\n",
    "    #                   optimizer=optimizer,\n",
    "    #                   learning_rate=config['training']['base_learning_rate'],\n",
    "    #                   epochs=config['training']['epochs'],\n",
    "    #                   evaluate_functions={'acc': lambda y_hat, y: (y_hat.round() == y).float().mean()},\n",
    "    #                   evaluate_on_train=True,\n",
    "    #                   verbose='text',\n",
    "    #                   seed=0)\n",
    "\n",
    "    # Train model (conceptual)\n",
    "    # trainer.fit(train_dataset, val_dataset=None) # Assuming train_dataset is correctly formatted\n",
    "\n",
    "    print(\"Conceptual setup for lambeq.PennylaneModel complete.\")\n",
    "    print(\"Actual implementation for pair comparison may require a custom PyTorch module wrapping QNodes.\")\n",
    "\n",
    "    # Return None for now as the direct PennylaneModel path is unclear for pairs\n",
    "    return None, None\n",
    "from lambeq import IQPAnsatz # Ensure IQPAnsatz is imported\n",
    "\n",
    "def main_pennylane_model_branch(config_path: str):\n",
    "    \"\"\"Main function for the PennylaneModel branch.\"\"\"\n",
    "    with open(config_path, 'r') as f: config = yaml.safe_load(f)\n",
    "    print(\"Configuration loaded:\\n\", yaml.dump(config, indent=2))\n",
    "\n",
    "    # --- Initialize Lambeq Objects ---\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser() # Or OncillaParser / discocirc when ready\n",
    "    rewriter = Rewriter(config['qnlp']['rewrite_rules'])\n",
    "    N = AtomicType.NOUN; S = AtomicType.SENTENCE\n",
    "    # Use IQPAnsatz for preprocessing as it's most compatible\n",
    "    ansatz = IQPAnsatz({N: 1, S: 1}, n_layers=config['qnlp']['n_layers'])\n",
    "\n",
    "    # --- Load Data and Models ---\n",
    "    fasttext_model = load_fasttext_model(config['data']['fasttext_path'])\n",
    "    if not fasttext_model: return\n",
    "\n",
    "    sentences1, sentences2, labels_raw = load_data(config['data']['path'], config['data']['sample_fraction'])\n",
    "    data_pairs = list(zip(sentences1, sentences2, labels_raw))\n",
    "\n",
    "    # Preprocess to get circuits and labels\n",
    "    circuits1, circuits2, labels, symbols, n_max = preprocess_for_lambeq_model(\n",
    "        data_pairs, tokeniser, ansatz, parser, rewriter, config['data']['qubit_limit']\n",
    "    )\n",
    "\n",
    "    if circuits1 and n_max > 0:\n",
    "        # --- Split Data (if necessary for Trainer) ---\n",
    "        # Trainer might handle splitting, or do it manually:\n",
    "        # train_c1, test_c1, train_c2, test_c2, train_labels, test_labels = train_test_split(...)\n",
    "\n",
    "        print(f\"\\nPreprocessing successful. Proceeding with {len(circuits1)} pairs.\")\n",
    "\n",
    "        # --- Train using PennylaneModel structure ---\n",
    "        # Note: This function currently returns None due to pair comparison complexity\n",
    "        trained_model, history = train_with_lambeq_pennylane_model(\n",
    "            circuits1, circuits2, labels, symbols, fasttext_model, n_max,\n",
    "            config['simulation']['device'], config\n",
    "        )\n",
    "\n",
    "        if trained_model:\n",
    "             # --- Evaluate (conceptual) ---\n",
    "             # test_acc = trainer.test(test_dataset) # Assuming test_dataset setup\n",
    "             # print(f\"Test accuracy: {test_acc['acc']:.2f}\")\n",
    "             print(\"Evaluation step needs implementation based on chosen model structure.\")\n",
    "        else:\n",
    "            print(\"Training function needs further implementation for pair comparison.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo data to train on after preprocessing.\")\n",
    "\n",
    "# Example call for this branch\n",
    "# config_file_path = 'config.yaml'\n",
    "# main_pennylane_model_branch(config_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36013460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      " data:\n",
      "  fasttext_path: C:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\SemanticEquivilance\\cc.en.300.bin\n",
      "  path: C:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\SemanticEquivilance\\question_pairs\\questions.csv\n",
      "  qubit_limit: 12\n",
      "  sample_fraction: 0.05\n",
      "  test_size: 0.2\n",
      "  validation_size: 0.25\n",
      "logging:\n",
      "  log_file: error_log.txt\n",
      "  log_level: WARNING\n",
      "nlp:\n",
      "  spacy_model: en_core_web_sm\n",
      "qnlp:\n",
      "  ansatz_type: StronglyEntangling\n",
      "  embedding_method: parameterized\n",
      "  n_layers: 2\n",
      "  rewrite_rules:\n",
      "  - curry\n",
      "  - determiner\n",
      "simulation:\n",
      "  cpu_cores: 6\n",
      "  device: lightning.qubit\n",
      "training:\n",
      "  base_learning_rate: 0.001\n",
      "  epochs: 5\n",
      "  lambda_penalty: 0.1\n",
      "  use_local_cost: true\n",
      "\n",
      "SpaCy model en_core_web_sm loaded.\n"
     ]
    }
   ],
   "source": [
    "# === Standard Libraries ===\n",
    "import os\n",
    "import random\n",
    "import yaml\n",
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# === Data Handling ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Machine Learning (PyTorch) ===\n",
    "import torch\n",
    "import torch.nn as nn # Use nn alias consistently\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Parameter # Keep if using custom Parameters outside nn.Module\n",
    "\n",
    "# === Quantum Computing (PennyLane) ===\n",
    "import pennylane as qml\n",
    "\n",
    "# === Natural Language Processing ===\n",
    "import spacy\n",
    "import fasttext # For .bin model loading\n",
    "\n",
    "# === QNLP Libraries (DisCoPy and Lambeq) ===\n",
    "import discopy\n",
    "from discopy.rigid import Ty\n",
    "from discopy.quantum import gates # Keep if directly using discopy gates\n",
    "# from lambeq.backend.quantum import Diagram as LambeqDiagram # Usually not needed directly\n",
    "from lambeq import BobcatParser, Rewriter, SpacyTokeniser, AtomicType, IQPAnsatz, StronglyEntanglingAnsatz # Pick one ansatz\n",
    "\n",
    "# === Evaluation & Plotting ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Lambeq/DisCoPy Compatibility Patch ===\n",
    "monoidal_module = getattr(discopy, \"monoidal\", None)\n",
    "if monoidal_module:\n",
    "    diagram_class = getattr(monoidal_module, \"Diagram\", None)\n",
    "    if diagram_class and not hasattr(diagram_class, \"is_mixed\"):\n",
    "        diagram_class.is_mixed = property(lambda self: False)\n",
    "# === Load Configuration ===\n",
    "try:\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration loaded:\\n\", yaml.dump(config, indent=2))\n",
    "except FileNotFoundError:\n",
    "    print(\"Configuration file 'config.yaml' not found. Please ensure it exists in the working directory.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    raise e    \n",
    "# === Load SpaCy Model (using config) ===\n",
    "if config:\n",
    "    model_name = config['nlp']['spacy_model']\n",
    "    try:\n",
    "        spacy.load(model_name)\n",
    "        print(f\"SpaCy model {model_name} loaded.\")\n",
    "    except OSError:\n",
    "        print(f\"Downloading SpaCy model {model_name}\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
    "            spacy.load(model_name)\n",
    "            print(f\"SpaCy {model_name} loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download SpaCy model {model_name}: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540c7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading Function\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    sentences1, sentences2, is_duplicate = [], [], []\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentences1 = df['question1'].astype(str).tolist()\n",
    "        sentences2 = df['question2'].astype(str).tolist()\n",
    "        is_duplicate = df['is_duplicate'].tolist()\n",
    "        \n",
    "        return sentences1, sentences2, is_duplicate\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "def create_balanced_training_set(training_data: list) -> list:\n",
    "    \"\"\"Creates a balanced training set by undersampling the majority class.\"\"\"\n",
    "    positives = [pair for pair in training_data if pair['label'] == 1]\n",
    "    negatives = [pair for pair in training_data if pair['label'] == 0]\n",
    "    \n",
    "    # Undersample the larger class to match the size of the smaller class\n",
    "    if len(positives) > len(negatives):\n",
    "        positives = random.sample(positives, len(negatives))\n",
    "    else:\n",
    "        negatives = random.sample(negatives, len(positives))\n",
    "    \n",
    "    balanced_train_set = positives + negatives\n",
    "    random.shuffle(balanced_train_set)\n",
    "    \n",
    "    print(f\"Created a balanced training set with {len(positives)} positive and {len(negatives)} negative pairs.\")\n",
    "    return balanced_train_set\n",
    "def load_fasttext_model(model_path: str):\n",
    "    \"\"\"Loads the full FastText .bin model.\"\"\"\n",
    "    model = fasttext.load_model(model_path)\n",
    "    print(\"FastText model loaded successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "def plot_training_history(history: dict):\n",
    "    \"\"\"Plots the training loss and the average fidelity over epochs.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='Training Loss (Local Cost)')\n",
    "    if 'avg_fidelity' in history and history['avg_fidelity']:\n",
    "        plt.plot(history['avg_fidelity'], label='Avg. Fidelity (from SWAP Test)', linestyle='--')\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_parameter_history(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(len(param_history))\n",
    "    means = [d['mean'] for d in param_history]\n",
    "    stds = [d['std'] for d in param_history]\n",
    "    mins = [d['min'] for d in param_history]\n",
    "    maxs = [d['max'] for d in param_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, means, label='Mean Parameter Value')\n",
    "    plt.fill_between(epochs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.2, label='1 Std. Deviation')\n",
    "    plt.plot(epochs, mins, linestyle='--', color='gray', label='Min/Max Range')\n",
    "    plt.plot(epochs, maxs, linestyle='--', color='gray')\n",
    "    \n",
    "    plt.title('Evolution of Model Parameters During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_evolution_polar(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "    epochs = np.array(range(len(param_history)))\n",
    "    \n",
    "    mean_angles = np.array([d['mean'] for d in param_history]) % (4 * np.pi)\n",
    "\n",
    "    ax.plot(mean_angles, epochs, 'o-', label='Mean Parameter Path')\n",
    "\n",
    "    if len(epochs) > 0:\n",
    "        ax.plot(mean_angles[0], epochs[0], 'gX', markersize=12, label='Start')\n",
    "        ax.plot(mean_angles[-1], epochs[-1], 'rX', markersize=12, label='End')\n",
    "\n",
    "    ax.set_theta_zero_location('N')# pyright: ignore\n",
    "    ax.set_theta_direction(-1)# pyright: ignore\n",
    "    ax.set_rlabel_position(0)# pyright: ignore\n",
    "    ax.set_rlim(0, len(epochs) * 1.05)# pyright: ignore\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_title('Cyclical Evolution of Mean Parameter', pad=20)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "def plot_parameter_deltas(param_history):\n",
    "    if len(param_history) < 2:\n",
    "        print(\"Need at least 2 epochs to plot parameter deltas.\")\n",
    "        return\n",
    "\n",
    "    mean_angles = np.array([d['mean'] for d in param_history])\n",
    "    \n",
    "    # Calculate the shortest angle difference between each epoch\n",
    "    deltas = []\n",
    "    for i in range(1, len(mean_angles)):\n",
    "        prev_angle = mean_angles[i-1]\n",
    "        curr_angle = mean_angles[i]\n",
    "        delta = np.arctan2(np.sin(curr_angle - prev_angle), np.cos(curr_angle - prev_angle))\n",
    "        deltas.append(delta)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # We plot against epochs 1 to N, since the first delta occurs at epoch 1\n",
    "    plt.plot(range(1, len(mean_angles)), deltas, 'o-', label='Change in Mean Parameter (Delta)')\n",
    "    \n",
    "    plt.axhline(0, color='red', linestyle='--', label='No Change')\n",
    "    plt.title('Epoch-to-Epoch Change in Mean Parameter Value')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Shortest Angle Difference (Radians)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, len(mean_angles)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes and plots a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): The ground-truth labels (0s and 1s).\n",
    "        y_pred (np.array): The model's raw probability predictions (overlaps from 0 to 1).\n",
    "        threshold (float): The cutoff for classifying a prediction as 1.\n",
    "    \"\"\"\n",
    "    # Convert probability predictions to binary 0/1 predictions\n",
    "    binary_preds = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, binary_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Non-Duplicate', 'Predicted Duplicate'],\n",
    "                yticklabels=['Actual Non-Duplicate', 'Actual Duplicate'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes and plots the ROC curve and AUC score.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNLP MODEL AND TRAINING PIPELINE\n",
    "def execute_discopy_diagram(current_width, diagram, params, wires, embedding_method='simple_pad', rotation_param=None, entangling_param=None):\n",
    "    \"\"\"Executes a DisCoPy/lambeq diagram's instructions, including padding.\"\"\"\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "    ancilla_wires = wires[current_width:]\n",
    "    if embedding_method == 'parameterized':\n",
    "        if rotation_param is not None:\n",
    "            for w in ancilla_wires: qml.RY(rotation_param, wires=w)\n",
    "        if entangling_param is not None and len(ancilla_wires) > 1:\n",
    "            for i in range(len(ancilla_wires)):\n",
    "                qml.CPHASE(entangling_param, wires=[ancilla_wires[i], ancilla_wires[(i + 1) % len(ancilla_wires)]])\n",
    "\n",
    "def execute_discopy_diagram_local(diagram, params, wires):\n",
    "    \"\"\"Executes a DisCoPy/lambeq diagram without padding for local measurements.\"\"\"\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "def get_diagram_width(diagram):\n",
    "    \"\"\"Calculates the true maximum width of a diagram at any point.\"\"\"\n",
    "    if not diagram.boxes:\n",
    "        return len(diagram.cod)\n",
    "    # The width is the maximum wire index a box acts on.\n",
    "    return max(\n",
    "        [offset + len(box.dom) for box, offset in zip(diagram.boxes, diagram.offsets)]\n",
    "        + [len(diagram.cod)]\n",
    "    )\n",
    "    \n",
    "def preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    \"\"\"\n",
    "    Uses lambeq pipeline, logs errors, doesn't retry on failures.\n",
    "    \"\"\"\n",
    "    print(f\"Starting preprocessing with qubit limit {qubit_limit}...\")\n",
    "    filtered_pairs, all_symbols, n_max = [], set(), 0\n",
    "    N = AtomicType.NOUN\n",
    "\n",
    "    for i, (s1, s2, is_duplicate) in enumerate(data_pairs):\n",
    "        try:\n",
    "            # Pipeline: parse, rewrite, ansatz\n",
    "            d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "            \n",
    "            width1 = get_diagram_width(d1)\n",
    "            width2 = get_diagram_width(d2)\n",
    "\n",
    "            if width1 <= qubit_limit and width2 <= qubit_limit:\n",
    "                pair_data = {'s1': s1, 's2': s2, 'label': is_duplicate, 'd1': d1, 'd2': d2,\n",
    "                             'structural_disparity': abs(width1 - width2),\n",
    "                             'width1': width1, 'width2': width2}\n",
    "                filtered_pairs.append(pair_data)\n",
    "                all_symbols.update(d1.free_symbols)\n",
    "                all_symbols.update(d2.free_symbols)\n",
    "                n_max = max(n_max, width1, width2)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Error processing pair #{i+1}: Sentences: {s1} /// {s2}: {e}\")\n",
    "            continue # Skip this pair\n",
    "\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    print(f\"Total unique parameters (symbols) found: {len(all_symbols)}\")\n",
    "    print(f\"N_Max for the filtered dataset is: {n_max}\")\n",
    "    return filtered_pairs, sorted(list(all_symbols), key=lambda s: s.name), n_max\n",
    "\n",
    "class QNLPModel(nn.Module):\n",
    "    \"\"\"Generates the exact number of parameters required by the diagram.\"\"\"\n",
    "    def __init__(self, symbols, embedding_dim=300, max_params_heuristic=50):\n",
    "        super().__init__()\n",
    "        # We need a way to map the averaged sentence vector to a variable number of params.\n",
    "        # Let's use a simple MLP instead of just Linear. Kharti's method\n",
    "        # Outputting a large fixed size, we will slice later if needed, but based on symbols count.\n",
    "        # Max possible symbols could be large, let's estimate or find max_symbols needed across dataset.\n",
    "        # For now, let's assume a reasonable max (e.g., 50 parameters typical?)\n",
    "        # A better approach might be needed if this max is too small or too large.\n",
    "        max_params_heuristic = 50 # Adjust as needed\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim // 2, max_params_heuristic)\n",
    "        )\n",
    "        self.symbol_to_word = {s.name.replace('.', '_'): s.name.split('_')[-1] for s in symbols}\n",
    "        #trainable padding parameters\n",
    "        self.rotation_param = Parameter(torch.rand(1)*0.01)\n",
    "        self.entangling_param = Parameter(torch.rand(1)*0.01)\n",
    "        \n",
    "    def forward(self, diagram, fasttext_model):\n",
    "        symbols = diagram.free_symbols\n",
    "        num_required_params = len(symbols)\n",
    "        \n",
    "        if not symbols or num_required_params == 0:\n",
    "            return torch.tensor([]) # Return empty tensor if no params needed\n",
    "\n",
    "        words = [self.symbol_to_word.get(s.name.replace('.', '_')) for s in symbols]\n",
    "        vectors = [fasttext_model.get_word_vector(w) for w in words if w]\n",
    "        \n",
    "        if not vectors:\n",
    "            # If no words found, return zeros matching required params\n",
    "            return torch.zeros(num_required_params)\n",
    "\n",
    "        sentence_vector = torch.tensor(vectors, dtype=torch.float32).mean(dim=0)\n",
    "        \n",
    "        # Generate a large fixed set of potential parameters\n",
    "        all_params = self.encoder(sentence_vector)\n",
    "        \n",
    "        # Slice exactly the number needed for this diagram\n",
    "        if num_required_params > all_params.shape[0]:\n",
    "             # Handle cases needing more params than our heuristic max\n",
    "             # This might indicate the heuristic needs increasing or a different strategy\n",
    "             logging.warning(f\"Diagram requires {num_required_params} params, but model only outputs {all_params.shape[0]}. Truncating.\")\n",
    "             return all_params # Or handle error appropriately\n",
    "        \n",
    "        quantum_params = all_params[:num_required_params]\n",
    "        return quantum_params\n",
    "\n",
    "def train_model(model, fasttext_model, training_data, validation_data, n_max, device_name,\n",
    "                base_learning_rate, lambda_penalty, epochs, n_layers, embedding_method):\n",
    "    \"\"\"Trains with a local cost function and logs SWAP test fidelity.\"\"\"\n",
    "    optimizer = Adam(model.parameters(), lr=base_learning_rate)\n",
    "    dev = qml.device(device_name, wires=n_max)\n",
    "    swap_dev = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "    history = {'train_loss': [], 'val_loss': [], 'avg_fidelity': [], 'param_history': []}\n",
    "    print(\"--- Starting training with LOCAL cost function ---\")\n",
    "\n",
    "    @qml.qnode(dev, interface=\"torch\")\n",
    "    def local_expval_qnode(params, diagram, num_qubits):\n",
    "        execute_discopy_diagram_local(diagram, params, wires=range(num_qubits))\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]\n",
    "    @qml.qnode(swap_dev, interface=\"torch\")\n",
    "    def swap_test_qnode(p1, p2, theta, phi):\n",
    "        qml.Hadamard(wires=0)\n",
    "        execute_discopy_diagram(pair['width1'], pair['d1'], p1, range(1, 1+n_max), embedding_method, theta, phi)\n",
    "        execute_discopy_diagram(pair['width2'], pair['d2'], p2, range(1+n_max, 1+2*n_max), embedding_method, theta, phi)\n",
    "        for j in range(n_max): qml.CSWAP(wires=[0, 1+j, 1+n_max+j])\n",
    "        qml.Hadamard(wires=0)\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_fidelity, num_trained_pairs = 0, 0, 0\n",
    "        for pair in training_data:\n",
    "            optimizer.zero_grad()\n",
    "            params1 = model(pair['d1'], fasttext_model)\n",
    "            params2 = model(pair['d2'], fasttext_model)\n",
    "            \n",
    "            if params1.nelement() == 0 or params2.nelement() == 0:\n",
    "                print(f\"Skipping pair with no parameters needed.\\n\")\n",
    "                continue  # Skip pairs where no parameters are needed (highly probability of being an error case)\n",
    "            \n",
    "            exp_vals1 = torch.stack(local_expval_qnode(params1, pair['d1'], pair['width1']))\n",
    "            exp_vals2 = torch.stack(local_expval_qnode(params2, pair['d2'], pair['width2']))\n",
    "            \n",
    "            if pair['width1'] < n_max: exp_vals1 = torch.cat([exp_vals1, torch.zeros(n_max - pair['width1'])])\n",
    "            if pair['width2'] < n_max: exp_vals2 = torch.cat([exp_vals2, torch.zeros(n_max - pair['width2'])])\n",
    "\n",
    "            mse = torch.mean((exp_vals1 - exp_vals2)**2)\n",
    "            target_distance = 1 - pair['label']\n",
    "            local_loss = (mse - target_distance)**2\n",
    "            structural_penalty = torch.tensor(lambda_penalty * pair['structural_disparity'])\n",
    "            loss = local_loss + structural_penalty\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fidelity = swap_test_qnode(params1, params2).item()\n",
    "                total_fidelity += fidelity\n",
    "            num_trained_pairs += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        avg_fidelity = total_fidelity / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        history['train_loss'].append(avg_train_loss); history['avg_fidelity'].append(avg_fidelity)\n",
    "        all_params = torch.cat([p.data.flatten() for p in model.parameters()]).detach().numpy()\n",
    "        if all_params.size > 0:\n",
    "            history['param_history'].append({'mean': np.mean(all_params), 'std': np.std(all_params), 'min': np.min(all_params), 'max': np.max(all_params)})\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Fidelity: {avg_fidelity:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, fasttext_model, test_data, n_max, device_name, embedding_method):\n",
    "    \"\"\"Evaluates the trained model on the test set using the SWAP test.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation on Test Set ---\")\n",
    "    model.eval()\n",
    "    swap_dev = qml.device(device_name, wires=1 + 2 * n_max)\n",
    "    predictions, true_labels = [], []\n",
    "    @qml.qnode(swap_dev, interface=\"torch\")\n",
    "    def swap_test_qnode(p1, p2, theta, phi):\n",
    "        qml.Hadamard(wires=0)\n",
    "        execute_discopy_diagram(pair['width1'], pair['d1'], p1, range(1, 1 + n_max), embedding_method, theta, phi)\n",
    "        execute_discopy_diagram(pair['width2'], pair['d2'], p2, range(1 + n_max, 1 + 2 * n_max), embedding_method, theta, phi)\n",
    "        for j in range(n_max): qml.CSWAP(wires=[0, 1 + j, 1 + n_max + j])\n",
    "        qml.Hadamard(wires=0)\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    with torch.no_grad():\n",
    "        for pair in test_data:\n",
    "            params1 = model(pair['d1'], fasttext_model)\n",
    "            params2 = model(pair['d2'], fasttext_model)\n",
    "            measured_overlap = swap_test_qnode(params1, params2)\n",
    "            predictions.append(measured_overlap.item())\n",
    "            true_labels.append(pair['label'])\n",
    "    \n",
    "    if predictions:\n",
    "        # Convert predictions to binary (0 or 1) based on a threshold (e.g., 0.5)\n",
    "        threshold = 0.5\n",
    "        binary_preds = (np.array(predictions) >= threshold).astype(int)\n",
    "        true_labels_arr = np.array(true_labels)\n",
    "\n",
    "        # ✅ Calculate F1 Score\n",
    "        f1 = f1_score(true_labels_arr, binary_preds)\n",
    "        print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # You can also calculate accuracy here if desired\n",
    "        accuracy = np.mean(binary_preds == true_labels_arr)\n",
    "        print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Evaluation Plots ---\")\n",
    "        plot_confusion_matrix(true_labels_arr, predictions, threshold=threshold) # Pass true_labels_arr\n",
    "        plot_roc_curve(true_labels_arr, predictions) # Pass true_labels_arr\n",
    "    else:\n",
    "        print(\"No valid pairs in the test set to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error logging initialized. Errors will ONLY be logged to error_log.txt.\n",
      "Initialized Lambeq components with STRONGLYENTANGLING ansatz.\n",
      "FastText model loaded successfully.\n",
      "Total loaded data pairs: 20218\n",
      "Starting preprocessing with qubit limit 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Temp\\ipykernel_26728\\381777779.py\", line 59, in preprocess_data_for_model\n",
      "    d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\ansatz\\circuit.py\", line 120, in __call__\n",
      "    return self.functor(diagram)  # type: ignore[return-value]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2046, in __call__\n",
      "    return self.ar_with_cache(entity)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2077, in ar_with_cache\n",
      "    ret = ar.apply_functor(self)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 1325, in apply_functor\n",
      "    @ functor(box).to_diagram()\n",
      "      ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2046, in __call__\n",
      "    return self.ar_with_cache(entity)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2077, in ar_with_cache\n",
      "    ret = ar.apply_functor(self)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 1503, in apply_functor\n",
      "    functor(self.left),\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2044, in __call__\n",
      "    return self.ob_with_cache(entity)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2061, in ob_with_cache\n",
      "    ret = ob.apply_functor(self)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 356, in apply_functor\n",
      "    return functor.ob(self)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\grammar.py\", line 2097, in ob\n",
      "    return self.custom_ob(self, ob)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\ansatz\\circuit.py\", line 147, in _ob\n",
      "    return self.ob_map[ty]\n",
      "           ~~~~~~~~~~~^^^^\n",
      "KeyError: Ty(p)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u20b9' in position 235: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 618, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1951, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Temp\\ipykernel_26728\\2681172202.py\", line 95, in <module>\n",
      "    main(config_file_path)\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Temp\\ipykernel_26728\\2681172202.py\", line 45, in main\n",
      "    filtered_data, symbols, n_max = preprocess_data_for_model(\n",
      "  File \"C:\\Users\\Jash\\AppData\\Local\\Temp\\ipykernel_26728\\381777779.py\", line 75, in preprocess_data_for_model\n",
      "    logging.exception(f\"Error processing pair #{i+1}: Sentences: {s1} /// {s2}: {e}\")\n",
      "Message: 'Error processing pair #990: Sentences: How will scraping of 500 and 1000 rupees notes help in curbing corruption and black money? /// How does invalidating ₹500 and ₹1000 notes help us fight corruption and bring back black money?: Ty(p)'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "#Click this to run model\n",
    "def main(config_path: str):\n",
    "    \"\"\"Main function to run the entire workflow.\"\"\"\n",
    "    \n",
    "    # --- Error logging ---\n",
    "    log_file = config['logging']['log_file']\n",
    "    log_level_str = config['logging']['log_level'].upper()\n",
    "    log_level = getattr(logging, log_level_str, logging.WARNING)\n",
    "    \n",
    "    # Get root logger and clear default handlers\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(log_level)\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    # Create custom file handler\n",
    "    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - Func: %(funcName)s - Line: %(lineno)d - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    print(f\"Error logging initialized. Errors will ONLY be logged to {log_file}.\")\n",
    "    # --- End of Logging Setup ---\n",
    "\n",
    "    # --- Initialize Lambeq Objects ---\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(config['qnlp']['rewrite_rules'])\n",
    "    N = AtomicType.NOUN; S = AtomicType.SENTENCE\n",
    "    ansatz_type = config['qnlp'].get('ansatz_type', 'IQP').upper()\n",
    "    if ansatz_type == 'STRONGLYENTANGLING':\n",
    "        ansatz = StronglyEntanglingAnsatz({N: 1, S: 1}, n_layers=config['qnlp']['n_layers'])\n",
    "    else:\n",
    "        ansatz = IQPAnsatz({N: 1, S: 1}, n_layers=config['qnlp']['n_layers'])\n",
    "    print(f\"Initialized Lambeq components with {ansatz_type} ansatz.\")\n",
    "    \n",
    "    # --- Load Data and Models ---\n",
    "    fasttext_model = load_fasttext_model(config['data']['fasttext_path'])\n",
    "    if not fasttext_model: return\n",
    "    \n",
    "    sentences1, sentences2, value = load_data(config['data']['path'], config['data']['sample_fraction'])\n",
    "    data_pairs = list(zip(sentences1, sentences2, value))\n",
    "    print(f\"Total loaded data pairs: {len(data_pairs)}\")\n",
    "    gc.collect() # Clean up memory after loading raw data\n",
    "    \n",
    "    # --- Preprocess Data ---\n",
    "    filtered_data, symbols, n_max = preprocess_data_for_model(\n",
    "        data_pairs, tokeniser, ansatz, parser, rewriter, config['data']['qubit_limit']\n",
    "    )\n",
    "    del data_pairs, sentences1, sentences2, value; gc.collect() # Clean up memory after preprocessing\n",
    "    print(f\"Preprocessing finished. Filtered pairs: {len(filtered_data)}\")\n",
    "    \n",
    "    if filtered_data and n_max > 0:\n",
    "        # --- Create Datasets ---\n",
    "        val_size_fraction = config['data'].get('validation_size', 0.25)\n",
    "        train_val_data_raw, test_data = train_test_split(filtered_data, test_size=config['data']['test_size'], random_state=42)\n",
    "        train_data_raw, val_data = train_test_split(train_val_data_raw, test_size=val_size_fraction, random_state=42) # Use validation_size\n",
    "        training_data = create_balanced_training_set(train_data_raw)\n",
    "        print(f\"\\nData split using test_size={config['data']['test_size']}, validation_size={val_size_fraction}\")\n",
    "        print(f\"{len(training_data)} train, {len(val_data)} val, {len(test_data)} test pairs.\")\n",
    "        del train_val_data_raw; del train_data_raw; gc.collect() # Clean up memory after splitting data\n",
    "        \n",
    "        # --- Initialize Model and Train ---\n",
    "        embedding_dim = fasttext_model.get_dimension()\n",
    "        model = QNLPModel(symbols, embedding_dim=embedding_dim)\n",
    "        use_local_cost = config['training'].get('use_local_cost', True)\n",
    "        \n",
    "        if use_local_cost:\n",
    "            trained_model, history = train_model(\n",
    "            model=model, fasttext_model=fasttext_model, training_data=training_data,\n",
    "            validation_data=val_data, n_max=n_max, device_name=config['simulation']['device'],\n",
    "            base_learning_rate=config['training']['base_learning_rate'],\n",
    "            lambda_penalty=config['training']['lambda_penalty'], epochs=config['training']['epochs'],\n",
    "            n_layers=config['qnlp']['n_layers'], embedding_method=config['qnlp']['embedding_method']\n",
    "        )\n",
    "        else:\n",
    "            #swap test cost fx\n",
    "            print(\"Only local cost function training is implemented in this script.\")\n",
    "            return NotImplemented\n",
    "        gc.collect() # Clean up memory after training\n",
    "        \n",
    "        # --- Evaluate and Plot ---\n",
    "        evaluate_model(\n",
    "            model=trained_model, fasttext_model=fasttext_model, test_data=test_data,\n",
    "            n_max=n_max, device_name=config['simulation']['device'],\n",
    "            embedding_method=config['qnlp']['embedding_method']\n",
    "        )\n",
    "        plot_training_history(history)\n",
    "        if 'param_history' in history and history['param_history']:\n",
    "            plot_parameter_evolution_polar(history['param_history'])\n",
    "            plot_parameter_deltas(history['param_history'])\n",
    "    else:\n",
    "        print(\"\\nNo data to train on.\")\n",
    "\n",
    "# Call main\n",
    "config_file_path = 'config.yaml'\n",
    "main(config_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43923fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment ready (patched is_mixed if needed)\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import discopy\n",
    "\n",
    "# Patch missing .is_mixed (for lambeq + newer discopy)\n",
    "if not hasattr(getattr(discopy, \"monoidal\", None).Diagram, \"is_mixed\"):\n",
    "    discopy.monoidal.Diagram.is_mixed = property(lambda self: False)\n",
    "\n",
    "print(\"✅ Environment ready (patched is_mixed if needed)\")\n",
    "\n",
    "from lambeq import AtomicType\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "P = AtomicType.PREPOSITIONAL_PHRASE\n",
    "CONJ = AtomicType.CONJUNCTION\n",
    "PUNCT = AtomicType.PUNCTUATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42ac0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import AtomicType\n",
    "from lambeq import IQPAnsatz\n",
    "\n",
    "# Example: noun → 1 qubit, sentence → 1 qubit\n",
    "ob_map = dict({\n",
    "    N: 1,\n",
    "    S: 1,\n",
    "    P: 0,\n",
    "    CONJ: 0,\n",
    "    PUNCT: 0\n",
    "})\n",
    "\n",
    "ansatz = IQPAnsatz(ob_map, n_layers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a395963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 sentences.\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    \"\"\"Loads Question Pairs from a CSV file\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to csv_file\n",
    "        sample_fraction (float): Fraction of data to sample, default is 1.0\n",
    "    Returns:\n",
    "        tuple: A tuple containing supervised data pairs\n",
    "        returns [],[] on error\n",
    "    \"\"\"\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    is_duplicate = []\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        #print(\"Column names:\", df.columns)\n",
    "        \n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentence1_series = df['question1']\n",
    "        sentence2_series = df['question2']\n",
    "        is_duplicate_series = df['is_duplicate']\n",
    "        \n",
    "        sentences1 = sentence1_series.tolist()\n",
    "        sentences2 = sentence2_series.tolist()\n",
    "        is_duplicate = is_duplicate_series.tolist()\n",
    "        \n",
    "        if len(sentences1) != len(sentences2):\n",
    "            raise ValueError(\"The number of sentences in question1 and question2 do not match.\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(sentences1)} sentences.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Wrong Path\")\n",
    "        return [],[],[]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An {e} Error Occurred\")\n",
    "        return [],[],[]\n",
    "\n",
    "DATA_PATH = r'C:/Users/Jash\\Documents/Research\\Semantic Equivilance\\SemanticEquivilance/question_pairs/questions.csv'\n",
    "sentences1, sentences2, value = load_data(DATA_PATH, sample_fraction=0.0001)\n",
    "data_pairs = list(zip(sentences1, sentences2, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7815b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "from lambeq import AtomicType, BobcatParser, Rewriter, IQPAnsatz, SpacyTokeniser\n",
    "\n",
    "\n",
    "def swap_test(state1_vec, state2_vec, num_qubits, initial_state=0):\n",
    "    \"\"\"\n",
    "    Performs a Quantum Swap Test between two quantum state vectors.\n",
    "\n",
    "    Args:\n",
    "        state1_vec (np.ndarray): The first state vector.\n",
    "        state2_vec (np.ndarray): The second state vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The estimated squared overlap (fidelity) between the two states.\n",
    "    \"\"\"\n",
    "    SHOT_COUNT = 1000000 #expected less than 1% error only done on swap test, expectation values calculated with different amount of shots\n",
    "    if 2**num_qubits != len(state1_vec):\n",
    "        raise ValueError(\"State vectors must have a length that is a power of 2.\")\n",
    "\n",
    "    total_qubits = 1 + 2 * num_qubits #1 Ancilla qubit + 2 state qubits\n",
    "\n",
    "    dev = qml.device(\"lightning.qubit\", wires=total_qubits, shots=SHOT_COUNT)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(): #|0 , psi, phi>\n",
    "        # Step 1: Prepare the ancilla qubit in a superposition\n",
    "        qml.Hadamard(wires=0)\n",
    "        print(f\"Comparing: {state1_vec} and {state2_vec}\")\n",
    "        # Step 2: Prepare the two input states\n",
    "        #basis for protocol 1\n",
    "        qml.StatePrep(state1_vec, wires=range(1, 1 + num_qubits), normalize=True)\n",
    "        #basis for protocol 1\n",
    "        qml.StatePrep(state2_vec, wires=range(1 + num_qubits, 1 + 2 * num_qubits))\n",
    "\n",
    "        # Step 3: Apply controlled-SWAP gates\n",
    "        for i in range(num_qubits):\n",
    "            qml.CSWAP(wires=[0, 1 + i, 1 + num_qubits + i]) #selects every register of phi and psi for swap\n",
    "\n",
    "        # Step 4: Apply Hadamard to ancilla\n",
    "        qml.Hadamard(wires=0)\n",
    "        # Step 5: Measure the ancilla qubit\n",
    "        return qml.sample(wires=0)\n",
    "\n",
    "    measurement_results = circuit()\n",
    "    squared_overlap = 1 - 2/len(measurement_results) * np.sum(measurement_results)\n",
    "    \n",
    "    return abs(squared_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fbb2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.pennylane import to_pennylane as to_qml\n",
    "def lambeq_sentence_to_circuit(sentence, Tokeniser, ansatz, parser, rewriter, return_type='state', include_debug_prints=False, expval = qml.PauliZ(0), target_qubits=None):\n",
    "    \"\"\"\n",
    "    Converts a natural language sentence into a quantum state vector\n",
    "    using Lambeq's BobcatParser and IQPAnsatz, handling parameterization\n",
    "    via PennyLaneModel.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        ansatz (lambeq.ansatz.Ansatz): The quantum ansatz to apply.\n",
    "        parser (lambeq.parser.Parser): The parser to convert sentence to diagram. This is a GPT\n",
    "        rewriter (lambeq.rewrite.Rewriter): The rewriter to simplify the diagram. This is a GPT\n",
    "        include_debug_prints (bool): Whether to include detailed debug prints.\n",
    "        start_basis_state (int): The basis state to initialize the qubits. Default is |0>.\n",
    "        seed (int, optional): Seed for random number generation for reproducibility.\n",
    "        return_type (str): 'state' or 'expval' to specify the return type.\n",
    "        expval (qml.Observable): The observable for expval value calculation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the state vector (np.ndarray) and\n",
    "               the number of qubits (int).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if include_debug_prints:\n",
    "            print(f\"\\n--- Debugging: Sentence '{sentence}' ---\")\n",
    "        # Step 0: Tokenize the sentence\n",
    "        tokens = Tokeniser.tokenise_sentence(sentence) # Not optional for non-clean inputs\n",
    "        \n",
    "        # Step 1: Convert sentence to a DisCoPy diagram\n",
    "        diagram = parser.sentence2diagram(tokens, tokenised=True)\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 1: Sentence parsed to diagram.\")\n",
    "\n",
    "        # Step 2: Rewrite the diagram\n",
    "        rewritten_diagram = rewriter(diagram)\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 2: Diagram rewritten.\")\n",
    "\n",
    "        # Step 3: Normalize the diagram\n",
    "        normalized_diagram = rewritten_diagram.normal_form()\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 3: Diagram normalized.\")\n",
    "            \n",
    "        # Step 4: Apply the ansatz to the normalized diagram to get a DisCoPy circuit\n",
    "        circuit = ansatz(normalized_diagram)\n",
    "        if include_debug_prints:\n",
    "            print(\"Step 4: Ansatz applied to create DisCoPy circuit.\")\n",
    "\n",
    "        # Step 5: Convert the DisCoPy circuit to a PennyLane circuit object\n",
    "        temp_qml_circuit = to_qml(circuit)\n",
    "        \n",
    "        num_qubits = temp_qml_circuit._n_qubits\n",
    "        param_structure = temp_qml_circuit._params\n",
    "        device_qubits = target_qubits if target_qubits is not None and target_qubits >= num_qubits else num_qubits\n",
    "        \n",
    "        if include_debug_prints:\n",
    "            print(\"Step 5: DisCoPy circuit converted to PennyLane object.\")\n",
    "            print(f\"Parameter structure: {param_structure}\")\n",
    "            print(f\"Number of qubits: {num_qubits}, Device Wires: {device_qubits}\")\n",
    "        \n",
    "        # Build parameters in the exact same structure as _params\n",
    "        structured_params = []\n",
    "            \n",
    "        if include_debug_prints:\n",
    "            print(\"Step 6: Structured parameters generated.\")\n",
    "            \n",
    "        dev = qml.device(\"lightning.qubit\", wires=device_qubits)\n",
    "        \n",
    "        @qml.qnode(dev)\n",
    "        def qnode_circuit(structured_params):\n",
    "            circuit_func = temp_qml_circuit.make_circuit()\n",
    "            circuit_func(structured_params)\n",
    "            if return_type == 'expval':\n",
    "                return qml.expval(expval)\n",
    "            else:\n",
    "                return qml.state()  # Return the state vector after applying the circuit\n",
    "        \n",
    "        return qnode_circuit, param_structure, device_qubits\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Failed to process circuit: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3256c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def FischerInformation(Fidelity): #Statisitcal Information and DISTANCE metric Fubini-Study Metric/ Wooters Distance\n",
    "    rootFidelity = math.sqrt(Fidelity) #Square root fidelity term\n",
    "    return math.acos(rootFidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be0c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(\n",
    "    overlap, \n",
    "    target_fidelity,  # <--- THIS IS YOUR GROUND TRUTH (is_duplicate: 0 or 1)\n",
    "    structural_disparity, \n",
    "    lambda_penalty\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines Fidelity MSE loss (driven by ground truth) with Structural Disparity Penalty.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Fidelity MSE Loss (Ground Truth Term)\n",
    "    # The (target_fidelity - overlap)**2 term is where the ground truth is applied.\n",
    "    # It drives the measured overlap toward the ground-truth label.\n",
    "    fidelity_loss = (target_fidelity - overlap)**2\n",
    "    \n",
    "    # 2. Structural Penalty Term\n",
    "    structural_penalty = lambda_penalty * structural_disparity\n",
    "    \n",
    "    # 3. Total Loss\n",
    "    return fidelity_loss + structural_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab15ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(qnode_func, params1, params2):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between two quantum states prepared by the same QNode\n",
    "    but with different parameters. This is a shortcut that only works with quantum simulators, for real quantum hardware, use the swap test function.\n",
    "\n",
    "    Args:\n",
    "        qnode_func (qml.QNode): The quantum circuit QNode.\n",
    "        params1 (list): Structured parameters for the first state.\n",
    "        params2 (list): Structured parameters for the second state.\n",
    "\n",
    "    Returns:\n",
    "        float: The estimated squared overlap (fidelity) between the two states.\n",
    "    \"\"\"\n",
    "    # Get the state vectors for both sets of parameters\n",
    "    state1 = qnode_func(params1)\n",
    "    state2 = qnode_func(params2)\n",
    "    \n",
    "    # Calculate the overlap\n",
    "    overlap = np.abs(np.vdot(state1, state2))**2\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a3d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_gradients(qnode_func, params, s1, s2, target_fidelity, structural_disparity):\n",
    "    \"\"\"\n",
    "    Calculates gradients for a pair of sentences with respect to the shared parameters.\n",
    "    Returns a dictionary of gradients indexed by (group, element) tuple.\n",
    "    \"\"\"\n",
    "    gradients = {idx: 0.0 for idx in [(g, e) for g in range(len(params)) for e in range(len(params[g]))]}\n",
    "\n",
    "    # We need to compute the gradient for each parameter\n",
    "    for group_idx, group in enumerate(params):\n",
    "        for elem_idx in range(len(group)):\n",
    "            param_index = (group_idx, elem_idx)\n",
    "\n",
    "            # Shift the parameters for both sentences in the pair\n",
    "            params_plus_s1 = params.copy()\n",
    "            params_plus_s1[group_idx][elem_idx] += np.pi / 2\n",
    "            \n",
    "            params_minus_s1 = params.copy()\n",
    "            params_minus_s1[group_idx][elem_idx] -= np.pi / 2\n",
    "\n",
    "            # Overlap for shifted s1\n",
    "            overlap_plus = calculate_overlap(qnode_func, params_plus_s1, params)\n",
    "            overlap_minus = calculate_overlap(qnode_func, params_minus_s1, params)\n",
    "\n",
    "            # Gradient contribution from S1\n",
    "            grad_s1 = (loss_function(overlap_plus, target_fidelity, structural_disparity, 0) - loss_function(overlap_minus, target_fidelity, structural_disparity, 0)) / 2\n",
    "\n",
    "            # Now, shift the parameters for the second sentence (S2)\n",
    "            params_plus_s2 = params.copy()\n",
    "            params_plus_s2[group_idx][elem_idx] += np.pi / 2\n",
    "            \n",
    "            params_minus_s2 = params.copy()\n",
    "            params_minus_s2[group_idx][elem_idx] -= np.pi / 2\n",
    "\n",
    "            # Overlap for shifted S2\n",
    "            overlap_plus_2 = calculate_overlap(qnode_func, params, params_plus_s2)\n",
    "            overlap_minus_2 = calculate_overlap(qnode_func, params, params_minus_s2)\n",
    "\n",
    "            # Gradient contribution from S2\n",
    "            grad_s2 = (loss_function(overlap_plus_2, target_fidelity, structural_disparity, 0) - loss_function(overlap_minus_2, target_fidelity, structural_disparity, 0)) / 2\n",
    "            \n",
    "            # The total gradient is the sum of contributions from both states\n",
    "            gradients[param_index] = grad_s1 + grad_s2\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bd0183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BASE_LEARNING_RATE = 0.01\n",
    "LAMBDA_PENALTY = 0.1  # Weight for the structural disparity penalty\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "history = [] #for plots\n",
    "def main_workflow(data_pairs, parser, rewriter, ansatz, Tokeniser, base_learning_rate=BASE_LEARNING_RATE, lambda_penalty=LAMBDA_PENALTY):\n",
    "    all_qubit_counts = []\n",
    "    processed_metadata = []\n",
    "    # 1. Preprocessing: Determine the global maximum qubit count        \n",
    "    #Store relevent metadata (target-fidelity, structural disparity) for each pair\n",
    "\n",
    "    for s1, s2, is_duplicate in data_pairs:\n",
    "        _, _, N1 = lambeq_sentence_to_circuit(s1,Tokeniser,ansatz,parser,rewriter,return_type='state')\n",
    "        _, _, N2 = lambeq_sentence_to_circuit(s2,Tokeniser,ansatz,parser,rewriter,return_type='state')\n",
    "        \n",
    "        #calculate structural disparity (absolute qubit difference)\n",
    "        if N1 is not None and N2 is not None:\n",
    "            all_qubit_counts.extend([N1, N2])\n",
    "            structural_disparity = abs(N1 - N2) \n",
    "            processed_metadata.append((s1, s2, is_duplicate, structural_disparity))\n",
    "        else:\n",
    "            #sentence fails to parse, skip it\n",
    "            print(f\"Skipping pair due to parsing error.\")\n",
    "    if not all_qubit_counts:\n",
    "        print(\"No valid sentences found to train on. Exiting.\")\n",
    "        return {}, []\n",
    "    \n",
    "    N_Max = max(all_qubit_counts)\n",
    "    print(f\"\\n--- Starting Training on Dimension N_MAX = {N_Max} ---\")\n",
    "    \n",
    "    # 2. Model Initialization\n",
    "    rep_sentence = data_pairs[0][0] # Use first sentence to get parameter structure\n",
    "    \n",
    "    # The QNode will now be set to size N_MAX for the entire training\n",
    "    qnode_func, param_structure, device_qubits = lambeq_sentence_to_circuit(\n",
    "        rep_sentence, Tokeniser, ansatz, parser, rewriter, \n",
    "        return_type='state', target_qubits=N_Max # <--- Use N_MAX for device\n",
    "    )\n",
    "    \n",
    "    # Check if circuit generation succeeded\n",
    "    if qnode_func is None or device_qubits != N_Max:\n",
    "        print(\"Failed to generate representative circuit. Exiting.\")\n",
    "        return {}\n",
    "        \n",
    "    # 3. Initialize a single set of parameters for the N_MAX dimension\n",
    "    #params = initialize_structured_params(param_structure)\n",
    "    \n",
    "    # 4. Training Loop with Dynamic Feedback\n",
    "    for s1, s2, target_fidelity, structural_disparity in processed_metadata:\n",
    "        #overlap\n",
    "        #overlap = calculate_overlap(qnode_func, params, params)\n",
    "       # qangle = FischerInformation(overlap)\n",
    "        #normalized_qangle = qangle / (math.pi / 2)\n",
    "       # if target_fidelity == 1:\n",
    "         #   eta_dynamic = base_learning_rate * normalized_qangle\n",
    "       # else:\n",
    "        #    eta_dynamic = base_learning_rate * (1 - normalized_qangle)\n",
    "        # Calculate gradients using the shared QNode (qnode_func is already set to N_MAX)\n",
    "        #gradients = calculate_pair_gradients(qnode_func, params, s1, s2, target_fidelity, structural_disparity)\n",
    "        \n",
    "        # Apply the update\n",
    "        #for group_idx, group in enumerate(params):\n",
    "           # for elem_idx in range(len(group)):\n",
    "               # param_index = (group_idx, elem_idx)\n",
    "                #params[group_idx][elem_idx] -= eta_dynamic * gradients[param_index]\n",
    "        \n",
    "        # Logging for monitoring\n",
    "        #total_loss = loss_function(overlap, target_fidelity, structural_disparity, lambda_penalty)\n",
    "        history.append({\n",
    "            'step': len(history) + 1, # Use +1 since len(history) is the index of the next item\n",
    "            's1': s1,\n",
    "            's2': s2,\n",
    "            'target_fidelity': target_fidelity,\n",
    "            'structural_disparity': structural_disparity,\n",
    "           # 'fidelity': overlap,\n",
    "            #'q_angle': qangle,\n",
    "           # 'eta_dyn': eta_dynamic,\n",
    "            #'total_loss': total_loss\n",
    "        })\n",
    "    #trained_params = {N_Max: params} # Store the single trained model\n",
    "    \n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    \n",
    "    # 5. Final Evaluation with Swap Test\n",
    "    print(\"\\n--- Performing Final Swap Tests with Trained Parameters ---\")\n",
    "    # Sample a few pairs for evaluation\n",
    "    sample_pairs = data_pairs[:5] if len(processed_metadata) > 5 else processed_metadata\n",
    "    \n",
    "    for s1, s2, target_fidelity, structural_disparity in sample_pairs:\n",
    "        print(f\"\\nEvaluating Pair: '{s1}' vs. '{s2}' (Label: {target_fidelity}, Disparity: {structural_disparity}\")\n",
    "        \n",
    "        #optimized_params = trained_params[N_Max]\n",
    "        \n",
    "        # Generate State 1 and State 2 vectors, both will be padded to N_MAX\n",
    "        qnode1, _, _ = lambeq_sentence_to_circuit(s1, Tokeniser, ansatz, parser, rewriter, return_type='state', target_qubits=N_Max)\n",
    "        qnode2, _, _ = lambeq_sentence_to_circuit(s2, Tokeniser, ansatz, parser, rewriter, return_type='state', target_qubits=N_Max)\n",
    "        \n",
    "        #vec1 = qnode1(optimized_params)\n",
    "        #vec2 = qnode2(optimized_params)\n",
    "\n",
    "        # Call swap_test, passing N_MAX as the number of qubits for *one* register\n",
    "        #overlap = swap_test(vec1, vec2, N_Max) \n",
    "       # Qangle = FischerInformation(overlap)\n",
    "       # normalized_qangle = Qangle / (math.pi / 2)\n",
    "        \n",
    "        #Calculate final loss\n",
    "        #final_loss = loss_function(overlap, target_fidelity, structural_disparity, lambda_penalty)\n",
    "        #Determine dynamic learning rate based on Qangle\n",
    "      #  print(f\"Final Overlap (Fidelity): {overlap:.4f}\")\n",
    "       # print(f\"Estimated Quantum Angle (Normalized): {normalized_qangle:.4f}\")\n",
    "       # print(f\"Total Penalized Loss: {final_loss:.4f}\")\n",
    "    #return trained_params, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5456e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Generates two key plots showing VQC training convergence and feedback.\"\"\"\n",
    "    \n",
    "    if not history:\n",
    "        print(\"History list is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    # Extracting data from history list\n",
    "    steps = np.array([d['step'] for d in history])\n",
    "    losses = np.array([d['total_loss'] for d in history])\n",
    "    q_angles = np.array([d['q_angle'] for d in history])\n",
    "    eta_dyn = np.array([d['eta_dyn'] for d in history])\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # PLOT 1: Loss Convergence and Dynamic Learning Rate\n",
    "    # -------------------------------------------------------------\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot Loss (Left Y-Axis)\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Training Step (Pair)')\n",
    "    ax1.set_ylabel('Total Penalized Loss', color=color)\n",
    "    ax1.plot(steps, losses, color=color, label='Total Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_yscale('log') # Use log scale for loss for better visualization\n",
    "\n",
    "    # Plot Dynamic Learning Rate (Right Y-Axis)\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel(f'Dynamic Learning Rate (Max={BASE_LEARNING_RATE:.2f})', color=color)  \n",
    "    ax2.plot(steps, eta_dyn, color=color, linestyle='--', alpha=0.6, label='$\\eta_{dyn}$')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    plt.title('VQC Training Convergence and Dynamic Feedback')\n",
    "    fig.tight_layout() \n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.savefig('vqc_convergence_and_eta.png')\n",
    "    plt.close(fig)\n",
    "    print(\"Saved plot: vqc_convergence_and_eta.png\")\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # PLOT 2: Quantum Angle Evolution (Geometric Feedback)\n",
    "    # -------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.set_xlabel('Training Step (Pair)')\n",
    "    ax.set_ylabel('Quantum Angle $\\Theta$ (Fubini-Study Distance)')\n",
    "    \n",
    "    # The Quantum Angle is in [0, pi/2] radians\n",
    "    ax.plot(steps, q_angles, color='tab:green', label='Quantum Angle')\n",
    "    \n",
    "    # Add target lines (This is a simplified view)\n",
    "    ax.axhline(0, color='red', linestyle=':', linewidth=1.5, label='Target Angle (Duplicate)')\n",
    "    ax.axhline(np.pi/2, color='orange', linestyle=':', linewidth=1.5, label='Target Angle (Non-Duplicate)')\n",
    "    \n",
    "    plt.title('Evolution of Quantum Angle (Geometric Distance)')\n",
    "    plt.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.savefig('quantum_angle_evolution.png')\n",
    "    plt.close(fig)\n",
    "    print(\"Saved plot: quantum_angle_evolution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD FUNC DONT USE (FOR REFERENCE ONLY)\n",
    "import spacy\n",
    "if __name__ == \"__main__\":\n",
    "    spacy.load('en_core_web_sm')\n",
    "    Tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(['curry', 'prepositional_phrase', 'determiner'])\n",
    "    # Increase ansatz parameters to get more complex quantum states\n",
    "    # ansatz = StronglyEntanglingAnsatz(\n",
    "    # {AtomicType.NOUN: 2, AtomicType.SENTENCE: 1}, \n",
    "    # n_layers=1\n",
    "    ansatz = IQPAnsatz({AtomicType.NOUN: 2, AtomicType.SENTENCE: 1, AtomicType.CONJUNCTION: 1}, n_layers=1\n",
    ")\n",
    "\n",
    "    print(\"--- Generating states from sentences ---\")\n",
    "\n",
    "    sentence1 = \"Alice loves the dog that Bob purchased.\"\n",
    "    sentence2 = \"Bob loves the dog that Alice sold.\"\n",
    "    sentence3 = \"The big cat sleeps peacefully.\"\n",
    "    sentence4 = \"The small bird sings loudly.\"\n",
    "    sentence5 = \"The lizard basks in the sun.\"\n",
    "    sentence6 = \"The sun shines on the lizard\"\n",
    "    \n",
    "    sentences = [sentence1, sentence2, sentence3, sentence4, sentence5, sentence6]\n",
    "    state_data = {}\n",
    "    for s_idx, sentence in enumerate(sentences):\n",
    "        try:\n",
    "            state_vec, num_qubits = lambeq_sentence_to_circuit(sentence, Tokeniser, ansatz, parser, rewriter)\n",
    "            #state_vec, num_qubits = lambeq_sentence_to_state_vector(sentence, ansatz, parser, rewriter, include_debug_prints=False)\n",
    "            state_data[sentence] = (state_vec, num_qubits)\n",
    "            print(f\"Sentence {s_idx+1}: '{sentence}'\")\n",
    "            print(f\"Generated state with {num_qubits} qubits\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence '{sentence}': {e}\")\n",
    "            state_data[sentence] = (None, None)\n",
    "\n",
    "    print(\"\\n--- Performing Swap Tests ---\")\n",
    "    \n",
    "    # Filter out sentences that did not produce valid states\n",
    "    valid_sentences = [s for s in sentences if state_data[s][0] is not None and state_data[s][1] is not None and state_data[s][1] > 0]\n",
    "\n",
    "    if not valid_sentences:\n",
    "        print(\"No valid multi-qubit states generated. Cannot perform Swap Tests meaningfully.\")\n",
    "    else:\n",
    "        first_num_qubits = state_data[valid_sentences[0]][1]\n",
    "        all_same_qubits = all(state_data[s][1] == first_num_qubits for s in valid_sentences)\n",
    "\n",
    "        if not all_same_qubits:\n",
    "            print(\"\\nWarning: Not all valid sentences resulted in circuits with the same number of qubits.\")\n",
    "            print(\"Swap Test requires states to have the same number of qubits.\")\n",
    "            print(\"Pairs with different qubit counts will be skipped.\")\n",
    "            for s in valid_sentences:\n",
    "                print(f\"  '{s}': {state_data[s][1]} qubits\")\n",
    "\n",
    "        for i in range(len(valid_sentences)):\n",
    "            for j in range(i, len(valid_sentences)):\n",
    "                s1 = valid_sentences[i]\n",
    "                s2 = valid_sentences[j]\n",
    "\n",
    "                vec1, nq1 = state_data[s1]\n",
    "                vec2, nq2 = state_data[s2]\n",
    "\n",
    "                if nq1 == nq2:\n",
    "                    print(f\"\\nSwap Test between '{s1}' and '{s2}':\")\n",
    "                    # Fix: Use nq1 (or nq2, they're equal)\n",
    "                    overlap = swap_test(vec1, vec2, nq1)\n",
    "                    print(overlap)\n",
    "                    Qangle = FischerInformation(overlap) # 0 - pi/2\n",
    "                    \n",
    "                    print(f\"Estimated Quantum Angle: {Qangle:.4f}\")\n",
    "                    if s1 == s2:\n",
    "                        print(\" (Expected to be close to 1.0 for identical states)\")\n",
    "                else:\n",
    "                    print(f\"\\nSkipping Swap Test between '{s1}' ({nq1} qubits) and '{s2}' ({nq2} qubits) due to different qubit counts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e385c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another OLD FUNC DONT USE (FOR REFERENCE ONLY)\n",
    "if __name__ == \"__main__\":\n",
    "    import spacy\n",
    "    spacy.load('en_core_web_sm')\n",
    "    Tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(['curry', 'prepositional_phrase', 'determiner'])\n",
    "    # ansatz = StronglyEntanglingAnsatz(\n",
    "    # {AtomicType.NOUN: 2, AtomicType.SENTENCE: 1}, \n",
    "    # n_layers=1 \n",
    "    trained_params, history = main_workflow(data_pairs, parser, rewriter, ansatz, Tokeniser)\n",
    "    plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ae2404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "# We need a new QNode that calculates the Fidelity/Overlap using the Swap Test.\n",
    "# This QNode replaces the 'calculate_overlap' function for Autodiff training.\n",
    "\n",
    "def overlap_qnode(params, qnode_func_s1, qnode_func_s2, N_MAX):\n",
    "    \"\"\"\n",
    "    Defines a QNode that uses the Swap Test circuit to measure the Fidelity (Overlap).\n",
    "    \n",
    "    This QNode is designed to be fully differentiable for use with qml.AdamOptimizer.\n",
    "    \n",
    "    Args:\n",
    "        params (list): The shared structured parameters (theta_NMAX).\n",
    "        qnode_func_s1 (qml.QNode): The state prep circuit for sentence 1.\n",
    "        qnode_func_s2 (qml.QNode): The state prep circuit for sentence 2.\n",
    "        N_MAX (int): The number of qubits per state register.\n",
    "        \n",
    "    Returns:\n",
    "        float: The expectation value of the ancilla qubit (related to Fidelity).\n",
    "    \"\"\"\n",
    "    \n",
    "    # The total wires needed for the Swap Test circuit\n",
    "    total_wires = 1 + 2 * N_MAX \n",
    "    \n",
    "    # We use a state vector device for exact calculation\n",
    "    dev = qml.device(\"lightning.qubit\", wires=total_wires)\n",
    "    \n",
    "    @qml.qnode(dev, interface=\"autograd\") # Interface \"autograd\" is key for autodiff\n",
    "    def swap_test_qnode():\n",
    "        # Step 1: Prepare the ancilla qubit in superposition\n",
    "        qml.Hadamard(wires=0)\n",
    "        \n",
    "        # Step 2: Prepare the two input states (Psi and Phi) using the shared parameters\n",
    "        # QNode S1 acts on wires 1 to N_MAX\n",
    "        qnode_func_s1(params, wires=range(1, 1 + N_MAX))\n",
    "        \n",
    "        # QNode S2 acts on wires 1 + N_MAX to 1 + 2 * N_MAX\n",
    "        qnode_func_s2(params, wires=range(1 + N_MAX, 1 + 2 * N_MAX))\n",
    "\n",
    "        # Step 3: Apply controlled-SWAP gates\n",
    "        for i in range(N_MAX):\n",
    "            qml.CSWAP(wires=[0, 1 + i, 1 + N_MAX + i])\n",
    "\n",
    "        # Step 4: Apply Hadamard to ancilla\n",
    "        qml.Hadamard(wires=0)\n",
    "        \n",
    "        # Step 5: Measure the expectation value of PauliZ on the ancilla (0)\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "    return swap_test_qnode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    print(f\"Starting preprocessing with a qubit limit of {qubit_limit}...\")\n",
    "    filtered_pairs = []\n",
    "    all_symbols = set()\n",
    "    n_max = 0\n",
    "\n",
    "    for s1, s2, is_duplicate in data_pairs:\n",
    "        try:\n",
    "            d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        # --- THIS IS THE CORRECTED LINE ---\n",
    "        # Get the number of output qubits by taking the length of the codomain.\n",
    "        n1, n2 = len(d1.cod), len(d2.cod)\n",
    "\n",
    "        if n1 <= qubit_limit and n2 <= qubit_limit:\n",
    "            structural_disparity = abs(n1 - n2)\n",
    "            \n",
    "            pair_data = {\n",
    "                's1': s1, 's2': s2, 'label': is_duplicate,\n",
    "                'd1': d1, 'd2': d2,\n",
    "                'structural_disparity': structural_disparity\n",
    "            }\n",
    "            filtered_pairs.append(pair_data)\n",
    "            \n",
    "            all_symbols.update(d1.free_symbols)\n",
    "            all_symbols.update(d2.free_symbols)\n",
    "            n_max = max(n_max, n1, n2)\n",
    "            \n",
    "    if not filtered_pairs:\n",
    "        print(\"Warning: No valid sentence pairs found after filtering. Try a higher qubit_limit.\")\n",
    "        return [], [], 0\n",
    "\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    print(f\"Total unique parameters (symbols) found: {len(all_symbols)}\")\n",
    "    print(f\"N_Max for the filtered dataset is: {n_max}\")\n",
    "    return filtered_pairs, sorted(list(all_symbols), key=lambda s: s.name), n_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c751294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, ParameterDict, Parameter\n",
    "\n",
    "class QNLPModel(Module):\n",
    "    \"\"\"\n",
    "    A QNLP model that holds a shared pool of trainable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, symbols):\n",
    "        super().__init__()\n",
    "        # Create a dictionary of trainable parameters, one for each unique\n",
    "        # symbol found in the training set diagrams.\n",
    "        self.params = ParameterDict({\n",
    "            s.name: Parameter(torch.rand(1) * 2 * torch.pi)\n",
    "            for s in symbols\n",
    "        })\n",
    "\n",
    "    def forward(self, diagram):\n",
    "        \"\"\"\n",
    "        Takes a DisCoPy diagram and returns the concrete numerical parameters\n",
    "        needed to execute its circuit by looking them up from the shared pool.\n",
    "        \"\"\"\n",
    "        # Look up the value for each free_symbol in the diagram\n",
    "        param_values = [self.params[s.name] for s in diagram.free_symbols]\n",
    "        \n",
    "        # If there are no parameters, return an empty tensor\n",
    "        if not param_values:\n",
    "            return torch.tensor([])\n",
    "            \n",
    "        # Concatenate the parameters into a single flat tensor\n",
    "        return torch.cat(param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24894a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # Needed for FischerInformation\n",
    "from lambeq import AtomicType # Needed for FischerInformation\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Make sure you have your FischerInformation function defined\n",
    "def FischerInformation(Fidelity):\n",
    "    rootFidelity = math.sqrt(abs(Fidelity)) # Use abs() for safety\n",
    "    return math.acos(rootFidelity)\n",
    "\n",
    "def new_train_vqc_with_dynamic_lr(data, symbols, n_max=20, base_learning_rate=BASE_LEARNING_RATE, lambda_penalty=LAMBDA_PENALTY, epochs=NUM_EPOCHS):\n",
    "    model = QNLPModel(symbols)\n",
    "    # The optimizer is now only used to hold the parameters and for zero_grad()\n",
    "    optimizer = Adam(model.parameters(), lr=base_learning_rate) \n",
    "    \n",
    "    swap_dev = qml.device(\"default.qubit\", wires=1 + 2 * n_max)\n",
    "    \n",
    "    @qml.qnode(swap_dev, interface=\"torch\")\n",
    "    def swap_test_circuit(params1, params2, circuit1, circuit2):\n",
    "        qml.Hadamard(wires=0)\n",
    "        qml.pennylane_executor.apply_discopy_circuit(circuit1, params1, wires=range(1, 1 + n_max))\n",
    "        qml.pennylane_executor.apply_discopy_circuit(circuit2, params2, wires=range(1 + n_max, 1 + 2 * n_max))\n",
    "        for i in range(n_max):\n",
    "            qml.CSWAP(wires=[0, 1 + i, 1 + n_max + i])\n",
    "        qml.Hadamard(wires=0)\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    history = []\n",
    "    print(\"--- Starting training with CUSTOM DYNAMIC learning rate ---\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, pair in enumerate(data):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            params1 = model(pair['d1'])\n",
    "            params2 = model(pair['d2'])\n",
    "            \n",
    "            measured_overlap = swap_test_circuit(params1, params2, pair['d1'], pair['d2'])\n",
    "            \n",
    "            target_fidelity = pair['label']\n",
    "            structural_disparity = pair['structural_disparity']\n",
    "            fidelity_loss = (measured_overlap - target_fidelity)**2\n",
    "            structural_penalty = LAMBDA_PENALTY * structural_disparity\n",
    "            total_loss = fidelity_loss + structural_penalty\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # --- DYNAMIC LEARNING RATE CALCULATION ---\n",
    "            qangle = FischerInformation(measured_overlap.item())\n",
    "            normalized_qangle = qangle / (math.pi / 2)\n",
    "            \n",
    "            if target_fidelity == 1:\n",
    "                eta_dynamic = base_learning_rate * normalized_qangle\n",
    "            else:\n",
    "                eta_dynamic = base_learning_rate * (1 - normalized_qangle)\n",
    "            \n",
    "            # --- MANUAL PARAMETER UPDATE ---\n",
    "            with torch.no_grad(): # Disable gradient tracking for the update\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.data.add_(-eta_dynamic * p.grad) # The update rule: p = p - lr * grad\n",
    "            \n",
    "            total_loss += total_loss.item()\n",
    "            history.append(total_loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(data)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0802342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 sentences.\n",
      "Starting preprocessing with a qubit limit of 20...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__getattr__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\quantum.py:211\u001b[39m, in \u001b[36mDiagram.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     gate = \u001b[43mGATES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(gate):\n",
      "\u001b[31mKeyError\u001b[39m: 'width'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m data_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sentences1, sentences2, value))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2. Calculate N_Max *before* training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m filtered_data, symbols, n_max = \u001b[43mpreprocess_data_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTokeniser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mansatz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filtered_data:\n\u001b[32m     10\u001b[39m     trained_params, history = new_train_vqc_with_dynamic_lr(filtered_data, symbols)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mpreprocess_data_for_model\u001b[39m\u001b[34m(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m n1, n2 = \u001b[43md1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwidth\u001b[49m.size, d2.width.size\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n1 <= qubit_limit \u001b[38;5;129;01mand\u001b[39;00m n2 <= qubit_limit:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# --- THIS IS THE ADDITION ---\u001b[39;00m\n\u001b[32m     18\u001b[39m     structural_disparity = \u001b[38;5;28mabs\u001b[39m(n1 - n2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\backend\\quantum.py:216\u001b[39m, in \u001b[36mDiagram.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partial(\u001b[38;5;28mself\u001b[39m.apply_gate, gate)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattr__\u001b[39;49m(name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '__getattr__'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Load your data\n",
    "    sentences1, sentences2, value = load_data(DATA_PATH, sample_fraction=0.0001)\n",
    "    data_pairs = list(zip(sentences1, sentences2, value))\n",
    "\n",
    "    # 2. Calculate N_Max *before* training\n",
    "    filtered_data, symbols, n_max = preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter)\n",
    "\n",
    "    if filtered_data:\n",
    "        trained_params, history = new_train_vqc_with_dynamic_lr(filtered_data, symbols)\n",
    "        \n",
    "        # 4. Plot the results\n",
    "        plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

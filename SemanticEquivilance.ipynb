{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36013460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x263706e1730>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment and Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# PennyLane and PyTorch\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.nn import Module, ParameterDict, Parameter\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Lambeq\n",
    "from lambeq.backend.quantum import Diagram as LambeqDiagram\n",
    "from discopy.quantum import gates\n",
    "import spacy\n",
    "import discopy\n",
    "from lambeq import BobcatParser, Rewriter, IQPAnsatz, SpacyTokeniser, AtomicType\n",
    "from discopy.rigid import Ty\n",
    "\n",
    "#data handling and plotting\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Patch for discopy\n",
    "monoidal_module = getattr(discopy, \"monoidal\", None)\n",
    "if monoidal_module:\n",
    "    diagram_class = getattr(monoidal_module, \"Diagram\", None)\n",
    "    if diagram_class and not hasattr(diagram_class, \"is_mixed\"):\n",
    "        diagram_class.is_mixed = property(lambda self: False)\n",
    "\n",
    "# Load spacy model\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540c7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading Function\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    sentences1, sentences2, is_duplicate = [], [], []\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentences1 = df['question1'].astype(str).tolist()\n",
    "        sentences2 = df['question2'].astype(str).tolist()\n",
    "        is_duplicate = df['is_duplicate'].tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(sentences1)} sentence pairs.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], [], []\n",
    "from itertools import combinations\n",
    "\n",
    "def create_combinatorial_pairs(filtered_data):\n",
    "    \"\"\"\n",
    "    Expands a list of filtered pairs into a larger dataset by creating all\n",
    "    possible comparisons between the unique sentences.\n",
    "    \"\"\"\n",
    "    print(\"Generating combinatorial pairs...\")\n",
    "    \n",
    "    original_pairs = {}\n",
    "    unique_diagrams = {}\n",
    "    unique_widths = {}\n",
    "    \n",
    "    for pair in filtered_data:\n",
    "        key = tuple(sorted((pair['s1'], pair['s2'])))\n",
    "        original_pairs[key] = pair['label']\n",
    "        unique_diagrams[pair['s1']] = pair['d1']\n",
    "        unique_diagrams[pair['s2']] = pair['d2']\n",
    "        unique_widths[pair['s1']] = pair['width1']\n",
    "        unique_widths[pair['s2']] = pair['width2']\n",
    "    new_training_data = []\n",
    "    sentence_keys = list(unique_diagrams.keys())\n",
    "    \n",
    "    for s1, s2 in combinations(sentence_keys, 2):\n",
    "        key = tuple(sorted((s1, s2)))\n",
    "        label = original_pairs.get(key, 0)\n",
    "            \n",
    "        d1 = unique_diagrams[s1]\n",
    "        d2 = unique_diagrams[s2]\n",
    "        \n",
    "        #Look up storedwidths for the sentences\n",
    "        w1 = unique_widths[s1]\n",
    "        w2 = unique_widths[s2]\n",
    "        # Calculate and add the structural_disparity for the new pair\n",
    "        new_pair_data = {\n",
    "            's1': s1,\n",
    "            's2': s2,\n",
    "            'label': label,\n",
    "            'd1': d1,\n",
    "            'd2': d2,\n",
    "            'structural_disparity': abs(len(d1.cod) - len(d2.cod)),\n",
    "            'width1': w1,\n",
    "            'width2': w2\n",
    "        }\n",
    "        new_training_data.append(new_pair_data)\n",
    "        \n",
    "    print(f\"Expanded {len(unique_diagrams)} unique sentences into {len(new_training_data)} training pairs.\")\n",
    "    return new_training_data\n",
    "def create_balanced_dataset(combinatorial_data):\n",
    "    \"\"\"\n",
    "    Creates a balanced dataset by undersampling the majority class.\n",
    "    \"\"\"\n",
    "    positives = [pair for pair in combinatorial_data if pair['label'] == 1]\n",
    "    negatives = [pair for pair in combinatorial_data if pair['label'] == 0]\n",
    "    \n",
    "    if not positives:\n",
    "        print(\"Warning: No positive examples found to create a balanced dataset.\")\n",
    "        return []\n",
    "\n",
    "    # Randomly sample a subset of negatives equal in size to the positives\n",
    "    random.shuffle(negatives)\n",
    "    balanced_negatives = negatives[:len(positives)]\n",
    "    \n",
    "    balanced_data = positives + balanced_negatives\n",
    "    random.shuffle(balanced_data)\n",
    "    \n",
    "    print(f\"Created a balanced dataset with {len(positives)} positive and {len(balanced_negatives)} negative pairs.\")\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "def plot_training_history(history):\n",
    "    if not history:\n",
    "        print(\"History is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history, label='Average Loss per Epoch')\n",
    "    plt.title('Training Loss Convergence')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Penalized Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_history(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(len(param_history))\n",
    "    means = [d['mean'] for d in param_history]\n",
    "    stds = [d['std'] for d in param_history]\n",
    "    mins = [d['min'] for d in param_history]\n",
    "    maxs = [d['max'] for d in param_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, means, label='Mean Parameter Value')\n",
    "    plt.fill_between(epochs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.2, label='1 Std. Deviation')\n",
    "    plt.plot(epochs, mins, linestyle='--', color='gray', label='Min/Max Range')\n",
    "    plt.plot(epochs, maxs, linestyle='--', color='gray')\n",
    "    \n",
    "    plt.title('Evolution of Model Parameters During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_evolution_polar(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "    # Use a slightly larger figure for polar plots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Create a polar subplot\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "    epochs = np.array(range(len(param_history)))\n",
    "    # We use modulo 2*pi to keep the angles in one circle\n",
    "    mean_angles = np.array([d['mean'] for d in param_history]) % (2 * np.pi)\n",
    "\n",
    "    # The radius will be the epoch number, so the path spirals outwards over time\n",
    "    # The angle will be the mean parameter value\n",
    "    ax.plot(mean_angles, epochs, 'o-', label='Mean Parameter Path')\n",
    "\n",
    "    # Add markers for the start and end points for clarity\n",
    "    if len(epochs) > 0:\n",
    "        ax.plot(mean_angles[0], epochs[0], 'gX', markersize=12, label='Start')\n",
    "        ax.plot(mean_angles[-1], epochs[-1], 'rX', markersize=12, label='End')\n",
    "\n",
    "    # Formatting the plot\n",
    "    ax.set_theta_zero_location('N') # pyright: ignore[reportAttributeAccessIssue] # Set 0 degrees to the top\n",
    "    ax.set_theta_direction(-1) # pyright: ignore[reportAttributeAccessIssue] # Make it clockwise\n",
    "    ax.set_rlabel_position(0) # pyright: ignore[reportAttributeAccessIssue]\n",
    "    ax.set_rlim(0, len(epochs) * 1.05) # pyright: ignore[reportAttributeAccessIssue] # Set radius limit\n",
    "    ax.set_xlabel(\"Epoch\") # The radius represents the epoch\n",
    "    ax.set_title('Cyclical Evolution of Mean Parameter', pad=20)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "def plot_parameter_deltas(param_history):\n",
    "    if len(param_history) < 2:\n",
    "        print(\"Need at least 2 epochs to plot parameter deltas.\")\n",
    "        return\n",
    "\n",
    "    mean_angles = np.array([d['mean'] for d in param_history])\n",
    "    \n",
    "    # Calculate the shortest angle difference between each epoch\n",
    "    deltas = []\n",
    "    for i in range(1, len(mean_angles)):\n",
    "        prev_angle = mean_angles[i-1]\n",
    "        curr_angle = mean_angles[i]\n",
    "        delta = np.arctan2(np.sin(curr_angle - prev_angle), np.cos(curr_angle - prev_angle))\n",
    "        deltas.append(delta)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # We plot against epochs 1 to N, since the first delta occurs at epoch 1\n",
    "    plt.plot(range(1, len(mean_angles)), deltas, 'o-', label='Change in Mean Parameter (Delta)')\n",
    "    \n",
    "    plt.axhline(0, color='red', linestyle='--', label='No Change')\n",
    "    plt.title('Epoch-to-Epoch Change in Mean Parameter Value')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Shortest Angle Difference (Radians)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, len(mean_angles)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes and plots a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): The ground-truth labels (0s and 1s).\n",
    "        y_pred (np.array): The model's raw probability predictions (overlaps from 0 to 1).\n",
    "        threshold (float): The cutoff for classifying a prediction as 1.\n",
    "    \"\"\"\n",
    "    # Convert probability predictions to binary 0/1 predictions\n",
    "    binary_preds = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, binary_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Non-Duplicate', 'Predicted Duplicate'],\n",
    "                yticklabels=['Actual Non-Duplicate', 'Actual Duplicate'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes and plots the ROC curve and AUC score.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b00316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNLP MODEL AND TRAINING PIPELINE\n",
    "# 1. THE QNLP MODEL CLASS\n",
    "# ===============================================================\n",
    "class QNLPModel(Module):\n",
    "    def __init__(self, symbols):\n",
    "        super().__init__()\n",
    "        # Initialize parameters from a normal distribution with a small standard deviation\n",
    "        self.params = ParameterDict()\n",
    "        for s in symbols:\n",
    "            self.params[s.name.replace('.', '_')] = Parameter(torch.randn(1) * 0.1)\n",
    "\n",
    "    def initialise_weights(self):\n",
    "        \"\"\"Re-initialises the weights of the model.\"\"\"\n",
    "        for param in self.params.values():\n",
    "            param.data.normal_(0, 0.1) # Modify tensor in-place\n",
    "\n",
    "    def forward(self, diagram):\n",
    "        param_values = [self.params[s.name.replace('.', '_')] for s in diagram.free_symbols]\n",
    "        if not param_values:\n",
    "            return torch.tensor([])\n",
    "        return torch.cat(param_values)\n",
    "\n",
    "# ===============================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ===============================================================\n",
    "def FischerInformation(Fidelity):\n",
    "    \"\"\"Calculates the Fubini-Study distance from the fidelity.\"\"\"\n",
    "    rootFidelity = math.sqrt(abs(Fidelity))\n",
    "    clamped_val = max(-1.0, min(1.0, rootFidelity))\n",
    "    return math.acos(clamped_val)\n",
    "\n",
    "def get_diagram_width(diagram):\n",
    "    \"\"\"Calculates the true maximum width of a diagram at any point.\"\"\"\n",
    "    if not diagram.boxes:\n",
    "        return len(diagram.cod)\n",
    "    # The width is the maximum wire index a box acts on.\n",
    "    return max(\n",
    "        [offset + len(box.dom) for box, offset in zip(diagram.boxes, diagram.offsets)]\n",
    "        + [len(diagram.cod)]\n",
    "    )\n",
    "    \n",
    "def execute_discopy_diagram(diagram, params, wires, embedding_method='simple_pad'):\n",
    "    \"\"\"\n",
    "    Executes a DisCoPy/lambeq diagram's instructions on a specific set of wires,\n",
    "    and optionally applies an entangling layer afterwards.\n",
    "    \"\"\"\n",
    "    # Step 1: Execute the original sentence diagram as before\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "\n",
    "    # Step 2: If the method is 'entangle', apply an entangling layer\n",
    "    if embedding_method == 'entangle':\n",
    "        # Entangle every wire with its neighbor in a circular pattern\n",
    "        for i in range(len(wires)):\n",
    "            qml.CNOT(wires=[wires[i], wires[(i + 1) % len(wires)]])\n",
    "# ===============================================================\n",
    "# 3. PREPROCESSING FUNCTION\n",
    "# ===============================================================\n",
    "def preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    print(f\"Starting preprocessing with a qubit limit of {qubit_limit}...\")\n",
    "    filtered_pairs, all_symbols, n_max = [], set(), 0\n",
    "    for s1, s2, is_duplicate in data_pairs:\n",
    "        try:\n",
    "            d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "            width1 = get_diagram_width(d1)\n",
    "            width2 = get_diagram_width(d2)\n",
    "            if width1 <= qubit_limit and width2 <= qubit_limit:\n",
    "                pair_data = {\n",
    "                    's1': s1, 's2': s2, 'label': is_duplicate, 'd1': d1, 'd2': d2,\n",
    "                    'structural_disparity': abs(len(d1.cod) - len(d2.cod)),\n",
    "                    'width1': width1,\n",
    "                    'width2': width2\n",
    "                }\n",
    "                filtered_pairs.append(pair_data)\n",
    "                all_symbols.update(d1.free_symbols)\n",
    "                all_symbols.update(d2.free_symbols)\n",
    "                n_max = max(n_max, width1, width2)\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    print(f\"Total unique parameters (symbols) found: {len(all_symbols)}\")\n",
    "    print(f\"N_Max for the filtered dataset is: {n_max}\")\n",
    "    return filtered_pairs, sorted(list(all_symbols), key=lambda s: s.name), n_max\n",
    "\n",
    "# ===============================================================\n",
    "# 4. THE TRAINING FUNCTION (with Adam & Param Tracking)\n",
    "# ===============================================================\n",
    "def train_model(data, symbols, n_max, base_learning_rate, lambda_penalty, epochs, embedding_method='entangle'):\n",
    "    model = QNLPModel(symbols)\n",
    "    optimizer = Adam(model.parameters(), lr=base_learning_rate) \n",
    "    swap_dev = qml.device(\"lightning.qubit\", wires=1 + 2 * n_max)\n",
    "\n",
    "    loss_history = []\n",
    "    param_history = []\n",
    "    print(\"--- Starting training with EXPLICIT gradient calculation ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch, num_trained_pairs = 0, 0\n",
    "        for i, pair in enumerate(data):\n",
    "            \n",
    "            params1_torch, params2_torch = model(pair['d1']), model(pair['d2'])\n",
    "            if params1_torch.nelement() == 0 or params2_torch.nelement() == 0:\n",
    "                continue\n",
    "\n",
    "            num_trained_pairs += 1\n",
    "\n",
    "            def cost_fn(p1_np, p2_np):\n",
    "                @qml.qnode(swap_dev)\n",
    "                def swap_test_qnode(p1, p2):\n",
    "                    qml.Hadamard(wires=0)\n",
    "                    \n",
    "                    # Execute the diagrams with the chosen embedding method\n",
    "                    execute_discopy_diagram(\n",
    "                        pair['d1'], p1, wires=range(1, 1 + n_max), \n",
    "                        embedding_method=embedding_method)\n",
    "                    execute_discopy_diagram(\n",
    "                        pair['d2'], p2, wires=range(1 + n_max, 1 + 2 * n_max), \n",
    "                        embedding_method=embedding_method)\n",
    "\n",
    "                    for j in range(n_max):\n",
    "                        qml.CSWAP(wires=[0, 1 + j, 1 + n_max + j])\n",
    "                    qml.Hadamard(wires=0)\n",
    "                    return qml.expval(qml.PauliZ(0))\n",
    "                \n",
    "                measured_overlap = swap_test_qnode(p1_np, p2_np)\n",
    "                fidelity_loss = (measured_overlap - pair['label'])**2\n",
    "                structural_penalty = lambda_penalty * pair['structural_disparity']\n",
    "                return fidelity_loss + structural_penalty\n",
    "\n",
    "            params1_np = params1_torch.detach().numpy()\n",
    "            params2_np = params2_torch.detach().numpy()\n",
    "\n",
    "            grad_fn = qml.grad(cost_fn, argnum=[0, 1])\n",
    "            grads_np_tuple = grad_fn(params1_np, params2_np)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            grad_dict = {}\n",
    "            param_idx = 0\n",
    "            for sym in pair['d1'].free_symbols:\n",
    "                sanitized_name = sym.name.replace('.', '_')\n",
    "                grad_dict[sanitized_name] = grads_np_tuple[0][param_idx]\n",
    "                param_idx += 1\n",
    "            \n",
    "            param_idx = 0\n",
    "            for sym in pair['d2'].free_symbols:\n",
    "                sanitized_name = sym.name.replace('.', '_')\n",
    "                if sanitized_name in grad_dict:\n",
    "                    grad_dict[sanitized_name] += grads_np_tuple[1][param_idx]\n",
    "                else:\n",
    "                    grad_dict[sanitized_name] = grads_np_tuple[1][param_idx]\n",
    "                param_idx += 1\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in grad_dict:\n",
    "                    param.grad = torch.tensor(grad_dict[name])\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_val = cost_fn(params1_np, params2_np)\n",
    "            total_loss_epoch += loss_val\n",
    "        \n",
    "        avg_loss = total_loss_epoch / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        all_params = torch.cat([p.data.flatten() for p in model.parameters()]).detach().numpy()\n",
    "        if all_params.size > 0:\n",
    "            param_history.append({'mean': np.mean(all_params), 'std': np.std(all_params),\n",
    "                                  'min': np.min(all_params), 'max': np.max(all_params)})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Penalized Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, loss_history, param_history\n",
    "# ===============================================================\n",
    "# 5. Inference Function\n",
    "# ===============================================================\n",
    "def evaluate_model(model, test_data, n_max, embedding_method='entangle'):\n",
    "    \"\"\"\n",
    "    Evaluates a trained model on a test dataset without updating weights.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Evaluation on Test Set ---\")\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    swap_dev = qml.device(\"lightning.qubit\", wires=1 + 2 * n_max)\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for i, pair in enumerate(test_data):\n",
    "            params1, params2 = model(pair['d1']), model(pair['d2'])\n",
    "            \n",
    "            # Skip pairs with no parameters, as they can't be processed\n",
    "            if params1.nelement() == 0 or params2.nelement() == 0:\n",
    "                continue\n",
    "\n",
    "            @qml.qnode(swap_dev, interface=\"torch\")\n",
    "            def swap_test_qnode(p1, p2):\n",
    "                qml.Hadamard(wires=0)\n",
    "                execute_discopy_diagram(pair['d1'], p1, wires=range(1, 1 + n_max))\n",
    "                execute_discopy_diagram(pair['d2'], p2, wires=range(1 + n_max, 1 + 2 * n_max))\n",
    "                for j in range(n_max):\n",
    "                    qml.CSWAP(wires=[0, 1 + j, 1 + n_max + j])\n",
    "                qml.Hadamard(wires=0)\n",
    "                return qml.expval(qml.PauliZ(0))\n",
    "            \n",
    "            # Get the model's prediction (the overlap)\n",
    "            measured_overlap = swap_test_qnode(params1, params2)\n",
    "            \n",
    "            predictions.append(measured_overlap.item())\n",
    "            true_labels.append(pair['label'])\n",
    "\n",
    "    # --- Calculate and Report Metrics ---\n",
    "    if not predictions:\n",
    "        print(\"No valid pairs in the test set to evaluate.\")\n",
    "        return\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "    mse = np.mean((predictions - true_labels)**2)\n",
    "    print(f\"Test Set Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "    # Calculate simple accuracy (is overlap > 0.5 a correct prediction?)\n",
    "    binary_preds = (predictions > 0.5).astype(int)\n",
    "    accuracy = np.mean(binary_preds == true_labels) * 100\n",
    "    print(f\"Test Set Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\n--- Evaluation Plots ---\")\n",
    "    plot_confusion_matrix(true_labels, predictions)\n",
    "    plot_roc_curve(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e94f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "{'data': {'path': '<ABSOLUTE_PATH_TO_YOUR_QUESTIONS.CSV>', 'sample_fraction': 0.01, 'qubit_limit': 12}, 'qnlp': {'n_layers': 1, 'embedding_method': 'entangle', 'rewrite_rules': ['curry', 'prepositional_phrase', 'determiner']}, 'training': {'epochs': 15, 'base_learning_rate': 0.01, 'lambda_penalty': 0.1}}\n",
      "Loaded 4044 sentence pairs.\n",
      "Starting preprocessing with a qubit limit of 12...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Define the path to your configuration file\u001b[39;00m\n\u001b[32m     70\u001b[39m     config_file_path = \u001b[33m'\u001b[39m\u001b[33mconfig.yaml\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(config_path)\u001b[39m\n\u001b[32m     28\u001b[39m sentences1, sentences2, value = load_data(\n\u001b[32m     29\u001b[39m     config[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     30\u001b[39m     sample_fraction=config[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msample_fraction\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     31\u001b[39m )\n\u001b[32m     32\u001b[39m data_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sentences1, sentences2, value))\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m filtered_data, symbols, n_max = \u001b[43mpreprocess_data_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokeniser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mansatz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqubit_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqubit_limit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filtered_data \u001b[38;5;129;01mand\u001b[39;00m n_max > \u001b[32m0\u001b[39m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# --- 4. Create Datasets ---\u001b[39;00m\n\u001b[32m     41\u001b[39m     combinatorial_data = create_combinatorial_pairs(filtered_data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mpreprocess_data_for_model\u001b[39m\u001b[34m(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s1, s2, is_duplicate \u001b[38;5;129;01min\u001b[39;00m data_pairs:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m         d1 = ansatz(rewriter(\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43msentence2diagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTokeniser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenise_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenised\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m))\n\u001b[32m     75\u001b[39m         d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=\u001b[38;5;28;01mTrue\u001b[39;00m)))\n\u001b[32m     76\u001b[39m         width1 = get_diagram_width(d1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py:236\u001b[39m, in \u001b[36mCCGParser.sentence2diagram\u001b[39m\u001b[34m(self, sentence, tokenised, planar, collapse_noun_phrases, suppress_exceptions)\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33m`tokenised` set to `True`, but variable \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    233\u001b[39m                          \u001b[33m'\u001b[39m\u001b[33m`sentence` does not have type \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    234\u001b[39m                          \u001b[33m'\u001b[39m\u001b[33m`list[str]`.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    235\u001b[39m     sent: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = [\u001b[38;5;28mstr\u001b[39m(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence]\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentences2diagrams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplanar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplanar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollapse_noun_phrases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollapse_noun_phrases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenised\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVerbosityLevel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSUPPRESS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py:167\u001b[39m, in \u001b[36mCCGParser.sentences2diagrams\u001b[39m\u001b[34m(self, sentences, tokenised, planar, collapse_noun_phrases, suppress_exceptions, verbose)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msentences2diagrams\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    128\u001b[39m                        sentences: SentenceBatchType,\n\u001b[32m    129\u001b[39m                        tokenised: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m                        suppress_exceptions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    133\u001b[39m                        verbose: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mlist\u001b[39m[Diagram | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    134\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse multiple sentences into a list of lambeq diagrams.\u001b[39;00m\n\u001b[32m    135\u001b[39m \n\u001b[32m    136\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \n\u001b[32m    166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     trees = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentences2trees\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43msuppress_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtokenised\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     diagrams: \u001b[38;5;28mlist\u001b[39m[Diagram | \u001b[38;5;28;01mNone\u001b[39;00m] = []\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\text2diagram\\model_based_reader\\bobcat_parser.py:252\u001b[39m, in \u001b[36mBobcatParser.sentences2trees\u001b[39m\u001b[34m(self, sentences, tokenised, suppress_exceptions, verbose)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose == VerbosityLevel.TEXT.value:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTagging sentences.\u001b[39m\u001b[33m'\u001b[39m, file=sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m tag_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m tags = tag_results.tags\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose == VerbosityLevel.TEXT.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\bobcat\\tagger.py:411\u001b[39m, in \u001b[36mTagger.__call__\u001b[39m\u001b[34m(self, inputs, batch_size, verbose)\u001b[39m\n\u001b[32m    403\u001b[39m output = TaggerOutput(tags=\u001b[38;5;28mself\u001b[39m.model.config.tags,\n\u001b[32m    404\u001b[39m                       cats=\u001b[38;5;28mself\u001b[39m.model.config.cats,\n\u001b[32m    405\u001b[39m                       sentences=[])\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(inputs), batch_size,\n\u001b[32m    408\u001b[39m                 desc=\u001b[33m'\u001b[39m\u001b[33mTagging sentences\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    409\u001b[39m                 leave=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    410\u001b[39m                 disable=verbose != VerbosityLevel.PROGRESS.value):\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     output.sentences.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\bobcat\\tagger.py:342\u001b[39m, in \u001b[36mTagger.parse\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse a batch of sentences.\"\"\"\u001b[39;00m\n\u001b[32m    341\u001b[39m encodings = \u001b[38;5;28mself\u001b[39m.prepare_inputs(inputs, word_mask=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m tag_output: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[TagListT]] = []\n\u001b[32m    346\u001b[39m span_output: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[TagListT]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\lambeq\\bobcat\\tagger.py:141\u001b[39m, in \u001b[36mBertForChartClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, tag_labels, span_labels, word_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    125\u001b[39m     input_ids: torch.LongTensor | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     return_dict: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m ) -> ChartClassifierOutput | \u001b[38;5;28mtuple\u001b[39m[Any, ...]:\n\u001b[32m    138\u001b[39m     return_dict = (return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    139\u001b[39m                    \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m word_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# remove ignored tensors and pack remaining ones\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    994\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1009\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:651\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    649\u001b[39m past_key_value = past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:595\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    592\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    593\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:250\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:507\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jash\\Documents\\Research\\Semantic Equivilance\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Click this to run model\n",
    "# ===============================================================\n",
    "# MAIN EXECUTION BLOCK (with Configuration File)\n",
    "# ===============================================================\n",
    "def main(config_path: str):\n",
    "    \"\"\"Main function to run the entire workflow from a config file.\"\"\"\n",
    "    \n",
    "    # --- 1. Load Configuration ---\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration loaded:\")\n",
    "    print(config)\n",
    "\n",
    "    # --- 2. Initialize Objects from Config ---\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser()\n",
    "    rewriter = Rewriter(config['qnlp']['rewrite_rules'])\n",
    "    \n",
    "    # Define the mapping from grammatical types to qubits\n",
    "    N = AtomicType.NOUN\n",
    "    S = AtomicType.SENTENCE\n",
    "    OB_MAP: dict[Ty, int] = { N: 1, S: 1 }\n",
    "\n",
    "    # Define the ansatz from config\n",
    "    ansatz = IQPAnsatz(OB_MAP, n_layers=config['qnlp']['n_layers'])\n",
    "\n",
    "    # --- 3. Load and Preprocess Data ---\n",
    "    sentences1, sentences2, value = load_data(\n",
    "        config['data']['path'], \n",
    "        sample_fraction=config['data']['sample_fraction']\n",
    "    )\n",
    "    data_pairs = list(zip(sentences1, sentences2, value))\n",
    "    \n",
    "    filtered_data, symbols, n_max = preprocess_data_for_model(\n",
    "        data_pairs, tokeniser, ansatz, parser, rewriter, \n",
    "        qubit_limit=config['data']['qubit_limit']\n",
    "    )\n",
    "    \n",
    "    if filtered_data and n_max > 0:\n",
    "        # --- 4. Create Datasets ---\n",
    "        combinatorial_data = create_combinatorial_pairs(filtered_data)\n",
    "        training_data = create_balanced_dataset(combinatorial_data)\n",
    "        \n",
    "        print(f\"\\nUsing {len(training_data)} pairs for training.\")\n",
    "        \n",
    "        # --- 5. Train Model ---\n",
    "        trained_model, loss_history, param_history = train_model(\n",
    "            training_data, \n",
    "            symbols, \n",
    "            n_max, \n",
    "            base_learning_rate=config['training']['base_learning_rate'],\n",
    "            lambda_penalty=config['training']['lambda_penalty'],\n",
    "            epochs=config['training']['epochs'],\n",
    "            embedding_method=config['qnlp']['embedding_method']\n",
    "        )\n",
    "        \n",
    "        # --- 6. Evaluate and Plot ---\n",
    "        test_data = combinatorial_data\n",
    "        evaluate_model(trained_model, test_data, n_max) # Note: evaluate_model also needs the embedding_method param\n",
    "        \n",
    "        print(\"\\n--- Training Analysis ---\")\n",
    "        plot_training_history(loss_history)\n",
    "        plot_parameter_evolution_polar(param_history)\n",
    "        plot_parameter_deltas(param_history)\n",
    "    else:\n",
    "        print(\"\\nNo data to train on. Please check your config file.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define the path to your configuration file\n",
    "    config_file_path = 'config.yaml'\n",
    "    main(config_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

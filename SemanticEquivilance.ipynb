{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a395963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')\n",
      "Loaded 404351 sentences.\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "def load_data(csv_file):\n",
    "    \"\"\"Loads Question Pairs from a CSV file\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to csv_file\n",
    "    Returns:\n",
    "        tuple: A tuple containing supervised data pairs\n",
    "        returns [],[] on error\n",
    "    \"\"\"\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    is_duplicate = []\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        #print(\"Column names:\", df.columns)\n",
    "        sentence1_series = df['question1']\n",
    "        sentence2_series = df['question2']\n",
    "        is_duplicate_series = df['is_duplicate']\n",
    "        \n",
    "        sentences1 = sentence1_series.tolist()\n",
    "        sentences2 = sentence2_series.tolist()\n",
    "        is_duplicate = is_duplicate_series.tolist()\n",
    "        \n",
    "        if len(sentences1) != len(sentences2):\n",
    "            raise ValueError(\"The number of sentences in question1 and question2 do not match.\")\n",
    "        else:\n",
    "            print(f\"Loaded {len(sentences1)} sentences.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Wrong Path\")\n",
    "        return [],[],[]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An {e} Error Occurred\")\n",
    "        return [],[],[]\n",
    "\n",
    "DATA_PATH = r'C:/Users/Jash\\Documents/Research\\Semantic Equivilance\\SemanticEquivilance/question_pairs/questions.csv'\n",
    "sentences1, sentences2, value = load_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54dcd9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 processes.\n"
     ]
    }
   ],
   "source": [
    "from lambeq import BobcatParser, SpacyTokeniser, Rewriter, AtomicType, IQPAnsatz\n",
    "from lambeq.backend.quantum import Diagram as quantum_circuit\n",
    "from typing import Optional\n",
    "import os, time, multiprocessing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" #environment variable for multithreading\n",
    "\n",
    "#Global data sequencing variables\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(f\"Using {num_processes} processes.\")\n",
    "\n",
    "_tokenizer = None\n",
    "_parser = None\n",
    "_rewriter = None\n",
    "_ansatz = None\n",
    "\n",
    "def _initializer():\n",
    "    global _tokenizer, _parser, _rewriter, _ansatz\n",
    "    _tokenizer = SpacyTokeniser()  # Initialize tokenizer\n",
    "    _parser = BobcatParser(verbose=\"suppress\")  # Initialize parser \n",
    "    _rewriter = Rewriter(['prepositional_phrase', 'determiner'])  # Initialize rewriter\n",
    "    _ansatz = IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 1}, n_layers=2, n_single_qubit_params=3)  # Initialize ansatz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42ee7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sentence: str, tokeniser, parser, rewriter, ansatz) -> Optional[quantum_circuit]:\n",
    "    \"\"\"Process a single sentence to a diagram.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Sentence to be converted to a diagram.\n",
    "\n",
    "    Returns:\n",
    "        Optional[quantum_circuit]: Either returns a diagram or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentence = sentence.strip().lower()\n",
    "        tokens = tokeniser.tokenise_sentence(sentence)\n",
    "        diagram = parser.sentence2diagram(tokens, tokenised=True)\n",
    "        if diagram is not None:\n",
    "            diagram = rewriter(diagram)\n",
    "            normalised_diagram = diagram.normal_form()\n",
    "            curry_functor = Rewriter(['curry'])\n",
    "            curried_diagram = curry_functor(normalised_diagram)\n",
    "            circuit = ansatz(curried_diagram)\n",
    "            return circuit\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence {sentence}\")\n",
    "        return None\n",
    "def _process_data_for_pool(sentence: str) -> Optional[quantum_circuit]:\n",
    "    \"\"\"Process a single sentence for the multiprocessing pool.\"\"\"\n",
    "    return process_data(sentence, _tokenizer, _parser, _rewriter, _ansatz)\n",
    "\n",
    "def process_sentences(sentences: list[str]) -> list[Optional[quantum_circuit]]:\n",
    "    \"\"\"Process sentences in parallel using multiprocessing.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[str]): List of sentences to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list[Optional[quantum_circuit]]: List of processed diagrams or None for errors.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    with multiprocessing.Pool(processes=num_processes, initializer=_initializer) as pool:\n",
    "        results = pool.map(_process_data_for_pool, sentences)\n",
    "        end_time = time.time()\n",
    "        print(f\"Processed {len(sentences)} sentences in {end_time - start_time:.4f} seconds.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bbccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentences1...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Process sentences1 and sentences2 in parallel\n",
    "    print(\"Processing sentences1...\")\n",
    "    diagrams1 = process_sentences(sentences1)\n",
    "    print(\"Processing sentences2...\")\n",
    "    diagrams2 = process_sentences(sentences2)\n",
    "\n",
    "    # Filter out None values (errors)\n",
    "    diagrams1 = [d for d in diagrams1 if d is not None]\n",
    "    diagrams2 = [d for d in diagrams2 if d is not None]\n",
    "\n",
    "    print(f\"Processed {len(diagrams1)} diagrams from sentences1 and {len(diagrams2)} from sentences2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

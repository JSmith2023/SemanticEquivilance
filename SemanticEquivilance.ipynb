{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "36013460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x278f1dac440>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment and Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PennyLane and PyTorch\n",
    "import pennylane as qml\n",
    "import torch\n",
    "from torch.nn import Module, ParameterDict, Parameter\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Lambeq\n",
    "import discopy\n",
    "from lambeq import BobcatParser, Rewriter, IQPAnsatz, SpacyTokeniser, AtomicType\n",
    "from discopy.rigid import Ty\n",
    "import spacy\n",
    "\n",
    "# Patch for discopy\n",
    "monoidal_module = getattr(discopy, \"monoidal\", None)\n",
    "if monoidal_module:\n",
    "    diagram_class = getattr(monoidal_module, \"Diagram\", None)\n",
    "    if diagram_class and not hasattr(diagram_class, \"is_mixed\"):\n",
    "        diagram_class.is_mixed = property(lambda self: False)\n",
    "\n",
    "# Load spacy model\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "540c7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading Function\n",
    "def load_data(csv_file, sample_fraction=1.0):\n",
    "    sentences1, sentences2, is_duplicate = [], [], []\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        if sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        sentences1 = df['question1'].astype(str).tolist()\n",
    "        sentences2 = df['question2'].astype(str).tolist()\n",
    "        is_duplicate = df['is_duplicate'].tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(sentences1)} sentence pairs.\")\n",
    "        return sentences1, sentences2, is_duplicate\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNLP MODEL AND TRAINING PIPELINE\n",
    "# 1. THE QNLP MODEL CLASS\n",
    "# ===============================================================\n",
    "class QNLPModel(Module):\n",
    "    def __init__(self, symbols):\n",
    "        super().__init__()\n",
    "        # Initialize parameters from a normal distribution with a small standard deviation\n",
    "        self.params = ParameterDict()\n",
    "        for s in symbols:\n",
    "            self.params[s.name.replace('.', '_')] = Parameter(torch.randn(1) * 0.1)\n",
    "\n",
    "    def initialise_weights(self):\n",
    "        \"\"\"Re-initialises the weights of the model.\"\"\"\n",
    "        for param in self.params.values():\n",
    "            param.data.normal_(0, 0.1) # Modify tensor in-place\n",
    "\n",
    "    def forward(self, diagram):\n",
    "        param_values = [self.params[s.name.replace('.', '_')] for s in diagram.free_symbols]\n",
    "        if not param_values:\n",
    "            return torch.tensor([])\n",
    "        return torch.cat(param_values)\n",
    "\n",
    "# ===============================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ===============================================================\n",
    "def FischerInformation(Fidelity):\n",
    "    \"\"\"Calculates the Fubini-Study distance from the fidelity.\"\"\"\n",
    "    rootFidelity = math.sqrt(abs(Fidelity))\n",
    "    clamped_val = max(-1.0, min(1.0, rootFidelity))\n",
    "    return math.acos(clamped_val)\n",
    "\n",
    "def get_diagram_width(diagram):\n",
    "    \"\"\"Calculates the true maximum width of a diagram at any point.\"\"\"\n",
    "    if not diagram.boxes:\n",
    "        return len(diagram.cod)\n",
    "    # The width is the maximum wire index a box acts on.\n",
    "    return max(\n",
    "        [offset + len(box.dom) for box, offset in zip(diagram.boxes, diagram.offsets)]\n",
    "        + [len(diagram.cod)]\n",
    "    )\n",
    "\n",
    "def execute_discopy_diagram(diagram, params, wires):\n",
    "    \"\"\"Executes a DisCoPy/lambeq diagram's instructions on a specific set of wires.\"\"\"\n",
    "    wire_map = {i: w for i, w in enumerate(wires)}\n",
    "    param_idx = 0\n",
    "    for gate, offset in zip(diagram.boxes, diagram.offsets):\n",
    "        if hasattr(qml, gate.name):\n",
    "            op = getattr(qml, gate.name)\n",
    "            gate_params = []\n",
    "            num_params = len(gate.free_symbols)\n",
    "            if num_params > 0:\n",
    "                gate_params = params[param_idx : param_idx + num_params]\n",
    "                param_idx += num_params\n",
    "            target_wires = [wire_map[i + offset] for i in range(len(gate.dom))]\n",
    "            op(*gate_params, wires=target_wires)\n",
    "\n",
    "# ===============================================================\n",
    "# 3. PREPROCESSING FUNCTION\n",
    "# ===============================================================\n",
    "def preprocess_data_for_model(data_pairs, Tokeniser, ansatz, parser, rewriter, qubit_limit=20):\n",
    "    print(f\"Starting preprocessing with a qubit limit of {qubit_limit}...\")\n",
    "    filtered_pairs, all_symbols, n_max = [], set(), 0\n",
    "    for s1, s2, is_duplicate in data_pairs:\n",
    "        try:\n",
    "            d1 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s1), tokenised=True)))\n",
    "            d2 = ansatz(rewriter(parser.sentence2diagram(Tokeniser.tokenise_sentence(s2), tokenised=True)))\n",
    "            width1 = get_diagram_width(d1)\n",
    "            width2 = get_diagram_width(d2)\n",
    "            if width1 <= qubit_limit and width2 <= qubit_limit:\n",
    "                pair_data = {\n",
    "                    's1': s1, 's2': s2, 'label': is_duplicate, 'd1': d1, 'd2': d2,\n",
    "                    'structural_disparity': abs(len(d1.cod) - len(d2.cod))\n",
    "                }\n",
    "                filtered_pairs.append(pair_data)\n",
    "                all_symbols.update(d1.free_symbols)\n",
    "                all_symbols.update(d2.free_symbols)\n",
    "                n_max = max(n_max, width1, width2)\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"Preprocessing complete. Found {len(filtered_pairs)} valid pairs.\")\n",
    "    print(f\"Total unique parameters (symbols) found: {len(all_symbols)}\")\n",
    "    print(f\"N_Max for the filtered dataset is: {n_max}\")\n",
    "    return filtered_pairs, sorted(list(all_symbols), key=lambda s: s.name), n_max\n",
    "\n",
    "# ===============================================================\n",
    "# 4. THE TRAINING FUNCTION (with Fischer LR & Param Tracking)\n",
    "# ===============================================================\n",
    "def train_with_fischer_lr(data, symbols, n_max, base_learning_rate=0.01, lambda_penalty=0.1, epochs=10):\n",
    "    model = QNLPModel(symbols)\n",
    "    swap_dev = qml.device(\"lightning.qubit\", wires=1 + 2 * n_max)\n",
    "\n",
    "    loss_history = []\n",
    "    param_history = []\n",
    "    print(\"--- Starting training with CUSTOM Fischer Information LR ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0\n",
    "        num_trained_pairs = 0\n",
    "        for i, pair in enumerate(data):\n",
    "            params1_torch, params2_torch = model(pair['d1']), model(pair['d2'])\n",
    "            if params1_torch.nelement() == 0 or params2_torch.nelement() == 0:\n",
    "                continue\n",
    "            num_trained_pairs += 1\n",
    "\n",
    "            # Define a pure cost function that PennyLane can differentiate\n",
    "            def cost_fn(p1_np, p2_np, get_overlap=False):\n",
    "                @qml.qnode(swap_dev)\n",
    "                def swap_test_qnode(p1, p2):\n",
    "                    qml.Hadamard(wires=0)\n",
    "                    execute_discopy_diagram(pair['d1'], p1, wires=range(1, 1 + n_max))\n",
    "                    execute_discopy_diagram(pair['d2'], p2, wires=range(1 + n_max, 1 + 2 * n_max))\n",
    "                    for i in range(n_max):\n",
    "                        qml.CSWAP(wires=[0, 1 + i, 1 + n_max + i])\n",
    "                    qml.Hadamard(wires=0)\n",
    "                    return qml.expval(qml.PauliZ(0))\n",
    "                \n",
    "                measured_overlap = swap_test_qnode(p1_np, p2_np)\n",
    "                \n",
    "                if get_overlap:\n",
    "                    return measured_overlap\n",
    "\n",
    "                fidelity_loss = (measured_overlap - pair['label'])**2\n",
    "                structural_penalty = lambda_penalty * pair['structural_disparity']\n",
    "                return fidelity_loss + structural_penalty\n",
    "\n",
    "            params1_np = params1_torch.detach().numpy()\n",
    "            params2_np = params2_torch.detach().numpy()\n",
    "            \n",
    "            # Get gradients and loss\n",
    "            grad_fn = qml.grad(cost_fn, argnum=[0, 1])\n",
    "            grads_np_tuple = grad_fn(params1_np, params2_np)\n",
    "            loss_val = cost_fn(params1_np, params2_np)\n",
    "            total_loss_epoch += loss_val\n",
    "\n",
    "            # Get overlap for Fischer Information calculation\n",
    "            measured_overlap_val = cost_fn(params1_np, params2_np, get_overlap=True)\n",
    "            \n",
    "            # Calculate dynamic learning rate\n",
    "            qangle = FischerInformation(measured_overlap_val)\n",
    "            normalized_qangle = qangle / (math.pi / 2)\n",
    "            eta_dynamic = base_learning_rate * normalized_qangle if pair['label'] == 1 else base_learning_rate * (1 - normalized_qangle)\n",
    "\n",
    "            # Manually update the model's parameters\n",
    "            with torch.no_grad():\n",
    "                grad_dict = {}\n",
    "                param_idx = 0\n",
    "                for sym in pair['d1'].free_symbols:\n",
    "                    sanitized_name = sym.name.replace('.', '_')\n",
    "                    grad_dict[sanitized_name] = grads_np_tuple[0][param_idx] # pyright: ignore[reportGeneralTypeIssues]\n",
    "                    param_idx += 1\n",
    "                \n",
    "                param_idx = 0\n",
    "                for sym in pair['d2'].free_symbols:\n",
    "                    sanitized_name = sym.name.replace('.', '_')\n",
    "                    if sanitized_name in grad_dict:\n",
    "                        grad_dict[sanitized_name] += grads_np_tuple[1][param_idx] # pyright: ignore[reportGeneralTypeIssues]\n",
    "                    else:\n",
    "                        grad_dict[sanitized_name] = grads_np_tuple[1][param_idx] # pyright: ignore[reportGeneralTypeIssues]\n",
    "                    param_idx += 1\n",
    "\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in grad_dict:\n",
    "                        param.data.add_(-eta_dynamic * torch.tensor(grad_dict[name]))\n",
    "        \n",
    "        # Track parameter statistics at the end of each epoch\n",
    "        all_params = torch.cat([p.data.flatten() for p in model.parameters()]).numpy()\n",
    "        param_history.append({\n",
    "            'mean': np.mean(all_params), 'std': np.std(all_params),\n",
    "            'min': np.min(all_params), 'max': np.max(all_params)\n",
    "        })\n",
    "\n",
    "        avg_loss = total_loss_epoch / num_trained_pairs if num_trained_pairs > 0 else 0\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Penalized Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, loss_history, param_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d8aa2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Legacy methods for reference\n",
    "def loss_function_manual(overlap, target_fidelity, structural_disparity, lambda_penalty):\n",
    "    \"\"\"\n",
    "    Classical loss function used by the manual gradient calculator.\n",
    "    \"\"\"\n",
    "    fidelity_loss = (target_fidelity - overlap)**2\n",
    "    structural_penalty = lambda_penalty * structural_disparity\n",
    "    return fidelity_loss + structural_penalty\n",
    "\n",
    "def calculate_overlap_classical(qnode_func, params1, params2):\n",
    "    \"\"\"\n",
    "    Calculates overlap on a statevector simulator (classical shortcut).\n",
    "    \"\"\"\n",
    "    state1 = qnode_func(params1)\n",
    "    state2 = qnode_func(params2)\n",
    "    return np.abs(np.vdot(state1, state2))**2\n",
    "\n",
    "def calculate_pair_gradients(qnode_func, params, s1, s2, target_fidelity, structural_disparity, lambda_penalty):\n",
    "    \"\"\"\n",
    "    Calculates gradients for a pair of sentences using the parameter-shift rule.\n",
    "    \"\"\"\n",
    "    gradients = {idx: 0.0 for idx in [(g, e) for g in range(len(params)) for e in range(len(params[g]))]}\n",
    "\n",
    "    for group_idx, group in enumerate(params):\n",
    "        for elem_idx in range(len(group)):\n",
    "            param_index = (group_idx, elem_idx)\n",
    "\n",
    "            # Shift the parameters\n",
    "            params_plus = [p.copy() for p in params]\n",
    "            params_plus[group_idx][elem_idx] += np.pi / 2\n",
    "            \n",
    "            params_minus = [p.copy() for p in params]\n",
    "            params_minus[group_idx][elem_idx] -= np.pi / 2\n",
    "\n",
    "            # Calculate overlap for shifted parameters\n",
    "            # Note: This is a simplified gradient of a complex function. \n",
    "            # In a real scenario, the gradient of the loss with respect to\n",
    "            # both state preparations would need to be calculated.\n",
    "            overlap_plus = calculate_overlap_classical(qnode_func, params_plus, params_plus)\n",
    "            overlap_minus = calculate_overlap_classical(qnode_func, params_minus, params_minus)\n",
    "\n",
    "            loss_plus = loss_function_manual(overlap_plus, target_fidelity, structural_disparity, lambda_penalty)\n",
    "            loss_minus = loss_function_manual(overlap_minus, target_fidelity, structural_disparity, lambda_penalty)\n",
    "            \n",
    "            # The parameter-shift rule\n",
    "            grad = (loss_plus - loss_minus) / 2\n",
    "            gradients[param_index] = grad\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06792be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Legacy methods for reference\n",
    "def swap_test(state1_vec, state2_vec, num_qubits):\n",
    "    \"\"\"\n",
    "    Performs a Quantum Swap Test between two quantum state vectors by sampling.\n",
    "    \"\"\"\n",
    "    SHOT_COUNT = 100000  # High shot count for better statistical accuracy\n",
    "    \n",
    "    total_qubits = 1 + 2 * num_qubits  # 1 Ancilla qubit + 2 state registers\n",
    "    dev = qml.device(\"default.qubit\", wires=total_qubits, shots=SHOT_COUNT)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit():\n",
    "        # Step 1: Prepare the ancilla qubit in a superposition\n",
    "        qml.Hadamard(wires=0)\n",
    "        \n",
    "        # Step 2: Prepare the two input states\n",
    "        qml.StatePrep(state1_vec, wires=range(1, 1 + num_qubits))\n",
    "        qml.StatePrep(state2_vec, wires=range(1 + num_qubits, 1 + 2 * num_qubits))\n",
    "\n",
    "        # Step 3: Apply controlled-SWAP gates\n",
    "        for i in range(num_qubits):\n",
    "            qml.CSWAP(wires=[0, 1 + i, 1 + num_qubits + i])\n",
    "\n",
    "        # Step 4: Apply Hadamard to the ancilla\n",
    "        qml.Hadamard(wires=0)\n",
    "        \n",
    "        # Step 5: Measure the ancilla qubit\n",
    "        return qml.sample(wires=0)\n",
    "\n",
    "    # The squared overlap is related to the probability of measuring |0>\n",
    "    # P(0) = (1 + |<psi|phi>|^2) / 2\n",
    "    # Overlap^2 = 2 * P(0) - 1\n",
    "    measurement_results = circuit()\n",
    "    prob_0 = np.sum(measurement_results == 0) / SHOT_COUNT\n",
    "    squared_overlap = 2 * prob_0 - 1\n",
    "    \n",
    "    return abs(squared_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fde61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function\n",
    "def plot_training_history(history):\n",
    "    if not history:\n",
    "        print(\"History is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history, label='Average Loss per Epoch')\n",
    "    plt.title('Training Loss Convergence')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Penalized Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_history(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(len(param_history))\n",
    "    means = [d['mean'] for d in param_history]\n",
    "    stds = [d['std'] for d in param_history]\n",
    "    mins = [d['min'] for d in param_history]\n",
    "    maxs = [d['max'] for d in param_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, means, label='Mean Parameter Value')\n",
    "    plt.fill_between(epochs, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.2, label='1 Std. Deviation')\n",
    "    plt.plot(epochs, mins, linestyle='--', color='gray', label='Min/Max Range')\n",
    "    plt.plot(epochs, maxs, linestyle='--', color='gray')\n",
    "    \n",
    "    plt.title('Evolution of Model Parameters During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_parameter_evolution_polar(param_history):\n",
    "    if not param_history:\n",
    "        print(\"Parameter history is empty. Cannot plot.\")\n",
    "        return\n",
    "    # Use a slightly larger figure for polar plots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Create a polar subplot\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "    epochs = np.array(range(len(param_history)))\n",
    "    # We use modulo 2*pi to keep the angles in one circle\n",
    "    mean_angles = np.array([d['mean'] for d in param_history]) % (2 * np.pi)\n",
    "\n",
    "    # The radius will be the epoch number, so the path spirals outwards over time\n",
    "    # The angle will be the mean parameter value\n",
    "    ax.plot(mean_angles, epochs, 'o-', label='Mean Parameter Path')\n",
    "\n",
    "    # Add markers for the start and end points for clarity\n",
    "    if len(epochs) > 0:\n",
    "        ax.plot(mean_angles[0], epochs[0], 'gX', markersize=12, label='Start')\n",
    "        ax.plot(mean_angles[-1], epochs[-1], 'rX', markersize=12, label='End')\n",
    "\n",
    "    # Formatting the plot\n",
    "    ax.set_theta_zero_location('N') # pyright: ignore[reportAttributeAccessIssue] # Set 0 degrees to the top\n",
    "    ax.set_theta_direction(-1) # pyright: ignore[reportAttributeAccessIssue] # Make it clockwise\n",
    "    ax.set_rlabel_position(0) # pyright: ignore[reportAttributeAccessIssue]\n",
    "    ax.set_rlim(0, len(epochs) * 1.05) # pyright: ignore[reportAttributeAccessIssue] # Set radius limit\n",
    "    ax.set_xlabel(\"Epoch\") # The radius represents the epoch\n",
    "    ax.set_title('Cyclical Evolution of Mean Parameter', pad=20)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HYPERPARAMETERS AND CONFIGURATION\n",
    "# ===============================================================\n",
    "\n",
    "# --- Data and Preprocessing ---\n",
    "DATA_PATH = r'<ABSOLUTE_PATH_TO_YOUR_QUESTIONS.CSV>'\n",
    "SAMPLE_FRACTION = 0.001  # Fraction of the dataset to use for a quick run\n",
    "QUBIT_LIMIT = 12         # Max qubits for a diagram to be included (adjust based on RAM)\n",
    "\n",
    "# --- Model and Circuit ---\n",
    "# Define the mapping from grammatical types to qubits\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "OB_MAP: dict[Ty, int] = { N: 1, S: 1 } # pyright: ignore[reportAssignmentType]\n",
    "\n",
    "# Define the ansatz\n",
    "N_LAYERS = 1\n",
    "ANSATZ = IQPAnsatz(OB_MAP, n_layers=N_LAYERS) # pyright: ignore[reportArgumentType]\n",
    "\n",
    "# Define the rewrite rules\n",
    "REWRITE_RULES = ['curry', 'prepositional_phrase', 'determiner']\n",
    "\n",
    "# --- Training ---\n",
    "EPOCHS = 15\n",
    "BASE_LEARNING_RATE = 0.01\n",
    "LAMBDA_PENALTY = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 404 sentence pairs.\n",
      "Starting preprocessing with a qubit limit of 12...\n"
     ]
    }
   ],
   "source": [
    "#Click this to run model\n",
    "# 1. Initialize lambeq components using the defined hyperparameters\n",
    "tokeniser = SpacyTokeniser()\n",
    "parser = BobcatParser()\n",
    "rewriter = Rewriter(REWRITE_RULES)\n",
    "\n",
    "# 2. Load and preprocess data\n",
    "sentences1, sentences2, value = load_data(DATA_PATH, sample_fraction=SAMPLE_FRACTION)\n",
    "data_pairs = list(zip(sentences1, sentences2, value))\n",
    "\n",
    "filtered_data, symbols, n_max = preprocess_data_for_model(\n",
    "    data_pairs, tokeniser, ANSATZ, parser, rewriter, qubit_limit=QUBIT_LIMIT\n",
    ")\n",
    "\n",
    "# 3. Run training and plot results\n",
    "if filtered_data and n_max > 0:\n",
    "    # Call the new training function\n",
    "    trained_model, loss_history, param_history = train_with_fischer_lr(\n",
    "        filtered_data, \n",
    "        symbols, \n",
    "        n_max, \n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Create both plots\n",
    "    print(\"\\n--- Training Results ---\")\n",
    "    plot_training_history(loss_history)\n",
    "    plot_parameter_evolution_polar(param_history)\n",
    "else:\n",
    "    print(\"\\nNo data to train on.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# === Data Configuration ===
data:
  fasttext_path: <PATH_TO_FASTTEXT_BIN\cc.en.300.bin> # Path to .bin model
  path: <PATH_TO_DATASET\questions.csv> # Path to sentence pairs
  qubit_limit: 12            # Max qubits allowed per sentence circuit (20 absolute max)
  sample_fraction: 0.05      # Fraction of dataset to load (e.g., 0.05 for 5%)
  test_size: 0.2             # Fraction of filtered data for the test set (e.g., 0.2 for 20%)
  validation_size: 0.25      # Fraction of remaining data for validation (e.g., 0.25 of train+val = 20% of total)

# === QNLP Configuration ===
qnlp:
  ansatz_type: 'StronglyEntangling'         # Type of ansatz: 'IQP' or 'StronglyEntangling'
  n_layers: 2                # Number of layers for the chosen ansatz(1-3)
  rewrite_rules:             # List of lambeq rewrite rules to apply
    - curry
    # - prepositional_phrase # Often causes issues with IQP/SEA
    - determiner
  embedding_method: 'parameterized' #Padding for SWAP test Options: 'simple_pad', 'superposition', 'entangle', 'parameterized'

# === NLP Configuration ===
nlp:
  spacy_model: 'en_core_web_sm' # SpaCy model for tokenization

# === Simulation Configuration ===
simulation:
  cpu_cores: 6               # Max CPU cores for simulator (if supported)
  device: 'lightning.qubit'    # PennyLane device: 'default.qubit', 'lightning.qubit', etc.

# === Training Configuration ===
training:
  use_local_cost: True       # Use local MSE cost (True) or global SWAP test cost (False)
  base_learning_rate: 0.001  # Initial learning rate for Adam optimizer
  epochs: 5                  # Number of training epochs
  lambda_penalty: 0.1        # Weight for the structural disparity penalty

# === Logging Configuration ===
logging:
  log_file: 'error_log.txt'  # File to write exceptions to
  log_level: 'WARNING'       # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)